---
title: "Predicting seasonal influenza hospitalizations using an ensemble super learner: a simulation study"

bibliography: references.bib
csl: american-journal-of-epidemiology.csl

always_allow_html: true
---

**Running Head:** Predicting Influenza Hospitalizations

**Submission Type:** Original Contribution

\newpage
    
# Abstract

Accurate forecasts can inform response to outbreaks. Most efforts in influenza forecasting have focused on predicting influenza-like activity, but fewer on influenza-related hospitalizations. We conducted a simulation study to evaluate a super learner's predictions of three seasonal measures of influenza hospitalizations in the United States: peak hospitalization rate, peak hospitalization week, and cumulative hospitalization rate. We trained an ensemble machine learning algorithm on 15,000 simulated hospitalization curves and generated weekly predictions. We compared the performance of the ensemble (weighted combination of predictions from multiple prediction algorithms), the best-performing individual prediction algorithm, and a naive prediction (median of a simulated outcome distribution). Ensemble predictions performed similarly to the naive predictions early in the season but consistently improved as the season progressed for all prediction targets. The best-performing prediction algorithm in each week typically had similar predictive accuracy compared to the ensemble, but the specific prediction algorithm selected varied by week. An ensemble super learner improved predictions of influenza-related hospitalizations, relative to a naive prediction. Future work should examine the super learner's performance using additional empirical data on influenza-related predictors (e.g., influenza-like illness). The algorithm should also be tailored to produce prospective probabilistic forecasts of selected prediction targets.

(199/200 words)


# Introduction

```{r setup, include=FALSE}
source("manuscript-setup.R")
```

Between 2010 and 2017 approximately 140,000--570,000 individuals were hospitalized and 12,000--51,000 died annually due to seasonal influenza in the United States [@Centers_for_Disease_Control_and_Prevention2020-uc]. Accurately predicting future burden of influenza-related hospitalizations during an influenza season could help policymakers, public health officials, providers, and other stakeholders better allocate resources and prepare for expected changes in hospitalization rates [@Lutz2019-co]. For example, forecasts could provide hospitals with lead time to make inpatient beds available for patients admitted with influenza-related illnesses [@Nap2008-pandem; @Nap2007-pandem].

Recently, researchers have improved the quality of and capacity for influenza-like illness (ILI) forecasting [@Reich2019-uk; @Chretien2014-dy], using diverse classes of models to generate forecasts, including statistical time series models [@Reich2019-uk; @Biggerstaff2018-ns], Bayesian methods [@Chretien2014-dy; @Brooks2015-fl], and mechanistic mathematical models [@Chretien2014-dy; @Reich2019-uk]. Ensemble methods, which aggregate predictions from multiple models [@Reich2019-ca; @Hastie2009-ft; @Wolpert1992-pw; @Breiman1996-ez], promise to further improve the accuracy and stability of epidemic predictions [@Reich2019-ca; @Ray2018-ef; @Lutz2019-co].

By aggregating and weighting predictions, ensembles can borrow strengths and blunt weaknesses across the models used as inputs [@Van_der_Laan2007-ml; @Gomez2016-empirical; @Grosan1997-solving]. The Centers for Disease Control and Prevention's (CDC) primary in-season outpatient ILI forecasts are now based on ensemble forecasts combining predictions from individual forecasts submitted by research teams around the United States, who use a variety of prediction algorithms, including their own ensembles [@Reich2019-uk]. The "meta"-ensembles combine in-season forecasts using standardized protocols for submitting and scoring individual forecasts of pre-specified weekly and season-level prediction targets [@Reich2019-uk]. One ensemble machine learning method, dubbed "super learner" [@Van_der_Laan2007-ml; @Polley2010-cb; @Polley2011-oz], exhibits properties that may make it a powerful tool in prediction: asymptotically, the ensemble super learner is guaranteed to perform as well as the best-fitting component model [@Polley2010-cb; @Polley2011-oz]. 

Most influenza forecasting focuses on ILI [@Reich2019-uk; @Biggerstaff2018-ns; @McGowan2019-ph; @Kandula2018-sq; @Brooks2015-fl] rather than influenza-related hospitalizations [@Kandula2019-tg]. Influenza hospitalization dynamics may differ from those of transmission. Because hospitalizations are one marker of circulating influenza strains' severities [@Biggerstaff2018-ns], predicting hospitalizations using ensembles should complement existing ILI forecasting.

We conducted a simulation study to examine the super learner's potential as a tool for predicting seasonal influenza hospitalizations at the national level in the United States. Due to the small number of flu seasons available in surveillance data, we simulated 15,000 influenza hospitalization curves, using empirical data as templates, and trained the super learner on this simulated distribution. We predicted three targets selected following those specified in CDC forecasting competitions [@Centers_for_Disease_Control_and_Prevention_undated-tx]: peak hospitalization rate, peak hospitalization week, and cumulative hospitalization rate. At each week of the flu season, we compared the predictive performance of the ensemble super learner against that of the discrete super learner (best-performing component model) and a "naive" prediction.
    
# Methods
    
## Empirical data

We downloaded publicly available surveillance data on seasonal influenza-related hospitalizations from the CDC's FluView Interactive dashboard [@Centers_for_Disease_Control_and_Prevention_undated-vt], using Emerging Infections Program (EIP) data beginning with the 2003--2004 season and ending with 2018--2019. We omitted the 2009--2010 pandemic influenza year due to dynamics that differ between pandemic and seasonal influenza and modified curves so that each flu season spanned 30 weeks (see Web Appendix 1). 

Hospitalization rates reported in FluSurv-NET, of which EIP represents a subset of surveillance sites, are calculated based on influenza-related hospitalizations among residents within a given catchment area. Cases are defined as hospitalizations accompanied by laboratory-confirmed influenza during the hospitalization or in the 14 days preceding it [@2021-influenza]. Rate denominators are the total population in the specified catchment area [@2021-influenza].

## Prediction targets

Following from the CDC's Flu Sight challenge [@Centers_for_Disease_Control_and_Prevention_undated-tx], we specified 3 season-level prediction targets:

1. _Peak hospitalization rate_, highest weekly influenza-related hospitalization rate (per 100,000 persons) during a flu season,
2. _Peak hospitalization week_, week during which this peak rate occurred, and
3. _Cumulative hospitalization rate_, cumulative influenza-related hospitalization rate over the flu season. 

Fifteen empirical observations were available for each prediction target (Table \@ref(tab:targtable)).

## Simulating hospitalization curves

To simulate seasonal influenza hospitalization curves, we adapted an approach developed by Brooks et al. to predict ILI [@Brooks2015-fl]. First, using the `genlasso` package in R [@Arnold-2019-gl; @RCore2020-ct], we fitted a linear trend filter [@Kim2009-bz; @Tibshirani2014-tr] to each of the 15 observed influenza hospitalization curves in the EIP. The linear trend filter is a penalized method that fits a piecewise linear function to a time series, testing multiple values of the penalty ($\lambda$) by default [@Arnold-2019-gl]. We used these fits as templates for the simulated influenza hospitalization curves (Web Table 1), choosing the $\lambda$ penalty via 5-fold cross-validation [@Brooks2015-fl]. In the main analysis we used the $\lambda$ value that minimized the trend filter's prediction error across the folds [@Brooks2015-fl], hereafter referred to as $\lambda_{min}$ Web Figure 1.

Next, we used the resulting 15 trend filter fits in a slightly modified version of the curve generation scheme described by Brooks et al. (see Web Appendix 2), who conceptualize a seasonal influenza curve generically as a function of the week of the season, plus a noise term with mean 0 and variance $\tau^2$ [@Brooks2015-fl]. For each empirical season, we averaged the squared residuals from its trend filter fit over all weeks to estimate ${(\tau^s)}^2$.

For each simulated curve, we sampled each of the following parameters randomly and independently:

  * a vector of estimated hospitalization rates based on a linear trend filter fit (empirical shape template)
  * the root mean squared error of one linear trend filter fit 
  * a draw from a continuous uniform distribution, range: [0.75, 1.25]
  * a draw from a continuous uniform distribution bounded by the minimum and maximum peak hospitalization rates from the 15 trend filter fits
  * a draw from a continuous uniform distribution bounded by the minimum and maximum peak weeks from the 15 trend filter fits

We fed these parameters into the curve-generating function to produce a hypothetical influenza hospitalization curve, imposing a constraint that preserved positive simulated hospitalization rates and set negative hospitalization rates to 0 (`r round(prop_weeks_transformed * 100, 2)`% of simulated weeks). Negative rates could be generated at the tails of a season, when hospitalization rates are low, because noise was normally distributed.

```{r}
tmpcts <- fread(
  get_asset("TAB", "Simulation-Template-Counts", simdat_date)
)
```

We simulated 15,000 curves, where each empirical shape template was used to simulate between `r tmpcts[, min(as.numeric(stringr::str_remove(N, ",")))]` and `r tmpcts[, format(max(as.numeric(stringr::str_remove(N, ","))), big.mark = ",")]` curves. We considered the resultant set of simulated curves to be a plausible distribution of hypothetical flu seasons that could be observed in principle [@Brooks2015-fl] (Figure \@ref(fig:empsim-compare)). While the simulated distributions of prediction targets differed from the empirical distributions (Figure \@ref(fig:simcompare)), only 15 empirical seasons were available and may not capture the range of possible hospitalization curves [@Brooks2015-fl].

## Super learner
  
The super learner produces an ensemble prediction that is a weighted linear combination of predictions from a set of component prediction algorithms, where weights are selected by minimizing a user-specified loss function that determines the target for which the ensemble's predictions are optimized (e.g., conditional outcome mean) [@Van_der_Laan2007-ml; @Polley2010-cb; @Polley2011-oz]. The super learner takes as inputs a data set, prediction algorithms (component learners), and a loss function. Component learners could include parametric models (e.g., generalized linear models), semi-parametric models, and machine-learning algorithms (e.g., neural networks), among others [@Naimi2018-fv].

For each prediction target in the current study we specified the same library of learners and measured the component and ensemble super learners' performance using the absolute error loss, the average absolute difference between model predictions and the true "observed" values in the simulated data set [@Polley2011-oz]. This loss function targets the conditional median of the outcome distribution [@Polley2011-oz]---for example, median peak hospitalization rate---which we selected due to skewness in the simulated peak week and cumulative hospitalization distributions (Figure \@ref(fig:simcompare)). 

We implemented the super learner using the `sl3` package in R [@Coyle2020-ze]. Progressing sequentially through the flu season, we ran the super learner at each week *i* to predict the season-level target of interest as a function of current and past hospitalization rates. All component learners included the following predictor variables (see Web Appendix 3 for an exception; rates per 100,000 population):

  * hospitalization rate in week *i*, 
  * cumulative rate through week *i*,
  * lagged hospitalization rates from all weeks prior to week *i*, 
  * lagged cumulative rate from all weeks prior to week *i*,
  * the difference between the hospitalization rate in week *i* and the hospitalization rate in each prior week up to 5 weeks in the past, 
  * the difference between the cumulative hospitalization rate in week *i* and the cumulative hospitalization rate in each prior week up to 5 weeks in the past
  * product terms between the hospitalization rate in week *i* and each of the cumulative hospitalization differences, and 
  * product terms between the cumulative hospitalization rate in week *i* and each of the hospitalization rate differences.

Thus, we attempted to mimic a situation where forecasters must predict the future course of a seasonal influenza epidemic based only on available data. By specifying lagged terms, differences, and product terms, we intended to capture hospitalization dynamics like the rate of change in hospitalization rates and interdependencies between current hospitalization rates and cumulative rate. We fit the super learner 30 times for each prediction target, for each week in the season. For the peak rate and peak week outcomes, this procedure implies that models were used to predict peaks that may have already occurred. In the real world, forecasters would not always know whether the season's peak had occurred (e.g., potential for a second wave); generating predictions for all targets throughout the season might be useful under particular seasonal dynamics.

We estimated the predictive accuracy of the component models and ensemble learner using 15-fold cross-validation [@Harrell2015-cd]. Briefly, *V*-fold cross-validation is an iterative sample-splitting procedure where each observation is assigned to a group (called a "fold"), and each fold serves as the validation set for a model trained on the rest of the sample [@Harrell2015-cd; @Naimi2018-fv]. Cross-validation maximizes the amount of data available for model-fitting by avoiding the need for holding out validation data and by reducing potential for overfitting [@Van_der_Laan2007-ml; @Naimi2018-fv]. Each component learner's predictive accuracy is evaluated as a "risk", defined in our case as the average absolute error between a learner's prediction and the true simulated observations (e.g., peak hospitalization rate). Risks were estimated in each validation set fold. The "cross-validated risk" refers to the average risk across all 15 folds [@Van_der_Laan2007-ml; @Naimi2018-fv].

We repeated the following procedure for each prediction target:

1. Create 30 data sets, one for each week of the flu season, containing outcome variables and predictors.
1. Assign each observation to a cross-validation fold. Each weekly dataset contained 15,000 rows, one for each simulated influenza hospitalization curve. To account for the lack of independence between curves, we assigned all simulated curves based on the same empirical shape template to the same fold (e.g., all curves based on the trend filter fit to season 2004-05). We did not account for potential dependencies due to repeated sampling of the other terms, as we believed the season shape was the most consequential source of dependence. 
1. Execute the super learner algorithm:
   a. Train each learner on the data set using 15-fold cross-validation and calculate its cross-validated risk.
   b. Concatenate component learner predictions made in the validation sets.
   c. Regress simulated outcomes on the component predictions to create a linear weighted combination of the individual learners' predictions via a "metalearner" model set to optimize for the absolute error loss [@Van_der_Laan2007-ml; @Coyle2020-ze; @Naimi2018-fv].
   d. Fit the ensemble in the full data and calculate its prediction risk.

For each prediction target and at each week of the season, we trained 76 component models, including variations on tuning and input parameters (Table \@ref(tab:candmodels-tuning-table)). We used the `sl3` [@Coyle2020-ze] package's default (`solnp` function [@Ghalanos2015-sn]) as the metalearner. 

The procedure outlined above yields cross-validated prediction risks for each component learner and an in-sample prediction risk for the ensemble super learner. Because in-sample risks may underestimate prediction error, we also cross-validated the ensemble super learner's prediction risks by repeating the procedure outlined above in nested fashion. Holding out each of the 15 data folds in turn, we fit the super learner algorithm within the corresponding training set and measured the resulting ensemble's prediction risk in the holdout fold. We then averaged the 15 fold-specific risks yielded by this procedure in order to estimate the cross-validated ensemble risk. Following calculations implemented in the `sl3` package [@Coyle2020-ze], we estimated the standard error for this cross-validated risk by dividing the standard deviation of risks across the 15 validation folds by the square root of 15,000, the total number of simulated outcomes.


## Component models

Theory and finite-sample demonstrations indicate the super learner should benefit from using a variety of component models, even if some models perform poorly [@Van_der_Laan2007-ml]. Therefore, we specified a variety of component learners and tuning parameters (e.g., kernels, complexity penalties) to probe a diverse set of prediction algorithms. We used component learners provided in the `sl3` [@Coyle2020-ze] and `SuperLearner` [@Polley2019-sl] packages in R (Web Appendix 3).
 
For each prediction target, we compared the ensemble super learner's performance against the discrete super learner and a naive prediction (the outcome median). We also used cross-validation to calculate the naive prediction's error: for each validation set we used the outcome median from the training set as the prediction and averaged the fold-specific risks to get the cross-validated risk.
 
## Sensitivity analyses

We conducted three sensitivity analyses to explore the ensemble learner's performance under different analytic choices. 

First, we simulated curves using an alternate penalty in the linear trend filter ($\lambda_{SE}$), selected as the $\lambda$ that produced a cross-validated error estimate within one standard error of the prediction error produced by $\lambda_{min}$ [@Arnold-2019-gl] (Web Figure 3).

Second, we included only the elastic net and random forest learners, based on their good performance in the main analysis. This post-hoc analysis compared the quality of predictions produced by the ensemble super learner when given a smaller subset of well-performing component learners.

Third, we optimized the ensemble super learner using the squared error loss [@Polley2011-oz] instead of the absolute error loss, to determine whether the choice of optimization goal changed our conclusions. The squared error loss is calculated using mean squared error and targets the conditional outcome mean. All other procedures in this post-hoc analysis were conducted as described in the main analysis, except that 1) the naive prediction was the outcome mean (rather than the median), and 2) we included only a subset of weeks (1, 5, 10, 15, 20, 25, 30) to reduce computation time while still characterizing the ensemble's performance across the season.
  
## Subanalysis

Post-hoc we assessed the prospective performance of ensembles trained in simulated data versus those trained in empirical data. We selected the three most recent influenza seasons from surveillance data (2016--17, 2017--18, 2018--19) and generated predictions for each at weeks 5, 10, 15, 20, and 25.

To predict peak rate, peak week, and cumulative rate for each season, we first re-ran the super learner algorithm on simulated data based on empirical shape templates from seasons prior to the given target season. Next, we trained the super learner on empirical hospitalization data from prior seasons. In the observed data, we used 5-fold cross-validation due to small sample size (12--14 seasons). Using these fitted ensembles, we generated predictions for each target measure and (observed) season. The median observed target (e.g., peak rate) from prior empirical seasons served as the naive prediction. We also compared ensemble estimates to discrete super learners.

Finally, we conducted a subanalysis in which we fit the ensemble super learner on all 15 empirical flu hospitalization curves and then used these ensembles to predict outcomes in the simulated hospitalization curves. We compared the predictions from these ensembles to those in the main analysis.

# Results

## Peak rate

For the peak rate prediction target, the ensemble super learner produced risk estimates similar to the median prediction early in the season but improved on the median prediction as the season progressed (Table \@ref(tab:main-analysis-risk-table)). This improvement is indicated by the ensemble's lower prediction risks, which equate to better predictive accuracy. Throughout the flu season, the ensemble and discrete super learners exhibited similar prediction risks, though component learners selected as discrete super learners varied by week (Table \@ref(tab:main-analysis-risk-table), Figure \@ref(fig:ensemble-perf-main)A). Ensemble predictions for the peak hospitalization rate exhibited mean prediction risks ranging between `r min(presl_num)` and `r max(presl_num)` per 100,000 across the flu season (Figure \@ref(fig:ensemble-perf-regscale)A). Discrete super learner risks ranged between `r min(prdsl_num)` and `r max(prdsl_num)` per 100,000. Additional information regarding component learners appears in Web Table 2 and Web Figure 4.

## Peak week 

For the peak week prediction target, the ensemble super learner generally produced risk estimates similar to the median prediction early in the season but improved on the median prediction as the season progressed (Table \@ref(tab:main-analysis-risk-table)), where improvement again is based on the observation that ensemble prediction risks drop below those of the median prediction. As with peak rate, the ensemble and discrete super learners exhibited similar prediction risks across the season, and the discrete super learner varied by week. Unlike the peak rate analysis, prediction quality appeared to exhibit a threshold effect near the middle of the season, at which point ensembles' prediction risks improved and then remained relatively stable. Ensemble predictions for the peak week exhibited mean prediction risks ranging between `r min(pwesl_num)` and `r max(pwesl_num)` weeks across the flu season (Figure \@ref(fig:ensemble-perf-regscale)B). Discrete super learner risks ranged between `r min(pwdsl_num)` and `r max(pwdsl_num)` weeks. Additional information regarding component learners appears in Web Table 3 and Web Figure 5.
  
## Cumulative rate

For the cumulative rate prediction target, the ensemble super learner produced risk estimates similar to the median prediction early in the season but improved on the median prediction as the season progressed (Table \@ref(tab:main-analysis-risk-table)), where improvement again is based on the observation that ensemble prediction risks drop below those of the median prediction. As with peak rate and peak week, the ensemble and discrete super learners exhibited similar prediction risks across the season, and the discrete super learner varied by week (Table \@ref(tab:main-analysis-risk-table), Figure \@ref(fig:ensemble-perf-main)C). Ensemble predictions for the cumulative rate exhibited mean prediction risks between `r min(chesl_num, na.rm = TRUE)` and `r max(chesl_num, na.rm = TRUE)` per 100,000 population, and these risks appeared to be higher earlier in the season (Figure \@ref(fig:ensemble-perf-regscale)C). Discrete super learner risks ranged between `r min(chdsl_num, na.rm = TRUE)` and `r max(chdsl_num, na.rm = TRUE)` per 100,000 population. Additional information regarding component learners appears in Web Table 4 and Web Figure 6.


## Sensitivity analyses 

_Alternate trend filter penalty._ The ensemble super learner trained on simulated curves using an alternate trend filter penalty generally produced qualitatively similar results across all prediction targets (Figure \@ref(fig:ensemble-perf-regscale)). However, ensemble risks early in the season appear to vary more than in the main analysis. Additional results appear in Web Appendix 4, Web Tables 5--7, and Web Figures 7--9.

_Component learner subset._ The ensemble learner trained using only the elastic net and random forest learners also produced qualitatively similar results across all prediction targets (Figure \@ref(fig:ensemble-perf-regscale)). Variance in ensemble risks for peak week appeared to vary more than in the main analysis, suggesting the super learner did benefit slightly from the inclusion of a larger set of models. However, in absolute terms the differences were small. Additional results appear in Web Appendix 4, Web Tables 8--10, and Web Figures 10--12.

_Squared error loss._ The ensemble learner trained to optimize predictions for the squared error loss (rather than the absolute error loss) consistently exhibited mean prediction risks lower than those of the naive mean prediction across the flu season and for all prediction targets (Web Figure 13). Qualitatively, ensemble prediction risks were closer to the naive mean early in the season and improved as the season progressed, as in the main and sensitivity analyses. Additional results appear in Web Appendix 4, Web Tables 11--13, and Web Figures 14--16.

  
## Subanalysis

Across all prediction targets, the ensemble super learners trained on simulated and observed data both tended to improve on the naive median prediction using observed data from prior seasons (Figure  \@ref(fig:prosp-application), Web Figure 17). These improvements were more apparent in the 2017--18 influenza season which was more severe than 2016--17 and 2018--19 with respect to peak rate and cumulative hospitalizations. In addition, ensembles tended to produce predictions of similar quality across the flu season, while the ensemble trained only on observed data generated several predictions of much poorer quality than both the ensemble trained in simulated data and the naive median prediction. These instances occurred during the 2017--18 season---in Week 5 for peak rate and in Weeks 5 and 20 for peak week. The ensemble trained on simulated data produced predictions of more stable quality. As in the main analysis, the ensemble's prediction error from the super learner trained in simulated data was similar to that of the discrete super learner, though the model selected as the discrete super learner varied by prediction target, season, and week.

When used to predict outcomes among the simulated influenza hospitalization curves, ensembles trained only on observed data exhibited higher prediction errors than those trained on simulated data for all prediction targets throughout most of the flu season (Web Figure 18).

# Discussion

We found that an ensemble super learner improved over a naÃ¯ve median prediction in predicting three measures of seasonal influenza-related hospitalizations. Ensemble predictions at the beginning of the season tended to mirror those of the naive median, but the ensemble estimates improved progressively throughout the season. The ensemble also typically exhibited similar predictions risks when compared to the discrete super learner, though the choice of discrete super learner varied by week for all prediction targets. Our results have two key implications. First, a single type of model is unlikely to produce optimal predictions across the entire flu season. Second, the discrete super learner selected in a given week might not produce the best predictions for that week in future flu seasons. The ensemble provides a means for making reasonable predictions under uncertainty regarding model specification. Prior work on ensemble forecasting of ILI supports this assertion [@Reich2019-ca; @Ray2018-ef].

Theory suggests the ensemble super learner should benefit from the inclusion of a wide range of prediction algorithms in the component learner library [@Polley2010-cb; @Polley2011-oz]. Our results support these prior findings. However, ensembles trained using only random forests and elastic net performed comparably to ensembles fitted using the full component library. When decision-makers need results quickly, specifying fewer component learners _a priori_ based on subject matter knowledge may produce accurate predictions with less computation time. However, because no learning algorithm can outperform all others across all potential prediction tasks [@Wolpert1997-no; @Macready1996-what], even ensembles incorporating a limited number of component models may benefit from representing as wide a class of learning algorithms as possible, given time and resource constraints.

In our prospective subanalysis, the ensemble super learner trained on simulated data appeared to produce more stable predictions than one trained only on empirical data. This feature was more prominent in the 2017--18 season, the most severe season captured in surveillance. Training the super learner on simulated data that contained hospitalization curves of similar and greater severity than 2017--18 may have improved the ensemble in this case. Part of our original rationale for simulating hospitalization curves was that available data do not capture the range of possible seasonal hospitalization dynamics, and our results suggest that simulation-trained ensembles may be valuable for predicting under atypical seasonal flu dynamics. Nonetheless, the observed differences between the ensembles trained on simulated versus empirical data could be due to random variation under a single realization of the super learner algorithm (per week) or to the small number of observed influenza seasons. Averaging the ensemble predictions of repeated super learner fits on observed data may have improved their stability. That said, we noted similar results when we used ensemble super learners trained on empirical data to predict simulated outcomes: the prediction errors varied markedly throughout the season for each prediction target and included extreme errors in which the empirically trained super learners produced impossible predictions.

The super learner appears to be a tool with some promise for forecasting influenza hospitalizations, suggesting several directions for future research. First, predictions of seasonal influenza hospitalization may be improved by incorporating empirical data on influenza-related parameters such as viral activity and ILI [@Centers_for_Disease_Control_and_Prevention_undated-vt] as well as data on other factors believed to affect influenza transmission dynamics, such as climate and weather. Likewise, influenza hospitalization counts, as with other influenza-related surveillance data, are subject to retrospective correction during a flu season. To account for these corrections, component models could additionally model measurement error and reporting delays that exist in real-time flu data to improve ensemble predictions in real-world applications. 

Tailoring the super learner to generate forecasts similar to those used in current real-world applications would also be a potentially fruitful avenue to explore. Whereas we focused on minimizing the absolute error loss, FluSight competitions now require probabilistic forecasts in addition to point predictions optimized to minimize absolute error [@Centers_for_Disease_Control_and_Prevention_undated-tx]. Adapting the super learner for this purpose might treat the problem as a classification task [@Bi2019-what] in which we would make binary predictions of, for example, the peak hospitalization rate falling within a given range. 

Our findings should be interpreted respecting several limitations. 

First, we assumed the distribution of simulated curves accurately characterizes the hypothetical distribution of possible seasonal influenza hospitalization trajectories. We did not attempt to match simulated distributions to their empirical counterparts rigidly to allow for generating atypical influenza seasons that could be, but perhaps have not yet been, observed. Therefore, we assumed the 15 available empirical seasons do not capture the range of possible outcomes but do provide a reasonable basis for simulating the hypothetical distribution. Nonetheless, surveillance data likely underestimate influenza-attributable hospitalizations (Web Appendix 1). 

Second, we did not implement the super learner as a time series in which predictions are dynamically updated using previous weeks' predictions. Future work could explore "online" ensemble super learners of this sort [@Benkeser2018-if]. 

Third, because we modeled seasonal influenza hospitalizations, our results do not apply to influenza pandemics. However, ensembles have become important tools for forecasting infectious disease pandemics in general (e.g., the SARS-CoV-2 pandemic [https://viz.covid19forecasthub.org]).

Finally, in our prospective subanalysis we held out empirical shape templates from seasons prior to the target season in order to re-train the ensemble. However, we did not resimulate the underlying curves to avoid including curves based on sampled parameters from the target season (e.g., peak rate). Thus, the ensemble trained in simulation data may have had an inherent advantage over the ensemble trained in empirical data.

We found that an ensemble super learner consistently outperformed a naive prediction and performed approximately as well as the discrete super learner. Future work should 1) investigate the super learner's performance in empirical data incorporating additional information on influenza activity, 2) compare the external predictive validity of ensemble super learners trained using different component learner libraries specified _a priori_, and 3) translate the approach to generate probabilistic in-season forecasts. In addition, adapting the current approach to produce forecasts at the state or region levels may be important for targeting public health activities more precisely to mitigate the effects of seasonal flu epidemics.


# References

<div id="refs"></div>
\bibliography{references}
\newpage



\newpage
# Tables

<!-- TABLE ONE -->
```{r table1, tab.id="targtable", tab.cap="Empirical distributions of peak hospitalization rate, peak hospitalization week, and cumulative hospitalization rate in the United States, 2003--2019.^a^"}

targets <- fread(get_asset("TAB", "Prediction-Targets", simdat_date))
targets <- targets[Season != "2009-10"]

targets %>%
  flextable(., cwidth = c(0.7, rep(1.3, 3))) %>%
  compose(.,
    part = "header", j = "Peak rate",
    value = as_paragraph("Peak rate", as_sup("b"))
  ) %>%
  compose(.,
    part = "header", j = "Cumulative hospitalizations",
    value = as_paragraph("Cumulative hospitalizations", as_sup("b"))
  ) %>%
  bold(., part = "header") %>%
  font(., fontname = global_table_font, part = "all")

```

::: {custom-style="FlexTable Footnote"}
^a^ Source: CDC FluView (Emerging Infections Program). Pandemic influenza season 2009-2010 omitted.

^b^ Peak rate and cumulative hospitalizations expressed per 100,000 population. 
:::

\newpage

<!-- TABLE TWO -->
```{r table2, tab.id="candmodels-tuning-table", tab.cap="Component learners and tuning parameters."}

tunetab <- tribble(
  ~ Model,
  ~ `Tuning parameters`,
  ~ `R package`,

  "Median prediction",
  "Median of the outcome distribution",
  "N/A",

  "Linear regression with variable screening",
  "Variables screened for inclusion in the linear prediction model using correlation-based measures.",
  "base",

  "Random forest (regression trees)",
  "Number of trees: {50, 100, 500}; Terminal node sizes: {3, 5, 10}; Number of variables sampled for use in data-splitting: {ncov / 3, ncov / 2, ncov / 1.5}",
  "randomForest",

  "Support vector regression",
  "Kernel: {radial, polynomial}; Degree: {1, 2, 3}, applied only to polynomial",
  "e1071",

  "Penalized regression",
  "Penalty: {lasso, ridge}",
  "glmnet",

  "Elastic net",
  "Alpha penalty: {0.25, 0.5, 0.75}",
  "glmnet",

  "Neural network",
  "Number of nodes in hidden layer: {5, 10, 25, 50, 75, 100}; Decay: {0, 0.005, 0.1, 0.2, 0.4}",
  "SuperLearner (uses nnet)",

  "LOESS",
  "Span = {0.25, 0.5, 0.75, 1}",
  "SuperLearner (uses base)",

  "Polynomial multivariate adaptive regression spline",
  "Gcv penalty = {2, 4, 6, 8, 10}",
  "polspline",
)

fn <-  as_paragraph(as_i("ncov"),  ", number of predictors. For the linear regression model, the variable-screening procedure is not a tuning parameter per se. We describe it in the \"tuning parameter\" column for ease of presentation. For the random forest and neural network learners, all combinations of the tuning parameters presented were proposed as separate learning algorithms.")

linreg_row <- which(tunetab$Model %like% "Linear regression")
nnet_rf_rows <- which(tunetab$Model %like% "Neural|Random forest")

tt <- tunetab %>%
  flextable(., cwidth = c(1.9, 3.5, 1.2)) %>%
  fit_to_width(., max_width = 7.5) %>% 
  bold(., part = "header") %>%
  align(j = 1:2, align = "left", part = "header") %>%
  padding(padding = 7, part = "body") %>%
  align(j = 1:2, align = "left") %>%
  valign(valign = "top") %>%
  ## add footnote superscripts
  compose(., 
    i = linreg_row,
    j = "Model",
    value = as_paragraph(tunetab$Model[linreg_row], as_sup("a"))
  ) %>%
  compose(.,
    i = nnet_rf_rows,
    j = "Model",
    value = as_paragraph(tunetab$Model[nnet_rf_rows], as_sup("b"))
  ) %>%
  #add_footer_row(top = FALSE, colwidths = 3, values = "") %>%
  font(., fontname = global_table_font, part = "all")

tt <- compose(tt, part = "footer", value = fn)
tt
```

::: {custom-style="FlexTable Footnote"}
_ncov_, number of predictors

^a^ For the linear regression model, the variable-screening procedure is not a tuning parameter _per se_. We describe it in the "tuning parameter" column for ease of presentation. 

^b^ For the random forest and neural network learners, all combinations of the tuning parameters presented were proposed as separate learning algorithms.
:::

\newpage

<!-- TABLE THREE -->

```{r table3, tab.id="main-analysis-risk-table", tab.cap="Prediction risks for the ensemble and discrete super learner predictions of the peak hospitalization rate, peak hospitalization week, and cumulative hospitalization rate, by week of influenza season.^a^ Estimates presented as mean risk (standard error)."}

rt <- flextable(prwc_risks)
esl_lab <- as_paragraph("EnsembleSL", as_sup("b"))
dsl_lab <- as_paragraph("DiscreteSL", as_sup("c"))

prwc_risks %>%
  flextable(cwidth = c(0.52, rep((6.6 - 0.52) / 6, 6))) %>%
  compose(., 
    part = "header",
    j = rt$col_keys[which(rt$col_keys %like% "CV")],
    value = esl_lab
  ) %>%
  compose(.,
    part = "header",
    j = rt$col_keys[which(rt$col_keys %like% "Discrete")],
    value = dsl_lab
  ) %>%
  add_header_row(
    top = TRUE,
    values = c("", "Peak rate", "Peak week", "Cumulative rate"),
    colwidths = c(1, 2, 2, 2)
  ) %>%
  theme_booktabs() %>%
  fontsize(size = 11, part = "header") %>%
  bold(., part = "header") %>%
  align(., j = 2:7, part = "all", align = "center") %>%
  line_spacing(., part = "body", space = 0.75) %>%
  font(., fontname = global_table_font, part = "all") %>%
  fontsize(., size = 11, part = "all")
```


```{r table3-fn, results = "asis"}
fullfn <- risktabfn(c("Peak rate", "Peak week", "Cumulative hospitalizations"))

a_bit <- stringr::str_extract(fullfn, "The super learner.*$")
b_bit <- stringr::str_extract(fullfn, "The ensemble super learner.*component model predictions\\.")
c_bit <- stringr::str_extract(fullfn, "The discrete super learner.*component model\\.")

cat(
  "::: {custom-style='FlexTable Footnote'}", "\n",
  "SL, super learner", "\n\n",
  "^a^ ", a_bit, "\n\n",
  "^b^ ", b_bit, "\n\n",
  "^c^ ", c_bit, "\n\n",
  ":::",
  sep = ""
)
```

\newpage
# Figures

```{r fig.id="empsim-compare", fig.width = 4, fig.asp = 13200/8400, echo = FALSE, fig.cap = "**Simulated distributions of hospitalization rates at each week (boxplots), by empirical shape template (black line).** All simulated curves were based on linear trend filter fits using the $\\lambda_{min}$ trend filter penalty. Note that because each parameter used in the curve-generating function was drawn independently, simulated hospitalization curves based on an empirical shape template should have a similar shape (i.e., unimodal, bimodal) but may have different peak and/or cumulative hospitalization rates compared to the empirical template. Empirical data source: CDC, Emerging Infections Program (omitting 2009--2010 pandemic influenza season). Influenza season (empirical template): A) 2003--04, B) 2004--05, C) 2005--06, D) 2006--07, E) 2008--09, F) 2010--11, G) 2011--12, H) 2012--13, I) 2013--14, J) 2015--16, K) 2016--17, L) 2017--18, M) 2018--19. See Web Figure 2 for an illustration of individual simulated curves based on each shape template.", message = F}

knitr::include_graphics(get_asset("FIG", "Simulation-Curves-by-Template-Boxplot"))
```
<!-- \newpage -->
<!-- uncomment if needed; figure takes up whole page -->

```{r fig.id="simcompare", fig.width = 6.5, fig.asp = 5760/13200, fig.cap="**Empirical (*N* = 15) vs. simulated (*N* = 15,000) target distributions. $\\lambda_{min}$, trend filter penalty used in the main analysis; $\\lambda_{SE}$, trend filter penalty used in the alternate trend filter penalty sensitivity analysis.** A) Peak hospitalization rate per 100,000 population; B) Peak week; C) Cumulative hospitalizations per 100,000 population."}

knitr::include_graphics(get_asset("FIG", "TargetDists-Emp-vs-Sim"))
```

\newpage

```{r fig.id="ensemble-perf-main", fig.cap="**Cross-validated ensemble, component learner, and naive median prediction risks by week of simulated flu season and prediction target (main analysis).** A) Peak hospitalization rate per 100,000 population; B) Peak week; C) Cumulative hospitalizations per 100,000 population. Learners assigned zero weights by the metalearner are omitted. Pointranges display means and 95% confidence intervals for the ensemble prediction risks (natural log scale) in each week. Week 30 omitted from cumulative hospitalization rate to avoid distorting the y-axis; the true cumulative hospitalization rates are known to the algorithm at this point in the season.", fig.width = 6.5, fig.asp = 7740/21000}

knitr::include_graphics(get_asset("FIG", "Ensemble-Summary_All-Targets"))
```
\newpage

```{r fig.id="ensemble-perf-regscale", fig.cap="**Cross-validated ensemble prediction risks by week of simulated flu season, prediction target, and analysis.** A) Peak hospitalization rate per 100,000 population; B) Peak week; C) Cumulative hospitalizations per 100,00 population. Estimates plotted on the arithmetic scale for each prediction target. Pointranges display means and 95% confidence intervals for the ensemble prediction risks in each week. Main, main analysis; ATF, alternate trend filter penalty analysis; CS, component learner subset analysis.", fig.width = 6.5, fig.asp = 4800/13200}

knitr::include_graphics(get_asset("FIG", "Ensemble-Summary_All-Targets_Regular-Scale"))
```

\newpage
```{r fig.id="prosp-application", fig.cap = "**Prospective assessment of super learners trained on simulated data and observed data, as well as a naive prediction based on observed data.** Ensemble and discrete super learners are shown in each week for both super learner routines (i.e., trained on observed vs. simulated data). A) Peak rate, 2016--17 influenza season; B) Peak rate, 2017--18 influenza season; C) Peak rate, 2018--19 influenza season; D) Peak week, 2016--17 influenza season; E) Peak week, 2017--18 influenza season; F) Peak week, 2018--19 influenza season; G) Cumulative hospitalizations, 2016--17 inluenza season; H) Cumulative hospitalizations, 2017--18 influenza season; I) Cumulative hospitalizations, 2018--19 influenza season. Peak hospitalization rate and cumulative hospitalizations per 100,000 population.", fig.width = 5.5, fig.asp = 14400/14400}

knitr::include_graphics(get_asset("FIG", "Prospective-Observed-Application"))
```

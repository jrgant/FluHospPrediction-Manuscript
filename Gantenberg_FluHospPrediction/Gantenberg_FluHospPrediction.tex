% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended
% to minimize problems and delays during our production
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% Once your paper is accepted for publication,
% PLEASE REMOVE ALL TRACKED CHANGES in this file
% and leave only the final text of your manuscript.
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file.
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission.
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.
%
% Do not include text that is not math in the math environment.
%
% Please add line breaks to long display equations when possible in order to fit size of the column.
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
% \usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace}
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
% \bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother



% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}



\usepackage{booktabs}
\usepackage{threeparttable}



\usepackage{forarray}
\usepackage{xstring}
\newcommand{\getIndex}[2]{
  \ForEach{,}{\IfEq{#1}{\thislevelitem}{\number\thislevelcount\ExitForEach}{}}{#2}
}

\setcounter{secnumdepth}{0}

\newcommand{\getAff}[1]{
  \getIndex{#1}{}
}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Predicting seasonal influenza hospitalization using an ensemble super
learner: a simulation study} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
\\
\bigskip
\bigskip
\end{flushleft}
% Please keep the abstract below 300 words

% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step.
% Author Summary not valid for PLOS ONE submissions.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\textbf{Short Title:} Predicting influenza hospitalizations with a super
learner

Jason R. Gantenberg,(1,2) Kevin W. McConeghy,(2,3) Jon Steingrimsson,(4)
Chanelle J. Howe,(5) Robertus van Aalst,(6,7), Ayman Chit,(6), Andrew R.
Zullo(1,2,3)

\textbf{Corresponding author:} Jason R. Gantenberg (jrgant@brown.edu)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Department of Epidemiology, Brown University School of Public Health,
  121 S. Main St., Providence, RI, 02912
\item
  Department of Health Services, Policy and Practice, 121 S. Main St.,
  Providence, RI, 02912
\item
  Providence VA Medical Center, 830 Chalkstone Ave., Providence, RI,
  02908
\item
  Department of Biostatistics, Brown University School of Public Health,
  121 S. Main St., Providence, RI 02912
\item
  Center for Epidemiology and Environmental Health, Brown University
  School of Public Health, 121 S. Main St., Providence, RI, 02912
\item
  Sanofi Pasteur, Swiftwater, PA 18370
\item
  Department of Health Sciences, University of Groningen, University
  Medical Center Groningen, Groningen, the Netherlands
\end{enumerate}

\hypertarget{abstract}{%
\section{Abstract}\label{abstract}}

Seasonal influenza has a substantial impact on human health in the
United States and worldwide. Accurate forecasts of key features of
influenza epidemics and health care utilization can inform public health
response to outbreaks. To date, predicting seasonal influenza-related
hospitalizations has less frequently been the focus of forecasting
efforts. We conducted a simulation study to measure the performance of a
machine-learning algorithm (super learner) to predict three features of
a given seasonal influenza epidemic: peak hospitalization rate, the week
during which the hospitalization rate peaks, and the cumulative
hospitalizations. We fit a set of statistical models and the super
learner to 15,000 simulated influenza hospitalization curves to generate
weekly predictions for the targets of interest. In each week, we
compared the performance of the ensemble super learner (a weighted
combination of component model predictions), the discrete super learner
(best-performing individual statistical model), and a prediction based
on a simple mean of a given outcome (e.g., mean peak hospitalization
rate) across all simulated hospitalization curves. We found that the
general super learner algorithm---which subsumes both the selection of
the discrete super learner and estimation of the ensemble super
learner---was consistently capable of choosing individual statistical
models that improved predictions of the three seasonal prediction
targets over those of a simple mean prediction. However, the ensemble
super learner consistently exhibited higher prediction error than the
discrete super learners and often performed comparably to a simple mean
prediction. Given that national-level influenza hospitalization
forecasts based on empirical may rely on the small number of past flu
seasons recorded in surveillance data, the ensembles' underperformance
in a large sample of hypothetical influenza seasons suggests future work
should determine the conditions under which ensemble models can be
expected not to underperform the best-performing component model.

\hypertarget{author-summary}{%
\section{Author Summary}\label{author-summary}}

Influenza prediction is a still-maturing science, but notable
improvements have been made over the past decade in predicting
influenza-like activity. In addition, increasing focus on machine
learning approaches has provided hope that such approaches may further
improve predictive accuracy. Less work, however, has focused on
predicting influenza-related hospitalizations, and we sought to
investigate the potential for machine learning approaches to predict
several features of influenza hospitalization dynamics that could help
inform the allocation of public health efforts and resources during a
flu season. We used a machine learning approach, called \emph{super
learner}, which is capable of taking a large number of different
statistical models and either combining them into a single prediction
(the ensemble prediction) or choosing the best-performing model. Our
results suggest that ensemble super learner predictions may not perform
as well as the best-performing statistical model under certain
conditions. Future work should focus on investigating when ensemble
predictions based on machine learning would be expected to improve
predictive accuracy.

\hypertarget{meta}{%
\section{Meta}\label{meta}}

\textbf{Target journal:}

\begin{itemize}
\tightlist
\item
  \emph{PLoS Computational Biology}
\end{itemize}

\noindent \textbf{Section:}

\begin{itemize}
\tightlist
\item
  Epidemiology and Clinical/Translational Studies
\end{itemize}

\noindent \textbf{Potential editors:}

\begin{itemize}
\tightlist
\item
  Benjamin Althouse
\item
  Miles Davenport
\item
  Matthew Ferrari
\item
  Roger Kouyos
\item
  James Lloyd-Smith
\end{itemize}

\noindent \textbf{Potential reviewers:}

\begin{itemize}
\tightlist
\item
  Ryan J. Tibshirani (co-author on paper we use for curve simulation)
\item
  Logan C. Brooks (co-author on paper we use for curve simulation)
\item
  Roni Rosenfeld (co-author on paper we use for curve simulation)
\item
  Sherri Rose (expert in machine/super learning)
\item
  David A. Osthus (flu modeler whose work came highly recommended by
  Nick Reich)
\item
  Samrachana Adhikari (biostatistician and former lab member in the
  Carnegie Mellon group {[}with RJ Tibshirani)
\item
  Oleg Sofrygin (sl3 package author)
\item
  Nima Hejazi (sl3 package author)
\item
  Jeremy Coyle (sl3 package author)
\end{itemize}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Between 2010 and 2017, approximately 140,000--570,000 individuals were
hospitalized and 12,000--51,000 died annually due to seasonal influenza
in the United States {[}1{]}. The ability to predict the future burden
of influenza-related hospitalizations during a given influenza season
can help policymakers, public health officials, physicians, and other
stakeholders to allocate resources appropriately and prepare for
expected changes in hospitalization rates {[}2{]}. For example,
forecasts could provide hospitals with enough time to shift inpatient
hospital beds available for patients admitted with influenza-related
illnesses \textbf{{[}cite{]}}.

While influenza forecasting is a still-maturing science {[}3,4{]},
researchers have made considerable progress over the past decade in
improving the quality of and capacity for forecasting influenza-like
illness (ILI) {[}3{]}. Many of these improvements have been spurred by
the FluSight forecasting competitions sponsored by the Centers for
Disease Control and Prevention (CDC) since the 2013--14 influenza season
{[}3{]}. Many different types of models have been used to generate
forecasts, including statistical time series models {[}3,5{]}, Bayesian
methods {[}4,6{]}, and agent-based models {[}4{]}, among others.
Ensemble methods have emerged as a particularly promising approach to
improving further the accuracy and stability of epidemic predictions due
to ensembles' ability to combine predictions from multiple estimators in
a principled manner {[}2,7,8{]}.

Ensemble methods combine predictions generated by a set of component
models {[}7,9--11{]}. In some cases, ensembles aggreggate component
model predictions by weighting better predictions more highly in the
final ensemble prediction {[}7,8{]}, though other weighting criteria can
be applied {[}8{]}. One compelling rationale for using ensemble
predictions rests in their ability to borrow the strengths and avoid the
weaknesses of any single class of models used as inputs into the
ensemble {[}12{]}. This feature tends to lead not only to more accurate
predictions but to more stable ones that can be applied across a range
of scenarios {[}8{]}. The CDC's primary in-season outpatient ILI
forecasts are now based on an ensemble forecast generated by aggregating
predictions from a growing library of individual forecasts submitted by
research teams around the United States {[}3{]}.

To date, most work in influenza forecasting has focused on ILI
{[}3,5,6,13,14{]}, with considerably less work having been focused on
predicting influenza-related hospitalization rates {[}15{]}. Because
influenza-related hospitalization dynamics may evolve differently than
those of influenza transmission itself during a given flu season---for
instance, consequent hospitalizations usually lag influenza incidence by
one to two weeks \textbf{{[}citation needed{]}}---and because
hospitalization rates signal the severity of circulating flu strains
{[}5{]}, using ensemble methods to predict hospitalization rates could
be a value complement to existing efforts in ILI forecasting.

One ensemble machine learning method, dubbed ``super learner''
{[}12,16,17{]}, exhibits a number of desirable properties that suggest
it may be a powerful tool for predicting flu hospitalizations. First,
its developers have demonstrated that, asymptotically, the super learner
is an oracle estimator, performing as well as the best-fitting component
model {[}16,17{]}, where the best-fitting component model performs as
well as the true data-generating model {[}16{]} \textbf{{[}read and cite
van der Laan \& Dudoit, 2003?{]}}. Second, this oracle property has been
found to apply approximately in finite samples {[}12,16,17{]}. Several
packages have been developed to implement the super learner algorithm
{[}18,19{]}, providing researchers easy access to a large library of
component models and a means to efficiently evaluate their predictive
performance {[}19{]}.

The super learner requires a sufficient, though not exorbitant, sample
size in order to implement embedded procedures (e.g., cross-validation)
{[}12{]}. However, the 15 empirical flu seasons available are not
sufficient in number to develop the super learner.

In this study, we trained an ensemble super learner on 15,000 simulated
influenza hospitalization curves to generate predictions for three
national-level seasonal target parameters chosen based on the targets
specified in CDC forecasting competitions {[}20{]}. We conducted the
procedure independently at each of the 30 weeks in a typical flu season,
and we compared the predictive performance of the ensemble super learner
against those of the best-performing individual statistical model and a
prediction based on the mean of the outcome across the simulated
hospitalization curves.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{empirical-data}{%
\subsection{Empirical data}\label{empirical-data}}

We downloaded publicly available surveillance data on seasonal
influenza-related hospitalizations from the CDC's FluView Interactive
dashboard {[}21{]}. Specifically, we included data from the Emerging
Infections Program (EIP) beginning with the 2003--2004 season and ending
with the 2018--2019 season, omitting the 2009-2010 pandemic influenza
year. The EIP contains data on influenza-related hospitalizations in
California, Colorado, Connecticut, Georgia, Maryland, Minnesota, New
Mexico, New York, Oregon, and Tennessee {[}21{]}. Since the 2009--2010
season the FluSurv Network has included between 3 and 6 states in
addition to those represented in the EIP data, depending on the year
{[}21{]}. In order to maintain consistency within the empirical data,
and to increase the number of flu seasons available to inform curve
simulation, we did not include data from states outside the EIP.

Typically, the CDC releases data for epiweeks 40--53 and 1--17,
corresponding approximately to October through April of the next year
{[}21,22{]}. Epiweeks are simply integers assigned to each week of the
calendar year, where epiweek 1 is the first week of the year and
epiweeks 52/53 the last {[}22{]}. We renumbered the epiweeks 1--30,
omitting epiweek 53, as only three seasons had influenza hospitalization
data recorded in this week. When we refer subsequently to a \emph{flu
season}, we refer to this 30-week time period.

\hypertarget{prediction-targets}{%
\subsection{Prediction targets}\label{prediction-targets}}

Following from the CDC's Flu Sight challenge {[}20{]}, we focused on
three season-level prediction targets:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Peak rate}, defined as the highest weekly rate of
  influenza-related hospitalizations throughout the course of a flu
  season (per 100,000 population);
\item
  \emph{Peak week}, defined as the week during which this peak rate
  occurred; and
\item
  \emph{Cumulative hospitalizations}, defined as the cumulative
  influenza-related hospitalization rate over the 30 weeks of the flu
  season (per 100,000 population).
\end{enumerate}

Fifteen empirical observations were available for each prediction
target, corresponding to the flu seasons recorded in the CDC's
surveillance data (Table 1).

\begin{table}

\caption{\label{tab:target-table}Empirical distributions of peak hospitalization rate, peak week, and cumulative hospitalization rate in the United States, 2003--2019.}
\centering
\begin{threeparttable}
\begin{tabular}[t]{lrrr}
\toprule
Season & Peak rate & Peak week & Cumulative hospitalizations\\
\midrule
2003-04 & 5.4 & 12.0 & 30.6\\
2004-05 & 1.3 & 19.0 & 13.5\\
2005-06 & 1.1 & 23.0 & 10.9\\
2006-07 & 0.5 & 20.0 & 6.2\\
2007-08 & 2.3 & 21.0 & 18.3\\
\addlinespace
2008-09 & 0.8 & 20.5 & 7.8\\
2009-10 & 3.8 & 3.0 & 29.3\\
2010-11 & 2.1 & 21.0 & 20.1\\
2011-12 & 1.1 & 24.0 & 8.6\\
2012-13 & 5.3 & 14.0 & 43.0\\
\addlinespace
2013-14 & 3.9 & 14.5 & 35.2\\
2014-15 & 9.0 & 13.0 & 64.2\\
2015-16 & 4.2 & 23.0 & 31.5\\
2016-17 & 5.2 & 21.0 & 62.5\\
2017-18 & 10.3 & 14.0 & 102.5\\
\addlinespace
2018-19 & 5.4 & 24.0 & 64.9\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item Source: CDC FluView (Emerging Infections Program). Peak rate and cumulative rate expressed per 100,000 population. Pandemic influenza season 2009--2010 omitted.
\end{tablenotes}
\end{threeparttable}
\end{table}

\hypertarget{hospitalization-curve-simulation}{%
\subsection{Hospitalization curve
simulation}\label{hospitalization-curve-simulation}}

To simulate a distribution of seasonal influenza hospitalization curves,
we adapted an approach by Brooks et al.~originally used to predict
influenza-like illness {[}6{]}. First, we fitted a linear trend filter
{[}23,24{]} to the 15 observed influenza hospitalization curves from the
EIP using the \texttt{glmgen} package in R {[}25{]}. The \texttt{glmgen}
linear trend filter is a penalized method that fits a piecewise linear
function to a time series, testing 50 values of the penalty
(\(\lambda\)) by default {[}25{]}. We used these fits as templates for
the simulated influenza hospitalization curves. In all cases, we
selected the fit that used the 25th \(\lambda\) value tested (S1 Fig)
based on visual inspection of the overall distribution of simulated
curves.

Next, we used the resulting 15 trend filter fits into a modified version
of the curve generation scheme described in Brooks et al. {[}6{]}.
Specifically, the original scheme was developed to simulate potential
ILI curves, and therefore, the curve-generating function included a term
for season-onset, representing the threshold ILI at which the flu season
would be considered to have started {[}6{]}. This onset is an additional
prediction target in FluSight ILI forecasting because ILI surveillance
is conducted year-round. On the contrary, such a threshold does not
exist for influenza-related hospitalizations. Rather, hospitalization
rates are recorded starting in epiweek 40 through the end of the flu
season. Save for one season, hospitalization rates start at a rate of 0
per 100,000 {[}21{]}. We modified the curve-generating function to omit
the seasonal onset term and added a transformation that constrains
simulated hospitalization rates to be greater than or equal to 0.
Otherwise, our notation borrows and follows closely from theirs.

Briefly, Brooks et al.~conceptualize as seasonal influenza curve as some
function plus noise {[}6{]}. Adapted to the hospitalization case, the
hospitalization rate (\(y^s_i\)) in season \(s\) and week \(i\) is given
by

\[y^s_i = f^s(i) + \epsilon^s_i, \epsilon \sim N(0, \tau^s),\]

where \(f^s(i)\) is a hospitalization rate and \(\epsilon^s_i\) is
normally distributed error term with mean 0 and variance \(\tau^s\).

For each empirical season \(s\), we use its linear trend filter fit and
average the squared residuals over \(i\) to estimate \(\tau^s\):

\[\left( \hat{\tau}^s \right)^2 = \genfrac{}{}{0pt}{}{\text{avg}}{i} \left[ y^s_i - \hat{f}^s (i) \right]^2.\]
For each simulated curve, each of the following parameters is sampled
randomly and independently of one another:

\[\langle f, \sigma, \nu, \theta, \mu \rangle,\]

where \(f\) denotes a randomly selected vector of estimated
hospitalization rates based on a linear trend filter fit to empirical
season \(s\), \(\sigma\) denotes the squared error of a linear trend
filter fit to randomly sampled season \(s'\) and averaged across all
weeks, \(\nu\) denotes a random uniform draw from the range {[}0.75,
1.25{]}, \(\theta\) denotes a random draw from a uniform distribution
bounded by the minimum and maximum peak hospitalization rates from the
15 trend filter fits, and \(\mu\) denotes a random draw from a uniform
distribution bounded by the minimum and maximum peak weeks based on the
15 trend filter fits (Table 2).

\begin{table}

\caption{\label{tab:sim-param-input-table}Input parameters to the influenza hospitalization curve generating function.}
\centering
\begin{threeparttable}
\begin{tabular}[t]{ll}
\toprule
Parameter & Description\\
\midrule
Shape & $f \sim U \{ \hat{f} : \text{historical season } s \}$\\
Noise & $\sigma \sim U \{\hat{\tau}^{s'} \text{ : historical season } s' \}$\\
Pacing & $\nu \sim U[0.75, 1.25]$, stretches the curve around the peak week\\
Peak height & $\theta \sim U\left[\theta_{min} , \theta_{max}\right]$\\
Peak week & $\mu \sim U[\mu_{min}, \mu_{max}]$\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item The noise parameter for each simulation is drawn separately and may come from a season different than the season used as the shape.
\end{tablenotes}
\end{threeparttable}
\end{table}

The generating function for hospitalization rate in week \(i\) of
simulated season \(sim\) therefore is given by:

\[f^{sim} (i) = \frac{\theta}{\text{max}_j f(j)} \left[ f \left( \frac{i - \mu}{v} + \genfrac{}{}{0pt}{}{\text{arg max }{j}}{f(j)} \right) \right] + \epsilon_i, \epsilon_i \sim N(0, \hat{\tau}^{s'}),\]

where \(f(j)\) now denotes the vector of fitted hospitalization rates
from the linear trend filter fit (randomly selected shape \(f\)) and
\(j\) the integer week for which we want to retrieve the prediction from
this fit (equal to \(i\)). \(max_j f(j)\) denotes the peak
hospitalization rate from the selected shape, whereas
\(\genfrac{}{}{0pt}{}{\text{arg max } {j}}{f(j)}\) denotes the week in
which this peak occurred. The parameters \(\theta\), \(\mu\), and
\(\nu\) follow from their prior description. Finally, we introduce noise
for each simulated weekly hospitalization rate based on the selected
estimate of \(\tau^2\).

We impose a lower bound of 0 on the hospitalization rate via the
following transformation of \(\hat{y}^s_i\), denoted below as
\(\hat{z}^s_i\):

\[\hat{z}^s_i = 0.5 \bigg( | \hat{y}^s_i | + \hat{y}^s_i \bigg).\]

This transformation effectively preserves positive simulated
hospitalization rates and sets negative hospitalization rates to 0.
Negative hospitalization rates may be generated at the tails of a given
season, when hospitalization rates are generally low, because the error
is normally distributed in all weeks. This transformation was imposed on
20.5\% of the simulated weekly hospitalization rates simulated.

In all, we simulated 15,000 curves (S2 Fig). These simulated curves may
be thought of as a plausible distribution of hypothetical flu seasons
that could be observed in principle but perhaps have not yet been
realized {[}6{]}.

\begin{figure}
\includegraphics[width=0.7\linewidth]{C:/Users/jason/Documents/Github/FluHospPrediction/results/paper_output/fig01} \caption{Empirical (top) and 15 randomly selected simulated (bottom) hospitalization curves. Empirical source: CDC, Emerging Infections Program (omitting 2009--2010 pandemic influenza season).}\label{fig:fig01-empsim-compare}
\end{figure}

\begin{figure}
\includegraphics[width=1\linewidth]{C:/Users/jason/Documents/Github/FluHospPrediction/results/paper_output/fig02} \caption{Empirical (N = 15) vs. simulated (N = 15,000) target distributions.}\label{fig:fig02-simcompare}
\end{figure}

\hypertarget{super-learner}{%
\subsection{Super learner}\label{super-learner}}

The super learner is a loss-based estimation algorithm, meaning
predictive performance is estimated by defining a loss function that
targets a specific prediction target of interest {[}12,16,17{]}. For
instance, setting the loss function to be the squared error loss means
that model accuracy is optimized to minimize the squared error, and
hence, the implicit prediction target is the conditional mean. The super
learner takes as inputs training data, a \emph{library} of component
prediction algorithms (called \emph{learners} but which we refer to as
component models), and a desired loss function. See Naimi \& Balzer for
an accessible introduction {[}27{]}.

For each prediction target (peak rate, peak week, and cumulative
hospitalizations), we used the same library of component models and the
\(L_1\) absolute error loss function, which targets the conditional
median of the prediction target {[}17{]}. The absolute error loss is
defined as the absolute difference between the predicted and true
observations {[}17{]}.

We trained the super learner on the simulated hospitalization curves
using the \texttt{sl3} package in R {[}19{]}. We conducted the entire
super learner procedure sequentially at each week \emph{i}, proposing
the following covariates (i.e., independent predictors) to each learner:

\begin{itemize}
\tightlist
\item
  hospitalization rate per 100,000 population in week \emph{i},
\item
  cumulative hospitalizations per 100,000 population through week
  \emph{i},
\item
  hospitalization rates from all weeks prior to week \emph{i},
\item
  cumulative hospitalizations from all weeks prior to week \emph{i},
\item
  the difference between the hospitalization rate in week \emph{i} and
  the hospitalization rate in each prior week up to 5 weeks in the past,
\item
  the difference between the cumulative hospitalization rate in week
  \emph{i} and the cumulative hospitalization rate in each prior week up
  to 5 weeks in the past
\item
  product terms between the hospitalization rate in week \emph{i} and
  each of the cumulative hospitalization differences, and
\item
  product terms between the cumulative hospitalization rate in week
  \emph{i} and each of the hospitalization rate differences.
\end{itemize}

The predictive accuracy for the component models and the ensemble
learner using 15-fold cross-validation {[}28{]}. Briefly, \emph{V}-fold
cross-validation is an iterative sample-splitting procedure in which
each observation is assigned to a group, called a fold, and where each
fold serves as the validation set for a model trained on the rest of the
sample {[}27,28{]}. This procedure maximizes the amount of data
available for model-fitting, by avoiding the need for a hold-out
validation sample, while at the same time avoiding overfitting
{[}12,27{]}. The predictive accuracy of each learning algorithm is
evaluated as a \emph{risk}, in our case defined as the average absolute
error between a learner's prediction and the true observation. These
risks are estimated in each fold when it is assigned to be the
validation set as part of the cross-validation procedure. Therefore, in
the current context, the \emph{cross-validated risk} refers to the
average risk across all 15 folds {[}12{]}.

We repeated the following procedure for each prediction target:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Create 30 data sets, one for each week of the flu season, containing
  the outcome variables (peak rate, peak week, and cumulative
  hospitalizations) and the covariates described previously.
\item
  Assign each observation to a fold for use in cross-validation.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    We assigned all simulated curves based on a common observed season
    shape to the same fold, in order to account for the dependence
    between these simulated curves. We did not account for potential
    dependencies due to repeated sampling of the error term, as we
    believed the season shape was more consequential with respect to
    inducing dependence between simulated curves.
  \end{enumerate}
\item
  Run the super learner algorithm:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Train each learner on the data set using 15-fold cross-validation
    and calculate its cross-validated risk as the average risk across
    the 15 validation sets.
  \item
    Generate predictions with each component learner on the full data
    set.
  \item
    Regress the current prediction target on each learner's predictions
    in the full sample (\emph{N} = 15,000) using a non-negative least
    squares model constrained to produce coefficients in the range
    {[}0,1{]} and which sum to 1 {[}27{]}. These constraints are not
    necessary but are theoretically expected to improve the stability of
    the ensemble estimator {[}12{]}. This model is referred to as the
    \emph{metalearner} {[}19{]}.
  \item
    Calculate the cross-validated risk for the ensemble super learner as
    in step 4a.
  \end{enumerate}
\end{enumerate}

In all, we trained \textbf{{[}62{]}} component models in for each
prediction target and week of the season, including variations on tuning
and input parameters for the various learners (Table 2).

\hypertarget{component-models}{%
\subsection{Component models}\label{component-models}}

We used the same library of component models to build the super learners
for all three prediction targets (Table 3). The component models
included a standard linear regression model, generalized additive models
{[}29{]}, loss-based regression models (e.g., lasso, ridge) {[}30{]},
random forests {[}31{]}, support vector regression {[}9{]}, neural
networks {[}9{]}, loess {[}28{]}, and polynomial multivariate adaptive
regression splines (polyMARS) {[}32{]}. Theory and finite-sample
demonstrations support the idea that the super learner should benefit
from being provided with a large variety of models, even if some of
those models perform poorly in the given data {[}12{]}. Therefore, we
specified the component learners and tuning parameter sets to attempt to
probe a large portion of the component model space.

In all cases the learning algorithms incorporating these various
component models were implemented using functions provided in the
\texttt{sl3} {[}19{]} and \texttt{SuperLearner} {[}18{]} packages in R.

\begin{table}

\caption{\label{tab:candmodels-tuning-table}Component models and tuning parameters.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{2.1in}>{\raggedright\arraybackslash}p{2.1in}l}
\toprule
Model & Tuning parameters & R package\\
\midrule
\rowcolor{gray!6}  Linear regression & Variables screened for inclusion first using a cross-validated lasso procedure to omit variables with zero-valued coefficients. & base\\
Random forest (regression trees) & number of trees = [50, 100, 200, 500], terminal node sizes = [3, 5, 10] & randomForest\\
\rowcolor{gray!6}  Generalized additive model & gamma penalty = [1, 2, 3, 4, 5] & gam\\
Support vector regression & kernel = [radial, polynomial], degree = [1, 2, 3] applied only to polynomial & svm\\
\rowcolor{gray!6}  Loss-based regression & penalty = [lasso, ridge] & glmnet\\
\addlinespace
Elastic net & alpha penalty = [0.25, 0.5, 0.75] & glmnet\\
\rowcolor{gray!6}  Neural network & number of nodes in hidden layer = [5, 10, 25, 50, 75, 100], decay = [0, 0.005, 0.1, 0.2, 0.4] & nnet\\
Loess & span = [0.25, 0.5, 0.75, 1] & base\\
\rowcolor{gray!6}  Polynomial multivariate adaptive regression spline & gcv penalty = [2, 4, 6, 8, 10] & polspline\\
\bottomrule
\multicolumn{3}{l}{For the random forest and neural network learners, all combinations of the tuning parameters shown were proposed.}\\
\end{tabular}}
\end{table}

\hypertarget{linear-regression}{%
\subsubsection{Linear regression}\label{linear-regression}}

For each week of the flu season, we specified a linear regression model
that included covariates chosen by a lasso-based screening algorithm
{[}18,33{]} in which covariates with non-zero coefficients were
identified using 10-fold cross validation and then entered into the
linear regression. The linear model was specified using the
\texttt{glm()} function in R with a Gaussian error distribution and
identity link.

\hypertarget{loss-based-regression}{%
\subsubsection{Loss-based regression}\label{loss-based-regression}}

We implemented several loss-based regression learners using the
\texttt{glmnet} package in R {[}33{]}. These learners included a learner
with the standard lasso penalty {[}30{]}, a learner with the ridge
penalty {[}30{]}, and three versions of the elastic net model {[}cite{]}
using different \(\alpha\) penalties (Table 3). The elastic net model
combines lasso and ridge penalties within the same model {[}citation{]}.

\hypertarget{random-forests}{%
\subsubsection{Random forests}\label{random-forests}}

We implemented random forest algorithms {[}31{]} using the
\texttt{randomForest} package {[}34{]}, tuning parameters governing the
number of regression trees and minimum observations allowed in terminal
nodes (Table 3).

\hypertarget{support-vector-regression}{%
\subsubsection{Support vector
regression}\label{support-vector-regression}}

We implemented several support vector regressions {[}9{]} using the
\texttt{e1071} package {[}35{]}---one using a radial kernel and three
others using a polynomial kernel of differing degrees (Table 3).

\hypertarget{neural-networks}{%
\subsubsection{Neural networks}\label{neural-networks}}

We implemented several neural networks {[}9{]} using the \texttt{nnet}
package {[}36{]}. All learners proposed contained a single hidden layer,
altering tuning parameters governing the number of nodes in this hidden
layer and the magnitude of decay applied to node weights (Table 3).

\hypertarget{loess}{%
\subsubsection{Loess}\label{loess}}

We submitted several loess-based learners as implemented in base R,
varying the smoothing kernel span (Table 3).

\hypertarget{polynomial-multivariate-adaptive-regression-splines}{%
\subsubsection{Polynomial multivariate adaptive regression
splines}\label{polynomial-multivariate-adaptive-regression-splines}}

We submitted several polyMARS models using the \texttt{polspline}
package {[}32{]}, varying the generalized cross-validation value, larger
values of which produce smaller models {[}32{]}.

\hypertarget{technical-details}{%
\subsection{Technical details}\label{technical-details}}

The super learner algorithm was implemented on a high-performance
computing cluster housed at the Center for Computation and Visualization
at Brown University. Each run was submitted to a large-memory node and
parallelized across 32 central processing units based on the native
functionality provided in \texttt{sl3} {[}19{]} via the \texttt{delayed}
{[}37{]} and \texttt{future} {[}38{]} packages in R.

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{peak-rate}{%
\subsection{Peak rate}\label{peak-rate}}

For the peak rate prediction target, the ensemble super learner produced
risk estimates similar to the simple mean prediction and generally worse
than the discrete super learner (Table 4). Throughout the flu season,
the discrete super learner (best-performing component model)
consistently exhibited a lower average risk than a prediction based on
the mean, indicating better predictive ability. Furthermore, the
discrete super learner's cross-validated risk generally decreased as the
season progressed. In each week a different discrete super learner was
selected, with neural networks and penalized regressions being selected
in the first several weeks, support vector regressions being selected
toward the middle of the flu season, and polyMARS being selected more
frequently toward the end of the season.

\begin{table}

\caption{\label{tab:tab04-peakrate-risk-table}Cross-validated risks for the ensemble super learner, discrete super learner, and simple mean predictions of peak hospitalization rate, by week of influenza season. Estimates presented as mean risk (standard error).}
\centering
\resizebox{\linewidth}{!}{
\begin{threeparttable}
\begin{tabular}[t]{rllll}
\toprule
Week & Super Learner & Discrete SL Risk & Mean & Discrete SL\\
\midrule
1 & 2.83 (0.013) & 2.68 (0.013) & 2.75 (0.012) & Lrnr\_glmnet\_NULL\_deviance\_10\_1\_100\_TRUE\_TRUE\\
2 & 2.97 (0.022) & 2.20 (0.015) & 2.75 (0.012) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_10\_0.005\\
3 & 2.93 (0.025) & 1.77 (0.014) & 2.75 (0.012) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_5\_0.005\\
4 & 2.57 (0.020) & 1.85 (0.019) & 2.75 (0.012) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_50\_0.2\\
5 & 2.97 (0.032) & 2.50 (0.028) & 2.75 (0.012) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_5\_0.005\\
\addlinespace
6 & 2.59 (0.018) & 2.36 (0.019) & 2.75 (0.012) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_10\_0.2\\
7 & 2.74 (0.026) & 2.19 (0.014) & 2.75 (0.012) & Lrnr\_glmnet\_NULL\_deviance\_10\_0.5\_100\_TRUE\\
8 & 2.97 (0.017) & 2.46 (0.021) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
9 & 2.98 (0.033) & 2.40 (0.014) & 2.75 (0.012) & Lrnr\_randomForest\_100\_TRUE\_10\_NULL\_FALSE\\
10 & 3.00 (0.022) & 2.20 (0.022) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
\addlinespace
11 & 3.26 (0.015) & 2.27 (0.017) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
12 & 3.11 (0.018) & 2.32 (0.024) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
13 & 2.91 (0.017) & 1.99 (0.026) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
14 & 3.51 (0.016) & 1.97 (0.025) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
15 & 3.29 (0.012) & 2.11 (0.025) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
\addlinespace
16 & 3.24 (0.019) & 2.16 (0.025) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
17 & 3.36 (0.015) & 2.11 (0.021) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
18 & 3.14 (0.029) & 2.09 (0.030) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
19 & 2.71 (0.030) & 1.33 (0.025) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
20 & 2.42 (0.019) & 1.22 (0.022) & 2.75 (0.012) & Lrnr\_polspline\_5\_2, Lrnr\_polspline\_5\_4\\
\addlinespace
21 & 2.73 (0.018) & 1.06 (0.024) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
22 & 2.64 (0.021) & 0.86 (0.025) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
23 & 2.56 (0.020) & 0.44 (0.020) & 2.75 (0.012) & Lrnr\_polspline\_5\_2\\
24 & 2.75 (0.022) & 0.68 (0.021) & 2.75 (0.012) & Lrnr\_glmnet\_NULL\_deviance\_10\_1\_100\_TRUE\_TRUE\\
25 & 2.64 (0.017) & 0.74 (0.020) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
\addlinespace
26 & 2.73 (0.017) & 0.78 (0.021) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
27 & 2.69 (0.015) & 0.67 (0.022) & 2.75 (0.012) & Lrnr\_polspline\_5\_2\\
28 & 2.63 (0.017) & 0.63 (0.022) & 2.75 (0.012) & Lrnr\_polspline\_5\_2\\
29 & 2.41 (0.016) & 0.63 (0.022) & 2.75 (0.012) & Lrnr\_polspline\_5\_2\\
30 & 2.52 (0.018) & 0.72 (0.021) & 2.75 (0.012) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item The ensemble super learner (SuperLearner) is the prediction generated as a weighted combination of component model predictions. The discrete super learner (Discrete SL) is the best-performing component model. The mean (Mean) is the average peak hospitalization rate per 100,000 population across the simulated hospitalization curves. The absolute error loss function used to optimized the super learner targets the median peak hospitalization rate.
\end{tablenotes}
\end{threeparttable}}
\end{table}

\hypertarget{peak-week}{%
\subsection{Peak week}\label{peak-week}}

For the peak week prediction target, the ensemble superlearner generally
produced risk estimates close to those of the peak week predictions
based on a simple mean (Table 5). The discrete super learner
consistently exhibited lower mean risk than both the ensemble super
learner and predictions based on the simple mean. As with peak rate, the
discrete super learner tended to perform better as the flu season
progressed. Over the first 6 weeks, some form of neural network was
chosen as the discrete super learner in 5 of those weeks. Throughout the
rest of the season, various versions of the random forest algorithms
predominated the list of discrete super learners.

\begin{table}

\caption{\label{tab:tab05-peakweek-risk-table}Cross-validated risks for the ensemble super learner, discrete super learner, and mean predictions of the week in which the peak hospitalization rate occurs, by week of influenza season. Estimates presented as mean risk (standard error).}
\centering
\resizebox{\linewidth}{!}{
\begin{threeparttable}
\begin{tabular}[t]{rllll}
\toprule
Week & Super Learner & Discrete SL Risk & Mean & Discrete SL\\
\midrule
1 & 4.02 (0.027) & 3.34 (0.037) & 3.92 (0.027) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_50\_0.4\\
2 & 4.49 (0.032) & 3.50 (0.031) & 3.92 (0.027) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_25\_0.2\\
3 & 4.59 (0.024) & 2.58 (0.027) & 3.92 (0.027) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_5\_0.005\\
4 & 4.63 (0.025) & 2.80 (0.021) & 3.92 (0.027) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_100\_0.005\\
5 & 4.45 (0.030) & 2.43 (0.026) & 3.92 (0.027) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_10\_0.005\\
\addlinespace
6 & 3.98 (0.022) & 2.82 (0.018) & 3.92 (0.027) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_5\_0.005\\
7 & 4.45 (0.033) & 2.81 (0.024) & 3.92 (0.027) & Lrnr\_randomForest\_50\_TRUE\_3\_NULL\_FALSE\\
8 & 4.94 (0.042) & 3.23 (0.024) & 3.92 (0.027) & Lrnr\_randomForest\_50\_TRUE\_5\_NULL\_FALSE\\
9 & 3.80 (0.022) & 2.99 (0.026) & 3.92 (0.027) & Lrnr\_randomForest\_50\_TRUE\_10\_NULL\_FALSE\\
10 & 4.15 (0.023) & 3.04 (0.026) & 3.92 (0.027) & Lrnr\_randomForest\_500\_TRUE\_10\_NULL\_FALSE\\
\addlinespace
11 & 4.10 (0.038) & 2.60 (0.023) & 3.92 (0.027) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
12 & 3.97 (0.024) & 2.18 (0.034) & 3.92 (0.027) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_10\_0.4\\
13 & 3.72 (0.020) & 2.23 (0.025) & 3.92 (0.027) & Lrnr\_randomForest\_100\_TRUE\_10\_NULL\_FALSE\\
14 & 3.47 (0.021) & 2.29 (0.024) & 3.92 (0.027) & Lrnr\_polspline\_5\_2\\
15 & 3.66 (0.020) & 1.95 (0.028) & 3.92 (0.027) & Lrnr\_randomForest\_200\_TRUE\_10\_NULL\_FALSE\\
\addlinespace
16 & 3.97 (0.026) & 1.89 (0.027) & 3.92 (0.027) & Lrnr\_randomForest\_200\_TRUE\_5\_NULL\_FALSE\\
17 & 4.43 (0.036) & 1.85 (0.022) & 3.92 (0.027) & Lrnr\_randomForest\_100\_TRUE\_5\_NULL\_FALSE\\
18 & 3.90 (0.028) & 1.79 (0.023) & 3.92 (0.027) & Lrnr\_randomForest\_100\_TRUE\_3\_NULL\_FALSE\\
19 & 4.26 (0.032) & 1.71 (0.024) & 3.92 (0.027) & Lrnr\_randomForest\_50\_TRUE\_3\_NULL\_FALSE\\
20 & 3.97 (0.027) & 2.15 (0.023) & 3.92 (0.027) & Lrnr\_randomForest\_500\_TRUE\_10\_NULL\_FALSE\\
\addlinespace
21 & 4.14 (0.026) & 1.94 (0.023) & 3.92 (0.027) & Lrnr\_randomForest\_50\_TRUE\_10\_NULL\_FALSE\\
22 & 3.78 (0.022) & 1.78 (0.021) & 3.92 (0.027) & Lrnr\_randomForest\_100\_TRUE\_10\_NULL\_FALSE\\
23 & 3.94 (0.020) & 1.92 (0.022) & 3.92 (0.027) & Lrnr\_randomForest\_50\_TRUE\_10\_NULL\_FALSE\\
24 & 4.08 (0.032) & 1.84 (0.025) & 3.92 (0.027) & Lrnr\_randomForest\_50\_TRUE\_3\_NULL\_FALSE\\
25 & 3.91 (0.020) & 1.98 (0.022) & 3.92 (0.027) & Lrnr\_randomForest\_200\_TRUE\_3\_NULL\_FALSE\\
\addlinespace
26 & 3.91 (0.023) & 1.98 (0.022) & 3.92 (0.027) & Lrnr\_randomForest\_50\_TRUE\_3\_NULL\_FALSE\\
27 & 4.12 (0.025) & 2.06 (0.022) & 3.92 (0.027) & Lrnr\_randomForest\_100\_TRUE\_3\_NULL\_FALSE\\
28 & 3.60 (0.021) & 2.14 (0.019) & 3.92 (0.027) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
29 & 4.27 (0.023) & 2.00 (0.023) & 3.92 (0.027) & Lrnr\_randomForest\_50\_TRUE\_10\_NULL\_FALSE\\
30 & 3.83 (0.022) & 1.86 (0.023) & 3.92 (0.027) & Lrnr\_randomForest\_50\_TRUE\_3\_NULL\_FALSE\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item The ensemble super learner (SuperLearner) is the prediction generated as a weighted combination of component model predictions. The discrete super learner (Discrete SL) is the best-performing component model. The mean (Mean) is the average week during which the peak hospitalization rate occurred across the simulated hospitalization curves. The absolute error loss function used to optimized the super learner targets the median peak hospitalization rate.
\end{tablenotes}
\end{threeparttable}}
\end{table}

\hypertarget{cumulative-hospitalizations}{%
\subsection{Cumulative
hospitalizations}\label{cumulative-hospitalizations}}

For the cumulative hospitalizations prediction target, the ensemble
super learner produced risk estimates generally near to those based on a
prediction based on the simple mean (Table 6). Again, the discrete super
learner consistently outperformed both the ensemble super learner and
the simple mean prediction, improving substantially as the flu season
progressed. Some form of support vector regression was chosen most
frequently as the discrete super learner, with penalized regressions,
polyMARS, and neural networks among the other models chosen in at least
one of the weeks.

\begin{table}

\caption{\label{tab:tab06-cumhosp-risk-table}Cross-validated risks for the ensemble super learner, discrete super learner, and mean predictions of the cumulative hospitalization rate, by week of influenza season. Estimates presented as mean risk (standard error).}
\centering
\resizebox{\linewidth}{!}{
\begin{threeparttable}
\begin{tabular}[t]{rllll}
\toprule
Week & Super Learner & Discrete SL Risk & Mean & Discrete SL\\
\midrule
1 & 33.4 (1.01) & 28.4 (1.17) & 33.4 (1.01) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_75\_0.2\\
2 & 36.3 (1.65) & 23.2 (1.62) & 33.4 (1.01) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
3 & 30.6 (1.411) & 24.6 (0.797) & 33.4 (1.010) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_10\_0.2\\
4 & 28.6 (1.708) & 22.7 (0.861) & 33.4 (1.010) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_5\_0\\
5 & 38.4 (1.035) & 24.4 (0.966) & 33.4 (1.010) & Lrnr\_polspline\_5\_4\\
\addlinespace
6 & 32.5 (1.339) & 26.3 (0.584) & 33.4 (1.010) & Lrnr\_pkg\_SuperLearner\_SL.nnet\_100\_0.005\\
7 & 32.0 (1.88) & 25.0 (1.20) & 33.4 (1.01) & Lrnr\_glmnet\_NULL\_deviance\_10\_1\_100\_TRUE\_TRUE\\
8 & 32.6 (0.979) & 25.7 (1.627) & 33.4 (1.010) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
9 & 32.6 (1.25) & 24.1 (1.02) & 33.4 (1.01) & Lrnr\_glmnet\_NULL\_deviance\_10\_1\_100\_TRUE\_TRUE\\
10 & 32.4 (0.869) & 22.4 (1.416) & 33.4 (1.010) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
\addlinespace
11 & 36.1 (0.751) & 23.8 (1.333) & 33.4 (1.010) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
12 & 32.0 (0.993) & 22.9 (1.393) & 33.4 (1.010) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
13 & 30.6 (1.32) & 24.4 (1.54) & 33.4 (1.01) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
14 & 36.8 (0.902) & 23.7 (1.535) & 33.4 (1.010) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
15 & 36.4 (0.925) & 21.7 (1.523) & 33.4 (1.010) & Lrnr\_svm\_TRUE\_NULL\_radial\_TRUE\_FALSE\\
\addlinespace
16 & 35.8 (1.01) & 17.2 (1.31) & 33.4 (1.01) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
17 & 31.0 (0.993) & 20.3 (1.238) & 33.4 (1.010) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
18 & 35.3 (0.949) & 20.6 (1.448) & 33.4 (1.010) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
19 & 33.6 (1.09) & 15.8 (1.30) & 33.4 (1.01) & Lrnr\_glmnet\_NULL\_deviance\_10\_0.75\_100\_TRUE\\
20 & 31.2 (1.28) & 15.9 (1.35) & 33.4 (1.01) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
\addlinespace
21 & 36.34 (0.782) & 9.82 (1.031) & 33.37 (1.010) & Lrnr\_svm\_TRUE\_NULL\_polynomial\_TRUE\_FALSE\_1\\
22 & 33.49 (0.915) & 4.44 (0.862) & 33.37 (1.010) & Lrnr\_glmnet\_NULL\_deviance\_10\_1\_100\_TRUE\_TRUE\\
23 & 29.29 (0.924) & 2.89 (0.865) & 33.37 (1.010) & Lrnr\_polspline\_5\_2\\
24 & 30.50 (1.091) & 2.87 (0.886) & 33.37 (1.010) & Lrnr\_glmnet\_NULL\_deviance\_10\_1\_100\_TRUE\_TRUE\\
25 & 32.80 (1.027) & 2.46 (0.874) & 33.37 (1.010) & Lrnr\_glmnet\_NULL\_deviance\_10\_1\_100\_TRUE\_TRUE\\
\addlinespace
26 & 36.57 (0.828) & 1.93 (0.851) & 33.37 (1.010) & Lrnr\_polspline\_5\_4\\
27 & 31.29 (1.10) & 0.83 (0.85) & 33.37 (1.01) & Lrnr\_polspline\_5\_4\\
28 & 34.10 (0.942) & 0.85 (0.855) & 33.37 (1.010) & Lrnr\_polspline\_5\_4\\
29 & 34.97 (0.909) & 0.15 (0.842) & 33.37 (1.010) & Lrnr\_polspline\_5\_4\\
30 & 32.8 (1.001) & 0.0 (0.842) & 33.4 (1.010) & Pipeline(Lrnr\_pkg\_SuperLearner\_screener\_screen.glmnet->Lrnr\_glm\_TRUE)\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item The ensemble super learner (SuperLearner) is the prediction generated as a weighted combination of component model predictions. The discrete super learner (Discrete SL) is the best-performing component model. The mean (Mean) is the average cumulative hospitalization rate per 100,000 across the simulated hospitalization curves. The absolute error loss function used to optimized the super learner targets the median peak hospitalization rate.
\end{tablenotes}
\end{threeparttable}}
\end{table}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

In our study, we found that discrete super learners were consistently
better at predicting median peak rate, peak week, and cumulative
hospitalizations compared to a simple mean prediction when fit to a
distribution of hypothetical influenza hospitalization curves. On the
contrary, the ensemble super learner generally tended to perform
comparably to a simple mean prediction and consistently worse than the
discrete super learners with respect to cross-validated risk. In fact,
the discrete super learner routinely provided the best predictions
(lowest mean cross-validated risk) across all prediction targets. The
discrete super learner also appeared to provide greater improvements
over the simple mean prediction as a season wore on, a result that
comports with similar findings in the domain of influenza-like illness
{[}8{]}.

The relative underperformance of the ensemble super learner, a feature
observed across all prediction targets, raises several questions
regarding the likely usefulness of the super learner algorithm in the
context of predicting influenza hospitalizations. It may be that the
ensemble super learner should not be relied upon if used to generate
in-season forecasts given the small sample size of observed
hospitalization curves {[}21{]} and the fact that we did not observe
improvements in prediction in a sample size of 15,000. As discussed in
the introduction, the ensemble super learner has desirable asymptotic
oracle properties, theoretically guaranteed to preform as well as the
discrete super learner {[}12{]}. Furthermore, the ensemble super learner
is generally assumed not to be affected adversely by the inclusion of
poor algorithms or overfitting, as the cross-validation procedure should
result in a final weighted ensemble that assigns low or no weight to
poor-performing predictions {[}17{]}. It is possible that adding
additional algorithms such as gradient boosted models or exploring a
larger range of tuning parameters for the models incorporated in our
analysis would have improved the performance of the ensemble learner.

Our findings should be interpreted in light of several limitations.

First, while our simulating 15,000 hypothetical hospitalization curves
provided an adequate sample size with which to train the super
learner---an analysis that would not have been possible using only the
15 observed flu seasons---the validity of our findings rely on the
assumption that the distribution of simulated curves accurately
characterizes the hypothetical distribution of plausible seasonal
influenza hospitalization trajectories. We did not attempt to rigidly
match the simulated distributions to their empirical counterparts so as
not to restrict the possibility of atypical influenza seasons that could
be, but perhaps have not yet been, observed. In other words, we did not
assume the 15 observed seasons captured the full range of possible
values for the selected prediction targets. A related issue concerns the
selection of the \(\lambda\) value for each trend filter fit to an
empirical season, as these fits were used as templates for the simulated
hospitalization curves. This choice of penalty was somewhat arbitrary.
Selecting the \(\lambda\) penalty that minimized the root mean squared
error of the trend filter predictions with respect to the empirical data
may have resulted in a distribution of simulated curves with different
properties. However, while methods for selecting the \(\lambda\) penalty
for the \(\ell_1\) trend filter have been proposed {[}39{]}, to our
knowledge, no consensus currently exists regarding the best procedure to
do so.

Second, we did not include any mechanistic models in the component
learner library, such as compartmental or agent-based epidemic models.
Future work could incorporate such models or, alternatively, the
predictions generated by an ensemble super learner could be incorporated
into a wider ensemble model that incorporates predictions from such
models.

Third, our approach to simulating hypothetical influenza hospitalization
curves incorporated only information on hospitalization rates, without
relying on other mechanistic factors to generate these curves.
Therefore, we were unable to examine whether incorporating other
influenza-related surveillance data (e.g., ILI, viral activity {[}21{]})
or climate data may have improved the super learner's predictions.

Fourth, we did not model hospitalization rate time series explicitly, in
order to increase the number of component models available for inclusion
{[}19{]}. Nonetheless, we did attempt to incorporate information
regarding the trajectory of influenza hospitalization rates up to a
given week by incorporating lagged hospitalization rates, differences
between the hospitalization rates in the current week and prior weeks,
and selected interactions. Future work should explore the potential for
explicit modeling of time series, perhaps using an ``online'' ensemble
super learner, which continuously updates the super learner's
predictions based on its predictions in prior weeks {[}40{]}.

Finally, because we focused on predicting hospitalizations due to
seasonal influenza, these findings do not have implications for
predicting hospitalizations during years in which a pandemic influenza
strain is circulating.

The super learner remains a promising algorithm for predicting
influenza-related hospitalizations, but future work should determine the
proper breadth of component learners such that the ensemble learner can
be relied upon to perform as well as the discrete super learner.
Adapting this approach to incorporate empirical surveillance data as
well as other data relevant to influenza dynamics (e.g., viral activity,
weather patterns), and to focus on hospitalizations from specific
settings (e.g., skilled nursing facilities), may also be fruitful
avenues for future research.

\hypertarget{software-and-code}{%
\section{Software and code}\label{software-and-code}}

The code necessary to reproduce these results and the output from the
super learner procedures are provided in a repository at Open Science
Framework \textbf{(doi: XXXXXXXXX)}. These materials are also housed
permanently at the Brown Digital Repository \textbf{(doi/url:
XXXXXXXXXXX)}, and the package can be installed directly via the Github
repository (\url{https://www.github.com/jrgant/FluHospPrediction}).

\hypertarget{declarations}{%
\section{Declarations}\label{declarations}}

\hypertarget{acknowledgment}{%
\subsection{Acknowledgment}\label{acknowledgment}}

We thank Ashley Naimi, Laura Balzer, and Nicholas Reich for their
comments on the study aims and the super learner approach. The super
learner algorithm was fit using computational resources and services at
the Center for Computation and Visualization, Brown University.

\hypertarget{funding-statement}{%
\subsection{Funding statement}\label{funding-statement}}

This work was funded by an unrestricted grant from Sanofi (PI: Andrew
Zullo). The funders did not assist in the statistical analysis nor did
they have a say in the final decision to submit the manuscript for
publication.

\hypertarget{competing-interests}{%
\subsection{Competing interests}\label{competing-interests}}

{[}solicit competing interests from co-authors{]}

\newpage

\hypertarget{supporting-information}{%
\section{Supporting information}\label{supporting-information}}

\textbf{S1 Fig. Linear trend filter fits to observed influenza
hospitalization curves.} \bigskip

\begin{center}\includegraphics[width=0.9\linewidth]{C:/Users/jason/Documents/Github/FluHospPrediction/results/trendfilter-fit-facet} \end{center}

\newpage

\noindent \textbf{S2 Table. Number of simulated curves based on each
observed flu season (Emerging Infections Program).}

\begin{tabular}{ll}
\toprule
Template season & N\\
\midrule
2003-04 & 1,032\\
2004-05 & 979\\
2005-06 & 972\\
2006-07 & 1,021\\
2007-08 & 994\\
\addlinespace
2008-09 & 1,033\\
2010-11 & 946\\
2011-12 & 956\\
2012-13 & 999\\
2013-14 & 1,028\\
\addlinespace
2014-15 & 1,013\\
2015-16 & 995\\
2016-17 & 1,040\\
2017-18 & 986\\
2018-19 & 1,006\\
\bottomrule
\end{tabular}

\newpage

\noindent \textbf{S3 Figure. Simulated hospitalization curves generated
based on each empirical shape template.} Note that because each
parameter used in the curve-generating function was drawn independently,
simulated hospitalization curves based on an empirical shape template
should have a similar shape (i.e., unimodal, bimodal) but may have very
different peak and/or cumulative hospitalization rates compared to the
empirical template.

\includegraphics[width=25in]{C:/Users/jason/Documents/Github/FluHospPrediction/results/paper_output/tempsim_plot}

\newpage

\noindent \textbf{S4 Table. Weekly cross-validated risks across the
component learners used to predict peak hospitalization rate per 100,000
population.}

\begin{tabular}{rrrrrr}
\toprule
Week & Mean & SD & Median & Minimum & Maximum\\
\midrule
1 & 10.09 & 20.41 & 3.16 & 2.68 & 82.75\\
2 & 8.98 & 19.82 & 2.80 & 2.20 & 82.75\\
3 & 8.19 & 19.77 & 2.75 & 1.77 & 82.75\\
4 & 10.74 & 21.72 & 2.65 & 1.85 & 82.75\\
5 & 21.91 & 80.05 & 4.54 & 2.50 & 621.47\\
\addlinespace
6 & 9.12 & 19.60 & 3.31 & 2.36 & 82.75\\
7 & 9.02 & 19.61 & 3.36 & 2.19 & 82.75\\
8 & 8.65 & 19.65 & 3.30 & 2.46 & 82.75\\
9 & 9.02 & 19.63 & 3.28 & 2.40 & 82.75\\
10 & 8.41 & 19.70 & 2.92 & 2.20 & 82.75\\
\addlinespace
11 & 8.47 & 19.68 & 3.46 & 2.27 & 82.75\\
12 & 8.46 & 19.69 & 3.19 & 2.32 & 82.75\\
13 & 8.18 & 19.75 & 3.00 & 1.99 & 82.75\\
14 & 8.34 & 19.74 & 3.07 & 1.97 & 82.75\\
15 & 8.32 & 19.72 & 3.09 & 2.11 & 82.75\\
\addlinespace
16 & 8.25 & 19.74 & 2.98 & 2.16 & 82.75\\
17 & 8.16 & 19.75 & 3.02 & 2.11 & 82.75\\
18 & 8.10 & 19.77 & 2.89 & 2.09 & 82.75\\
19 & 7.77 & 19.86 & 2.68 & 1.33 & 82.75\\
20 & 7.58 & 19.91 & 2.59 & 1.22 & 82.75\\
\addlinespace
21 & 7.48 & 19.94 & 2.60 & 1.06 & 82.75\\
22 & 7.38 & 19.97 & 2.66 & 0.86 & 82.75\\
23 & 7.39 & 19.97 & 2.67 & 0.44 & 82.75\\
24 & 7.38 & 19.98 & 2.66 & 0.68 & 82.75\\
25 & 7.39 & 19.97 & 2.75 & 0.74 & 82.75\\
\addlinespace
26 & 7.36 & 19.98 & 2.70 & 0.78 & 82.75\\
27 & 7.32 & 19.99 & 2.63 & 0.67 & 82.75\\
28 & 7.32 & 19.99 & 2.64 & 0.63 & 82.75\\
29 & 7.32 & 19.99 & 2.65 & 0.63 & 82.75\\
30 & 7.32 & 19.99 & 2.69 & 0.72 & 82.75\\
\bottomrule
\end{tabular}

\newpage

\noindent \textbf{S5 Table. Weekly cross-validated risks across the
component learners used to predict peak week.}

\begin{tabular}{rrrrrr}
\toprule
Week & Mean & SD & Median & Minimum & Maximum\\
\midrule
1 & 12.55 & 29.68 & 3.92 & 3.34 & 122.43\\
2 & 17.44 & 36.86 & 4.33 & 3.50 & 164.76\\
3 & 11.88 & 29.30 & 3.92 & 2.58 & 122.43\\
4 & 11.89 & 29.32 & 3.82 & 2.80 & 122.43\\
5 & 60.17 & 368.34 & 3.76 & 2.43 & 2902.90\\
\addlinespace
6 & 11.97 & 29.32 & 3.91 & 2.82 & 122.43\\
7 & 12.12 & 29.28 & 4.07 & 2.81 & 122.43\\
8 & 12.62 & 29.23 & 4.42 & 3.23 & 122.43\\
9 & 12.39 & 29.44 & 4.16 & 2.99 & 122.43\\
10 & 12.39 & 29.18 & 4.60 & 3.04 & 122.43\\
\addlinespace
11 & 11.77 & 29.32 & 4.09 & 2.60 & 122.43\\
12 & 11.45 & 29.43 & 3.65 & 2.18 & 122.43\\
13 & 11.20 & 29.47 & 3.54 & 2.23 & 122.43\\
14 & 11.21 & 29.46 & 3.62 & 2.29 & 122.43\\
15 & 11.08 & 29.50 & 3.60 & 1.95 & 122.43\\
\addlinespace
16 & 11.30 & 29.44 & 3.84 & 1.89 & 122.43\\
17 & 11.21 & 29.47 & 3.53 & 1.85 & 122.43\\
18 & 11.14 & 29.49 & 3.52 & 1.79 & 122.43\\
19 & 10.94 & 29.53 & 3.36 & 1.71 & 122.43\\
20 & 10.93 & 29.53 & 3.44 & 2.15 & 122.43\\
\addlinespace
21 & 11.24 & 29.48 & 3.47 & 1.94 & 122.43\\
22 & 10.92 & 29.54 & 3.49 & 1.78 & 122.43\\
23 & 10.96 & 29.53 & 3.43 & 1.92 & 122.43\\
24 & 10.98 & 29.53 & 3.50 & 1.84 & 122.43\\
25 & 11.03 & 29.51 & 3.67 & 1.98 & 122.43\\
\addlinespace
26 & 11.07 & 29.51 & 3.61 & 1.98 & 122.43\\
27 & 11.14 & 29.48 & 3.78 & 2.06 & 122.43\\
28 & 11.07 & 29.49 & 3.69 & 2.14 & 122.43\\
29 & 11.13 & 29.48 & 3.67 & 2.00 & 122.43\\
30 & 11.03 & 29.51 & 3.67 & 1.86 & 122.43\\
\bottomrule
\end{tabular}

\newpage

\noindent \textbf{S6 Table. Weekly cross-validated risks across the
component learners used to predict cumulative hospitalization rate per
100,000 population.}

\begin{tabular}{rrrrrr}
\toprule
Week & Mean & SD & Median & Minimum & Maximum\\
\midrule
1 & 117.34 & 266.81 & 34.81 & 28.41 & 1026.83\\
2 & 100.78 & 246.65 & 32.08 & 23.19 & 1026.83\\
3 & 97.52 & 246.66 & 30.42 & 24.61 & 1026.83\\
4 & 95.06 & 247.04 & 28.60 & 22.74 & 1026.83\\
5 & 326.71 & 1647.27 & 34.07 & 24.44 & 12918.25\\
\addlinespace
6 & 98.59 & 245.86 & 33.26 & 26.26 & 1026.83\\
7 & 96.82 & 246.28 & 31.85 & 25.00 & 1026.83\\
8 & 99.44 & 245.84 & 32.86 & 25.73 & 1026.83\\
9 & 98.57 & 246.50 & 31.58 & 24.05 & 1026.83\\
10 & 96.72 & 246.30 & 32.71 & 22.36 & 1026.83\\
\addlinespace
11 & 97.63 & 246.06 & 32.87 & 23.78 & 1026.83\\
12 & 96.59 & 246.32 & 32.89 & 22.90 & 1026.83\\
13 & 96.32 & 246.41 & 32.87 & 24.35 & 1026.83\\
14 & 97.37 & 246.24 & 32.61 & 23.71 & 1026.83\\
15 & 97.47 & 246.13 & 33.03 & 21.71 & 1026.83\\
\addlinespace
16 & 95.84 & 246.54 & 32.41 & 17.21 & 1026.83\\
17 & 96.78 & 246.27 & 33.14 & 20.29 & 1026.83\\
18 & 96.59 & 246.33 & 33.01 & 20.55 & 1026.83\\
19 & 93.65 & 247.14 & 32.62 & 15.76 & 1026.83\\
20 & 92.52 & 247.45 & 32.03 & 15.90 & 1026.83\\
\addlinespace
21 & 91.71 & 247.74 & 32.52 & 9.82 & 1026.83\\
22 & 89.59 & 248.37 & 32.49 & 4.44 & 1026.83\\
23 & 90.31 & 248.20 & 32.84 & 2.89 & 1026.83\\
24 & 89.44 & 248.45 & 32.87 & 2.87 & 1026.83\\
25 & 88.91 & 248.60 & 32.51 & 2.46 & 1026.83\\
\addlinespace
26 & 88.60 & 248.70 & 32.87 & 1.93 & 1026.83\\
27 & 88.55 & 248.74 & 32.01 & 0.83 & 1026.83\\
28 & 88.16 & 248.85 & 31.90 & 0.85 & 1026.83\\
29 & 87.74 & 248.95 & 32.47 & 0.15 & 1026.83\\
30 & 87.94 & 248.92 & 32.57 & 0.00 & 1026.83\\
\bottomrule
\end{tabular}

\newpage

\noindent \textbf{S7 Figure. Peak rate, ensemble learner weights as a
function of log mean cross-validated risk.} Ensemble learner weights are
assigned by regressing the prediction target on corresponding
predictions made by each component learner. The coefficients estimated
for each independent predictor (i.e., component learner) represent the
weight given to the learner's predictions in the final ensemble super
learner prediction.

\includegraphics[width=27.08in]{C:/Users/jason/Documents/Github/FluHospPrediction/results/paper_output/pkrate_rw_plot}

\newpage

\noindent \textbf{S8 Figure. Peak week, ensemble learner weights as a
function of log mean cross-validated risk.} Ensemble learner weights are
assigned by regressing the prediction target on corresponding
predictions made by each component learner. The coefficients estimated
for each independent predictor (i.e., component learner) represent the
weight given to the learner's predictions in the final ensemble super
learner prediction.

\includegraphics[width=27.08in]{C:/Users/jason/Documents/Github/FluHospPrediction/results/paper_output/pkweek_rw_plot}

\newpage

\noindent \textbf{S9 Figure. Cumulative hospitalizations, ensemble
learner weights as a function of log mean cross-validated risk.}
Ensemble learner weights are assigned by regressing the prediction
target on corresponding predictions made by each component learner. The
coefficients estimated for each independent predictor (i.e., component
learner) represent the weight given to the learner's predictions in the
final ensemble super learner prediction.

\includegraphics[width=27.08in]{C:/Users/jason/Documents/Github/FluHospPrediction/results/paper_output/cumhosp_rw_plot}

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\bibliography{references}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-Centers_for_Disease_Control_and_Prevention2020-uc}{}%
1. Centers for Disease Control and Prevention. Burden of influenza.
\url{https://www.cdc.gov/flu/about/burden/index.html}; 2020.

\leavevmode\hypertarget{ref-Lutz2019-co}{}%
2. Lutz CS, Huynh MP, Schroeder M, Anyatonwu S, Dahlgren FS, Danyluk G,
et al. Applying infectious disease forecasting to public health: A path
forward using influenza forecasting examples. BMC Public Health.
2019;19: 1659.
doi:\href{https://doi.org/10.1186/s12889-019-7966-8}{10.1186/s12889-019-7966-8}

\leavevmode\hypertarget{ref-Reich2019-uk}{}%
3. Reich NG, Brooks LC, Fox SJ, Kandula S, McGowan CJ, Moore E, et al. A
collaborative multiyear, multimodel assessment of seasonal influenza
forecasting in the united states. Proc Natl Acad Sci U S A. 2019;116:
3146--3154.
doi:\href{https://doi.org/10.1073/pnas.1812594116}{10.1073/pnas.1812594116}

\leavevmode\hypertarget{ref-Chretien2014-dy}{}%
4. Chretien J-P, George D, Shaman J, Chitale RA, McKenzie FE. Influenza
forecasting in human populations: A scoping review. PLoS One. 2014;9:
e94130.
doi:\href{https://doi.org/10.1371/journal.pone.0094130}{10.1371/journal.pone.0094130}

\leavevmode\hypertarget{ref-Biggerstaff2018-ns}{}%
5. Biggerstaff M, Kniss K, Jernigan DB, Brammer L, Bresee J, Garg S, et
al. Systematic assessment of multiple routine and near Real-Time
indicators to classify the severity of influenza seasons and pandemics
in the united states, 2003-2004 through 2015-2016. Am J Epidemiol.
2018;187: 1040--1050.
doi:\href{https://doi.org/10.1093/aje/kwx334}{10.1093/aje/kwx334}

\leavevmode\hypertarget{ref-Brooks2015-fl}{}%
6. Brooks LC, Farrow DC, Hyun S, Tibshirani RJ, Rosenfeld R. Flexible
modeling of epidemics with an empirical bayes framework. PLoS Comput
Biol. 2015;11: e1004382.
doi:\href{https://doi.org/10.1371/journal.pcbi.1004382}{10.1371/journal.pcbi.1004382}

\leavevmode\hypertarget{ref-Reich2019-ca}{}%
7. Reich NG, McGowan CJ, Yamana TK, Tushar A, Ray EL, Osthus D, et al.
Accuracy of real-time multi-model ensemble forecasts for seasonal
influenza in the U.S. PLoS Comput Biol. 2019;15: e1007486.
doi:\href{https://doi.org/10.1371/journal.pcbi.1007486}{10.1371/journal.pcbi.1007486}

\leavevmode\hypertarget{ref-Ray2018-ef}{}%
8. Ray EL, Reich NG. Prediction of infectious disease epidemics via
weighted density ensembles. PLoS Comput Biol. 2018;14: e1005910.
doi:\href{https://doi.org/10.1371/journal.pcbi.1005910}{10.1371/journal.pcbi.1005910}

\leavevmode\hypertarget{ref-Hastie2009-ft}{}%
9. Hastie T, Tibshirani R, Friedman J. The elements of statistical
learning: Data mining, inference, and prediction. Springer, New York,
NY; 2009.
doi:\href{https://doi.org/10.1007/978-0-387-84858-7}{10.1007/978-0-387-84858-7}

\leavevmode\hypertarget{ref-Wolpert1992-pw}{}%
10. Wolpert DH. Stacked generalization. Neural Netw. 1992;5: 241--259.
doi:\href{https://doi.org/10.1016/S0893-6080(05)80023-1}{10.1016/S0893-6080(05)80023-1}

\leavevmode\hypertarget{ref-Breiman1996-ez}{}%
11. Breiman L. Stacked regressions. Mach Learn. 1996;24: 49--64.
doi:\href{https://doi.org/10.1007/BF00117832}{10.1007/BF00117832}

\leavevmode\hypertarget{ref-Van_der_Laan2007-ml}{}%
12. Laan MJ van der, Polley EC, Hubbard AE. Super learner. Stat Appl
Genet Mol Biol. De Gruyter; 2007;6: Article25.
doi:\href{https://doi.org/10.2202/1544-6115.1309}{10.2202/1544-6115.1309}

\leavevmode\hypertarget{ref-McGowan2019-ph}{}%
13. McGowan CJ, Biggerstaff M, Johansson M, Apfeldorf KM, Ben-Nun M,
Brooks L, et al. Collaborative efforts to forecast seasonal influenza in
the united states, 2015-2016. Sci Rep. nature.com; 2019;9: 683.
doi:\href{https://doi.org/10.1038/s41598-018-36361-9}{10.1038/s41598-018-36361-9}

\leavevmode\hypertarget{ref-Kandula2018-sq}{}%
14. Kandula S, Yamana T, Pei S, Yang W, Morita H, Shaman J. Evaluation
of mechanistic and statistical methods in forecasting influenza-like
illness. J R Soc Interface. 2018;15.
doi:\href{https://doi.org/10.1098/rsif.2018.0174}{10.1098/rsif.2018.0174}

\leavevmode\hypertarget{ref-Kandula2019-tg}{}%
15. Kandula S, Pei S, Shaman J. Improved forecasts of
influenza-associated hospitalization rates with google search trends. J
R Soc Interface. 2019;16: 20190080.
doi:\href{https://doi.org/10.1098/rsif.2019.0080}{10.1098/rsif.2019.0080}

\leavevmode\hypertarget{ref-Polley2010-cb}{}%
16. Polley EC, Laan MJ van der. Super learner in prediction. University
of California, Berkeley; 2010.

\leavevmode\hypertarget{ref-Polley2011-oz}{}%
17. Polley EC, Rose S, Laan MJ van der. Super learning. In: Laan MJ van
der, Rose S, editors. Targeted learning: Causal inference for
observational and experimental data. New York, NY: Springer New York;
2011. pp. 43--66.
doi:\href{https://doi.org/10.1007/978-1-4419-9782-1/_3}{10.1007/978-1-4419-9782-1\textbackslash{}\_3}

\leavevmode\hypertarget{ref-Polley2019-sl}{}%
18. Polley E, LeDell E, Kennedy C, van der Laan M. SuperLearner: Super
learner prediction {[}Internet{]}. 2019. Available:
\url{https://CRAN.R-project.org/package=SuperLearner}

\leavevmode\hypertarget{ref-Coyle2020-ze}{}%
19. Coyle JR, Hejazi NS, Malenica I, Sofrygin O. Sl3: Pipelines for
machine learning and super learning. 2020.
doi:\href{https://doi.org/10.5281/zenodo.1342293}{10.5281/zenodo.1342293}

\leavevmode\hypertarget{ref-Centers_for_Disease_Control_and_Prevention_undated-tx}{}%
20. Centers for Disease Control and Prevention. Epidemic prediction
initiative. \url{https://predict.cdc.gov/post/59973fe26f7559750d84a843};

\leavevmode\hypertarget{ref-Centers_for_Disease_Control_and_Prevention_undated-vt}{}%
21. Centers for Disease Control and Prevention. Flu view phase 3 quick
reference guide {[}Internet{]}. Available:
\url{https://gis.cdc.gov/GRASP/Fluview/FluHospRates.html}

\leavevmode\hypertarget{ref-Centers_for_Disease_Control_and_Prevention_undated-pu}{}%
22. Centers for Disease Control and Prevention. MMWR week overview.
\url{https://wwwn.cdc.gov/nndss/document/MMWR_Week_overview.pdf};

\leavevmode\hypertarget{ref-Kim2009-bz}{}%
23. Kim S, Koh K, Boyd S, Gorinevsky D. \(\ell\)1 trend filtering. SIAM
Rev. Society for Industrial; Applied Mathematics; 2009;51: 339--360.
doi:\href{https://doi.org/10.1137/070690274}{10.1137/070690274}

\leavevmode\hypertarget{ref-Tibshirani2014-tr}{}%
24. Tibshirani RJ. Adaptive piecewise polynomial estimation via trend
filtering. Ann Stat. Institute of Mathematical Statistics; 2014;42:
285--323.
doi:\href{https://doi.org/10.1214/13-AOS1189}{10.1214/13-AOS1189}

\leavevmode\hypertarget{ref-Arnold2015-tb}{}%
25. Arnold T, Sadhanala V, Tibshirani R. Glmgen. 2015.

\leavevmode\hypertarget{ref-RCore2020-ct}{}%
26. R Core Team. R: A language and environment for statistical computing
{[}Internet{]}. Vienna, Austria: R Foundation for Statistical Computing;
2020. Available: \url{https://www.R-project.org/}

\leavevmode\hypertarget{ref-Naimi2018-fv}{}%
27. Naimi AI, Balzer LB. Stacked generalization: An introduction to
super learning. Eur J Epidemiol. 2018;33: 459--464.
doi:\href{https://doi.org/10.1007/s10654-018-0390-z}{10.1007/s10654-018-0390-z}

\leavevmode\hypertarget{ref-Harrell2015-cd}{}%
28. Harrell FE Jr. Regression modeling strategies: With applications to
linear models, logistic and ordinal regression, and survival analysis.
Springer International Publishing; 2015.
doi:\href{https://doi.org/10.1007/978-3-319-19425-7}{10.1007/978-3-319-19425-7}

\leavevmode\hypertarget{ref-Wood2019-rc}{}%
29. Wood S. Mgcv: Mixed GAM computation vehicle with automatic
smoothness estimation {[}Internet{]}. 2019. Available:
\url{https://cran.r-project.org/web/packages/mgcv/index.html}

\leavevmode\hypertarget{ref-Tibshirani1996-vt}{}%
30. Tibshirani R. Regression shrinkage and selection via the lasso.
Journal of the Royal Statistical Society: Series B (Methodological).
1996. pp. 267--288.
doi:\href{https://doi.org/10.1111/j.2517-6161.1996.tb02080.x}{10.1111/j.2517-6161.1996.tb02080.x}

\leavevmode\hypertarget{ref-Breiman2001-vm}{}%
31. Breiman L. Random forests. Mach Learn. 2001;45: 5--32.
doi:\href{https://doi.org/10.1023/A:1010933404324}{10.1023/A:1010933404324}

\leavevmode\hypertarget{ref-Kooperberg2019-ma}{}%
32. Kooperberg C, Moler C, Dongarra J. Polspline: Polynomial spline
routines {[}Internet{]}. 2019. Available:
\url{https://cran.r-project.org/web/packages/polspline/index.html}

\leavevmode\hypertarget{ref-Friedman2019-uw}{}%
33. Friedman J, Hastie T, Tibshirani R, Narasimhan B, Simon N, Qian J.
Glmnet: Lasso and Elastic-Net regularized generalized linear models
{[}Internet{]}. 2019. Available:
\url{https://cran.r-project.org/web/packages/glmnet/index.html}

\leavevmode\hypertarget{ref-Liaw2018-fe}{}%
34. Liaw A, Wiener M. Breiman and cutler's random forests for
classification and regression {[}Internet{]}. 2018. Available:
\url{https://cran.r-project.org/web/packages/randomForest/randomForest.pdf}

\leavevmode\hypertarget{ref-Meyer2019-gr}{}%
35. Meyer D, Dimitriadou E, Hornik K, Weingessel A, Leisch F, Chang C-C,
et al. E1071 {[}Internet{]}. 2019. Available:
\url{https://cran.r-project.org/web/packages/e1071/e1071.pdf}

\leavevmode\hypertarget{ref-Ripley2020-ds}{}%
36. Ripley B, Venables W. Nnet {[}Internet{]}. 2020. Available:
\url{https://cran.r-project.org/web/packages/nnet/nnet.pdf}

\leavevmode\hypertarget{ref-Coyle2020-rq}{}%
37. Coyle J, Hejazi N. Delayed: A framework for parallelizing dependent
tasks {[}Internet{]}. 2020. Available:
\url{https://cran.r-project.org/web/packages/delayed/index.html}

\leavevmode\hypertarget{ref-Bengtsson2020-oz}{}%
38. Bengtsson H. Future: Unified parallel and distributed processing in
R for everyone {[}Internet{]}. 2020. Available:
\url{https://cran.r-project.org/web/packages/future/index.html}

\leavevmode\hypertarget{ref-Yamada2016-fq}{}%
39. Yamada H, Yoon G. Selecting the tuning parameter of the \(\ell1\)
trend filter. Stud Nonlinear Dyn Econom. 2016;20: 1.
doi:\href{https://doi.org/10.1515/snde-2014-0089}{10.1515/snde-2014-0089}

\leavevmode\hypertarget{ref-Van_der_Laan2018-xq}{}%
40. Laan MJ van der, Benkeser D. Online super learning. In: Laan MJ van
der, Rose S, editors. Targeted learning in data science: Causal
inference for complex longitudinal studies. Cham: Springer International
Publishing; 2018. pp. 303--315.
doi:\href{https://doi.org/10.1007/978-3-319-65304-4/_18}{10.1007/978-3-319-65304-4\textbackslash{}\_18}

\nolinenumbers


\end{document}


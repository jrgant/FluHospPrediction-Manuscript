---
title: "Predicting seasonal influenza hospitalization using an ensemble super learner: a simulation study"

bibliography: references.bib
output: rticles::plos_article
csl: plos.csl

header-includes:
    - \usepackage{booktabs}
    - \usepackage{threeparttable}

always_allow_html: true
---

**Short Title:** Predicting influenza hospitalizations with a super learner

Jason R. Gantenberg,(1,2) Kevin W. McConeghy,(2,3) Jon Steingrimsson,(4) Chanelle J. Howe,(5) Robertus van Aalst,(6,7), Ayman Chit,(6), Andrew R. Zullo(1,2,3)


**Corresponding author:** Jason R. Gantenberg (jrgant@brown.edu)

1. Department of Epidemiology, Brown University School of Public Health, 121 S. Main St., Providence, RI, 02912
2. Department of Health Services, Policy and Practice, 121 S. Main St., Providence, RI, 02912
3. Providence VA Medical Center, 830 Chalkstone Ave., Providence, RI, 02908
4. Department of Biostatistics, Brown University School of Public Health, 121 S. Main St., Providence, RI 02912
5. Center for Epidemiology and Environmental Health, Brown University School of Public Health, 121 S. Main St., Providence, RI, 02912
6. Sanofi Pasteur, Swiftwater, PA 18370
7. Department of Health Sciences, University of Groningen, University Medical Center Groningen, Groningen, the Netherlands


# Abstract

Seasonal influenza has a substantial impact on human health in the United States and worldwide. Accurate forecasts of key features of influenza epidemics and health care utilization can inform public health response to outbreaks. To date, predicting seasonal influenza-related hospitalizations has less frequently been the focus of forecasting efforts. We conducted a simulation study to measure the performance of a machine-learning algorithm (super learner) to predict three features of a given seasonal influenza epidemic: peak hospitalization rate, the week during which the hospitalization rate peaks, and the cumulative hospitalizations. We fit a set of statistical models and the super learner to 15,000 simulated influenza hospitalization curves to generate weekly predictions for the targets of interest. In each week, we compared the performance of the ensemble super learner (a weighted combination of component model predictions), the discrete super learner (best-performing individual statistical model), and a prediction based on a simple mean of a given outcome (e.g., mean peak hospitalization rate)  across all simulated hospitalization curves. We found that the general super learner algorithm---which subsumes both the selection of the discrete super learner and estimation of the ensemble super learner---was consistently capable of choosing individual statistical models that improved predictions of the three seasonal prediction targets over those of a simple mean prediction. However, the ensemble super learner consistently exhibited higher prediction error than the discrete super learners and often performed comparably to a simple mean prediction. Given that national-level influenza hospitalization forecasts based on empirical may rely on the small number of past flu seasons recorded in surveillance data, the ensembles' underperformance in a large sample of hypothetical influenza seasons suggests future work should determine the conditions under which ensemble models can be expected not to underperform the best-performing component model.

# Author Summary

Influenza prediction is a still-maturing science, but notable improvements have been made over the past decade in predicting influenza-like activity. In addition, increasing focus on machine learning approaches has provided hope that such approaches may further improve predictive accuracy. Less work, however, has focused on predicting influenza-related hospitalizations, and we sought to investigate the potential for machine learning approaches to predict several features of influenza hospitalization dynamics that could help inform the allocation of public health efforts and resources during a flu season. We used a machine learning approach, called _super learner_, which is capable of taking a large number of different statistical models and either combining them into a single prediction (the ensemble prediction) or choosing the best-performing model. Our results suggest that ensemble super learner predictions may not perform as well as the best-performing statistical model under certain conditions. Future work should focus on investigating when ensemble predictions based on machine learning would be expected to improve predictive accuracy.

# Meta

**Target journal:**

- _PLoS Computational Biology_

\noindent
**Section:**

- Epidemiology and Clinical/Translational Studies

\noindent
**Potential editors:**

- Benjamin Althouse
- Miles Davenport
- Matthew Ferrari
- Roger Kouyos
- James Lloyd-Smith

\noindent
**Potential reviewers:**

- Ryan J. Tibshirani (co-author on paper we use for curve simulation)
- Logan C. Brooks (co-author on paper we use for curve simulation)
- Roni Rosenfeld (co-author on paper we use for curve simulation)
- Sherri Rose (expert in machine/super learning)
- David A. Osthus (flu modeler whose work came highly recommended by Nick Reich)
- Samrachana Adhikari (biostatistician and former lab member in the Carnegie Mellon group [with RJ Tibshirani])
- Oleg Sofrygin (sl3 package author)
- Nima Hejazi (sl3 package author)
- Jeremy Coyle (sl3 package author)

```{r include=FALSE}
pacman::p_load(
  FluHospPrediction,
  ggplot2,
  ggthemes,
  gridExtra,
  data.table,
  tidyverse,
  kableExtra,
  knitr,
  flextable
)

opts_chunk$set(
  echo = FALSE,
  cache = TRUE
)

# paper output directory
assetdir <-
  "C:/Users/jason/Documents/Github/FluHospPrediction/results/paper_output"

# function to select which figure format to use based on pandoc output format
putfigure <- function(slug) {
  if (knitr::is_latex_output()) {
    knitr::include_graphics(paste0(assetdir, slug, ".pdf"))
  } else {
    knitr::include_graphics(paste0(assetdir, slug, ".png"))
  }
}

```

# Introduction

Between 2010 and 2017, approximately 140,000--570,000 individuals were hospitalized and 12,000--51,000 died annually due to seasonal influenza in the United States [@Centers_for_Disease_Control_and_Prevention2020-uc]. The ability to predict the future burden of influenza-related hospitalizations during a given influenza season can help policymakers, public health officials, physicians, and other stakeholders to allocate resources appropriately and prepare for expected changes in hospitalization rates [@Lutz2019-co]. For example, forecasts could provide hospitals with enough time to shift inpatient hospital beds available for patients admitted with influenza-related illnesses **[cite]**.

While influenza forecasting is a still-maturing science [@Reich2019-uk; @Chretien2014-dy], researchers have made considerable progress over the past decade in improving the quality of and capacity for forecasting influenza-like illness (ILI) [@Reich2019-uk]. Many of these improvements have been spurred by the FluSight forecasting competitions sponsored by the Centers for Disease Control and Prevention (CDC) since the 2013--14 influenza season [@Reich2019-uk]. Many different types of models have been used to generate forecasts, including statistical time series models [@Reich2019-uk; @Biggerstaff2018-ns], Bayesian methods [@Chretien2014-dy; @Brooks2015-fl], and agent-based models [@Chretien2014-dy], among others. Ensemble methods have emerged as a particularly promising approach to improving further the accuracy and stability of epidemic predictions due to ensembles' ability to combine predictions from 
multiple estimators in a principled manner [@Reich2019-ca; @Ray2018-ef; @Lutz2019-co].

Ensemble methods combine predictions generated by a set of component models [@Reich2019-ca; @Hastie2009-ft; @Wolpert1992-pw; @Breiman1996-ez]. In some cases, ensembles aggreggate component model predictions by weighting better predictions more highly in the final ensemble prediction [@Ray2018-ef; @Reich2019-ca], though other weighting criteria can be applied [@Ray2018-ef]. One compelling rationale for using ensemble predictions rests in their ability to borrow the strengths and avoid the weaknesses of any single class of models used as inputs into the ensemble [@Van_der_Laan2007-ml]. This feature tends to lead not only to more accurate predictions but to more stable ones that can be applied across a range of scenarios [@Ray2018-ef]. The CDC's primary in-season outpatient ILI forecasts are now based on an ensemble forecast generated by aggregating predictions from a growing library of individual forecasts submitted by research teams around the United States [@Reich2019-uk].

To date, most work in influenza forecasting has focused on ILI [@Reich2019-uk; @Biggerstaff2018-ns; @McGowan2019-ph; @Kandula2018-sq; @Brooks2015-fl], with considerably less work having been focused on predicting influenza-related hospitalization rates [@Kandula2019-tg]. Because influenza-related hospitalization dynamics may evolve differently than those of influenza transmission itself during a given flu season---for instance, consequent hospitalizations usually lag influenza incidence by one to two weeks **[citation needed]**---and because hospitalization rates signal the severity of circulating flu strains [@Biggerstaff2018-ns], using ensemble methods to predict hospitalization rates could be a value complement to existing efforts in ILI forecasting.

One ensemble machine learning method, dubbed "super learner" [@Van_der_Laan2007-ml; @Polley2010-cb; @Polley2011-oz], exhibits a number of desirable properties that suggest it may be a powerful tool for predicting flu hospitalizations. First, its developers have demonstrated that, asymptotically, the super learner is an oracle estimator, performing as well as the best-fitting component model [@Polley2010-cb; @Polley2011-oz], where the best-fitting component model performs as well as the true data-generating model [@Polley2010-cb] **[read and cite van der Laan & Dudoit, 2003?]**. Second, this oracle property has been found to apply approximately in finite samples [@Polley2010-cb; @Polley2011-oz; @Van_der_Laan2007-ml]. Several packages have been developed to implement the super learner algorithm [@Polley2019-sl; @Coyle2020-ze], providing researchers easy access to a large library of component models and a means to efficiently evaluate their predictive performance [@Coyle2020-ze]. 

The super learner requires a sufficient, though not exorbitant, sample size in order to implement embedded procedures (e.g., cross-validation) [@Van_der_Laan2007-ml]. However, the 15 empirical flu seasons available are not sufficient in number to develop the super learner.

In this study, we trained an ensemble super learner on 15,000 simulated influenza hospitalization curves to generate predictions for three national-level seasonal target parameters chosen based on the targets specified in CDC forecasting competitions [@Centers_for_Disease_Control_and_Prevention_undated-tx]. We conducted the procedure independently at each of the 30 weeks in a typical flu season, and we compared the predictive performance of the ensemble super learner against those of the best-performing individual statistical model and a prediction based on the mean of the outcome across the simulated hospitalization curves. 

# Methods

## Empirical data

We downloaded publicly available surveillance data on seasonal influenza-related hospitalizations from the CDC's FluView Interactive dashboard [@Centers_for_Disease_Control_and_Prevention_undated-vt]. Specifically, we included data from the Emerging Infections Program (EIP) beginning with the 2003--2004 season and ending with the 2018--2019 season, omitting the 2009-2010 pandemic influenza year. The EIP contains data on influenza-related hospitalizations in California, Colorado, Connecticut, Georgia, Maryland, Minnesota, New Mexico, New York, Oregon, and Tennessee [@Centers_for_Disease_Control_and_Prevention_undated-vt]. Since the 2009--2010 season the FluSurv Network has included between 3 and 6 states in addition to those represented in the EIP data, depending on the year [@Centers_for_Disease_Control_and_Prevention_undated-vt]. In order to maintain consistency within the empirical data, and to increase the number of flu seasons available to inform curve simulation, we did not include data from states outside the EIP.

Typically, the CDC releases data for epiweeks 40--53 and 1--17, corresponding approximately to October through April of the next year [@Centers_for_Disease_Control_and_Prevention_undated-vt; @Centers_for_Disease_Control_and_Prevention_undated-pu]. Epiweeks are simply integers assigned to each week of the calendar year, where epiweek 1 is the first week of the year and epiweeks 52/53 the last [@Centers_for_Disease_Control_and_Prevention_undated-pu]. We renumbered the epiweeks 1--30, omitting epiweek 53, as only three seasons had influenza hospitalization data recorded in this week. When we refer subsequently to a _flu season_, we refer to this 30-week time period.

## Prediction targets

Following from the CDC's Flu Sight challenge [@Centers_for_Disease_Control_and_Prevention_undated-tx], we focused on three season-level prediction targets:

1. _Peak rate_, defined as the highest weekly rate of influenza-related hospitalizations throughout the course of a flu season (per 100,000 population);
2. _Peak week_, defined as the week during which this peak rate occurred; and
3. _Cumulative hospitalizations_, defined as the cumulative influenza-related hospitalization rate over the 30 weeks of the flu season (per 100,000 population). 

Fifteen empirical observations were available for each prediction target, corresponding to the flu seasons recorded in the CDC's surveillance data (Table 1).

```{r target-table}

targets <- fread(file.path(assetdir, "table-01_prediction-targets.csv"))

cap <- "Empirical distributions of peak hospitalization rate, peak week, and cumulative hospitalization rate in the United States, 2003--2019."

fn <- "Source: CDC FluView (Emerging Infections Program). Peak rate and cumulative rate expressed per 100,000 population. Pandemic influenza season 2009--2010 omitted."

if (knitr::is_latex_output()) { 
  targets %>%
    kable(
      format = "latex",
      booktabs = TRUE,
      caption = cap
    ) %>%
    kableExtra::footnote(
      fn,
      general_title = "",
      threeparttable = TRUE
    )
} else {
  targets %>%
    flextable(., cwidth = 1.6) %>%
    bold(., part = "header") %>%
    add_header_lines(., paste("Table 1:", cap)) %>%
    add_footer_lines(., fn)
}

```

## Hospitalization curve simulation

To simulate a distribution of seasonal influenza hospitalization curves, we adapted an approach by Brooks et al. originally used to predict influenza-like illness [@Brooks2015-fl]. First, we fitted a linear trend filter [@Kim2009-bz; @Tibshirani2014-tr] to the 15 observed influenza hospitalization curves from the EIP using the `glmgen` package in R [@Arnold2015-tb, @RCore2020-ct]. The `glmgen` linear trend filter is a penalized method that fits a piecewise linear function to a time series, testing 50 values of the penalty ($\lambda$) by default [@Arnold2015-tb]. We used these fits as templates for the simulated influenza hospitalization curves. In all cases, we selected the fit that used the 25th $\lambda$ value tested (S1 Fig) based on visual inspection of the overall distribution of simulated curves. 

Next, we used the resulting 15 trend filter fits into a modified version of the curve generation scheme described in Brooks et al. [@Brooks2015-fl]. Specifically, the original scheme was developed to simulate potential ILI curves, and therefore, the curve-generating function included a term for season-onset, representing the threshold ILI at which the flu season would be considered to have started [@Brooks2015-fl]. This onset is an additional prediction target in FluSight ILI forecasting because ILI surveillance is conducted year-round. On the contrary, such a threshold does not exist for influenza-related hospitalizations. Rather, hospitalization rates are recorded starting in epiweek 40 through the end of the flu season. Save for one season, hospitalization rates start at a rate of 0 per 100,000 [@Centers_for_Disease_Control_and_Prevention_undated-vt]. We modified the curve-generating function to omit the seasonal onset term and added a transformation that constrains simulated hospitalization rates to be greater than or equal to 0. Otherwise, our notation borrows and follows closely from theirs.

Briefly, Brooks et al. conceptualize as seasonal influenza curve as some function plus noise [@Brooks2015-fl]. Adapted to the hospitalization case, the hospitalization rate ($y^s_i$) in season $s$ and week $i$ is given by

$$y^s_i = f^s(i) + \epsilon^s_i, \epsilon \sim N(0, \tau^s),$$

where $f^s(i)$ is a hospitalization rate and $\epsilon^s_i$ is normally distributed error term with  mean 0 and variance $\tau^s$.

For each empirical season $s$, we use its linear trend filter fit and average the squared residuals over $i$ to estimate $\tau^s$:

```{r, results="asis"}
if (knitr::is_latex_output()) {
  cat("$$\\left( \\hat{\\tau}^s \\right)^2 = \\genfrac{}{}{0pt}{}{\\text{avg}}{i} \\left[ y^s_i - \\hat{f}^s (i) \\right]^2.$$")
} else {
  cat("$$( \\hat{\\tau}^s )^2 = \\substack{ \\text{avg} \\\\ i } [ y^s_i - \\hat{f}^s (i) ]^2.$$")
}
```
For each simulated curve, each of the following parameters is sampled randomly and independently of one another:

$$\langle f, \sigma, \nu, \theta, \mu \rangle,$$

where $f$ denotes a randomly selected vector of estimated hospitalization rates based on a linear trend filter fit to empirical season $s$, $\sigma$ denotes the squared error of a linear trend filter fit to randomly sampled season $s'$ and averaged across all weeks, $\nu$ denotes a random uniform draw from the range [0.75, 1.25], $\theta$ denotes a random draw from a uniform distribution bounded by the minimum and maximum peak hospitalization rates from the 15 trend filter fits, and $\mu$ denotes a random draw from a uniform distribution bounded by the minimum and maximum peak weeks based on the 15 trend filter fits (Table 2).

```{r sim-param-input-table}

ptab <- tibble::tribble(
  ~Parameter,
  ~Description,

  "Shape",
  "$f \\sim U \\{ \\hat{f} : \\text{historical season } s \\}$",

  "Noise",
  "$\\sigma \\sim U \\{\\hat{\\tau}^{s'} \\text{ : historical season } s' \\}$",

  "Pacing",
  "$\\nu \\sim U[0.75, 1.25]$, stretches the curve around the peak week",

  "Peak height",
  "$\\theta \\sim U\\left[\\theta_{min} , \\theta_{max}\\right]$",

  "Peak week",
  "$\\mu \\sim U[\\mu_{min}, \\mu_{max}]$"
)

cap <- "Input parameters to the influenza hospitalization curve generating function."
fn <- "The noise parameter for each simulation is drawn separately and may come from a season different than the season used as the shape."

if (knitr::is_latex_output()) {
    ptab %>%
      kable(
        format = "latex",
        escape = FALSE,
        booktabs = TRUE,
        caption = cap
      ) %>%
      kableExtra::footnote(
        fn,
        general_title = "",
        threeparttable = TRUE
      )
} else {
  ptab %>%
    flextable(cwidth = c(1, 4)) %>%
    bold(part = "header") %>%
    add_header_lines(paste("Table 2:", cap)) %>%
    add_footer_lines(fn)
}
```

The generating function for hospitalization rate in week $i$ of simulated season $sim$ therefore is given by:

```{r, results="asis"}
if (knitr::is_latex_output()) {
  cat("$$f^{sim} (i) = \\frac{\\theta}{\\text{max}_j f(j)} \\left[ f \\left( \\frac{i - \\mu}{v} + \\genfrac{}{}{0pt}{}{\\text{arg max }{j}}{f(j)} \\right) \\right] + \\epsilon_i, \\epsilon_i \\sim N(0, \\hat{\\tau}^{s'}),$$")
} else {
  cat("$$f^{sim} (i) = \\frac{\\theta}{\\text{max}_j f(j)} \\left[ f \\left( \\frac{i - \\mu}{v} + \\substack{\\text{arg max }{j} \\\\ f(j) } \\right) \\right] + \\epsilon_i, \\epsilon_i \\sim N(0, \\hat{\\tau}^{s'}),$$")
}
```

where $f(j)$ now denotes the vector of fitted hospitalization rates from the linear trend filter fit (randomly selected shape $f$) and $j$ the integer week for which we want to retrieve the prediction from this fit (equal to $i$). $max_j f(j)$ denotes the peak hospitalization rate from the selected shape, whereas `r ifelse(knitr::is_latex_output(), "$\\genfrac{}{}{0pt}{}{\\text{arg max } {j}}{f(j)}$", "$\\substack{\\text{arg max }{j} \\\\ f(j) }$")` denotes the week in which this peak occurred. The parameters $\theta$, $\mu$, and $\nu$ follow from their prior description. Finally, we introduce noise for each simulated weekly hospitalization rate based on the selected estimate of $\tau^2$.

We impose a lower bound of 0 on the hospitalization rate via the following transformation of $\hat{y}^s_i$, denoted below as $\hat{z}^s_i$:

$$\hat{z}^s_i = 0.5 \bigg( | \hat{y}^s_i | + \hat{y}^s_i \bigg).$$

This transformation effectively preserves positive simulated hospitalization rates and sets negative hospitalization rates to 0. Negative hospitalization rates may be generated at the tails of a given season, when hospitalization rates are generally low, because the error is normally distributed in all weeks. This transformation was imposed on 20.5% of the simulated weekly hospitalization rates simulated.

In all, we simulated 15,000 curves (S2 Fig). These simulated curves may be thought of as a plausible distribution of hypothetical flu seasons that could be observed in principle but perhaps have not yet been realized [@Brooks2015-fl].

```{r fig01-empsim-compare, echo = FALSE, out.width="70%", fig.height=8, fig.cap = "Empirical (top) and 15 randomly selected simulated (bottom) hospitalization curves. Empirical source: CDC, Emerging Infections Program (omitting 2009--2010 pandemic influenza season).", message = F}

putfigure("fig01")

```

```{r fig02-simcompare, out.width="100%", fig.height=3, fig.cap="Empirical (N = 15) vs. simulated (N = 15,000) target distributions."}

putfigure("fig02")

```

## Super learner

The super learner is a loss-based estimation algorithm, meaning predictive performance is estimated by defining a loss function that targets a specific prediction target of interest [@Van_der_Laan2007-ml; @Polley2010-cb; @Polley2011-oz]. For instance, setting the loss function to be the squared error loss means that model accuracy is optimized to minimize the squared error, and hence, the implicit prediction target is the conditional mean. The super learner takes as inputs training data, a _library_ of component prediction algorithms (called _learners_ but which we refer to as component models), and a desired loss function. See Naimi & Balzer for an accessible introduction [@Naimi2018-fv].

For each prediction target (peak rate, peak week, and cumulative hospitalizations), we used the same library of component models and the $L_1$ absolute error loss function, which targets the conditional median of the prediction target [@Polley2011-oz]. The absolute error loss is defined as the absolute difference between the predicted and true observations [@Polley2011-oz]. 

We trained the super learner on the simulated hospitalization curves using the `sl3` package in R [@Coyle2020-ze]. We conducted the entire super learner procedure sequentially at each week *i*, proposing the following covariates (i.e., independent predictors) to each learner: 

- hospitalization rate per 100,000 population in week *i*, 
- cumulative hospitalizations per 100,000 population through week *i*,
- hospitalization rates from all weeks prior to week *i*, 
- cumulative hospitalizations from all weeks prior to week *i*,
- the difference between the hospitalization rate in week *i* and the hospitalization rate in each prior week up to 5 weeks in the past, 
- the difference between the cumulative hospitalization rate in week *i* and the cumulative hospitalization rate in each prior week up to 5 weeks in the past
- product terms between the hospitalization rate in week *i* and each of the cumulative hospitalization differences, and 
- product terms between the cumulative hospitalization rate in week *i* and each of the hospitalization rate differences.

The predictive accuracy for the component models and the ensemble learner using 15-fold cross-validation [@Harrell2015-cd]. Briefly, *V*-fold cross-validation is an iterative sample-splitting procedure in which each observation is assigned to a group, called a fold, and where each fold serves as the validation set for a model trained on the rest of the sample [@Harrell2015-cd; @Naimi2018-fv]. This procedure maximizes the amount of data available for model-fitting, by avoiding the need for a hold-out validation sample, while at the same time avoiding overfitting [@Van_der_Laan2007-ml; @Naimi2018-fv]. The predictive accuracy of each learning algorithm is evaluated as a _risk_, in our case defined as the average absolute error between a learner's prediction and the true observation. These risks are estimated in each fold when it is assigned to be the validation set as part of the cross-validation procedure. Therefore, in the current context, the _cross-validated risk_ refers to the average risk across all 15 folds [@Van_der_Laan2007-ml, @Naimi2018-fv].

We repeated the following procedure for each prediction target:

1. Create 30 data sets, one for each week of the flu season, containing the outcome variables (peak rate, peak week, and cumulative hospitalizations) and the covariates described previously.

3. Assign each observation to a fold for use in cross-validation. 
   a. We assigned all simulated curves based on a common observed season shape to the same fold, in order to account for the dependence between these simulated curves. We did not account for potential dependencies due to repeated sampling of the error term, as we believed the season shape was more consequential with respect to inducing dependence between simulated curves. 

4. Run the super learner algorithm:
   a. Train each learner on the data set using 15-fold cross-validation and calculate its cross-validated risk as the average risk across the 15 validation sets.
   b. Generate predictions with each component learner on the full data set.
   c. Regress the current prediction target on each learner's predictions in the full sample (*N* = 15,000) using a non-negative least squares model constrained to produce coefficients in the range [0,1] and which sum to 1 [@Naimi2018-fv]. These constraints are not necessary but are theoretically expected to improve the stability of the ensemble estimator [@Van_der_Laan2007-ml]. This model is referred to as the _metalearner_ [@Coyle2020-ze].
   d. Calculate the cross-validated risk for the ensemble super learner as in step 4a.

In all, we trained **[62]** component models in for each prediction target and week of the season, including variations on tuning and input parameters for the various learners (Table 2).


## Component models

We used the same library of component models to build the super learners for all three prediction targets (Table 3). The component models included a standard linear regression model, generalized additive models [@Wood2019-rc], loss-based regression models (e.g., lasso, ridge) [@Tibshirani1996-vt], random forests [@Breiman2001-vm], support vector regression [@Hastie2009-ft], neural networks [@Hastie2009-ft], loess [@Harrell2015-cd], and polynomial multivariate adaptive regression splines (polyMARS) [@Kooperberg2019-ma]. Theory and finite-sample demonstrations support the idea that the super learner should benefit from being provided with a large variety of models, even if some of those models perform poorly in the given data [@Van_der_Laan2007-ml]. Therefore, we specified the component learners and tuning parameter sets to attempt to probe a large portion of the component model space.

In all cases the learning algorithms incorporating these various component models were implemented using functions provided in the `sl3` [@Coyle2020-ze] and `SuperLearner` [@Polley2019-sl] packages in R.


```{r candmodels-tuning-table}
tunetab <- tribble(
  ~ Model,
  ~ `Tuning parameters`,
  ~ `R package`,

  "Linear regression",
  "Variables screened for inclusion first using a cross-validated lasso procedure to omit variables with zero-valued coefficients.",
  "base",

  "Random forest (regression trees)",
  "number of trees = [50, 100, 200, 500], terminal node sizes = [3, 5, 10]",
  "randomForest",

  "Generalized additive model",
  "gamma penalty = [1, 2, 3, 4, 5]",
  "gam",

  "Support vector regression",
  "kernel = [radial, polynomial], degree = [1, 2, 3] applied only to polynomial",
  "svm",

  "Loss-based regression",
  "penalty = [lasso, ridge]",
  "glmnet",

  "Elastic net",
  "alpha penalty = [0.25, 0.5, 0.75]",
  "glmnet",

  "Neural network",
  "number of nodes in hidden layer = [5, 10, 25, 50, 75, 100], decay = [0, 0.005, 0.1, 0.2, 0.4]",
  "nnet",

  "Loess",
  "span = [0.25, 0.5, 0.75, 1]",
  "base",

  "Polynomial multivariate adaptive regression spline",
  "gcv penalty = [2, 4, 6, 8, 10]",
  "polspline",
  )

cap <- "Component models and tuning parameters."
fn <- "For the random forest and neural network learners, all combinations of the tuning parameters shown were proposed."

if (knitr::is_latex_output()) {
  tunetab %>%
    kable(
      booktabs = T,
      format = "latex",
      caption = cap
    ) %>%
    kable_styling(
      latex_options = c("striped", "scale_down"),
      ) %>%
    column_spec(1:2, width = "2.1in") %>%
    kableExtra::footnote(general_title = "", fn)
} else {
  tunetab %>%
    flextable(., cwidth = c(2.1, 3.2, 1.2)) %>%
    fit_to_width(., max_width = 8) %>% 
    bold(., part = "header") %>%
    add_header_lines(paste("Table 3:", cap)) %>%
    add_footer_lines(fn)
}
 
```

### Linear regression

For each week of the flu season, we specified a linear regression model that included covariates chosen by a lasso-based screening algorithm [@Friedman2019-uw; @Polley2019-sl] in which covariates with non-zero coefficients were identified using 10-fold cross validation and then entered into the linear regression. The linear model was specified using the `glm()` function in R with a Gaussian error distribution and identity link.

### Loss-based regression

We implemented several loss-based regression learners using the `glmnet` package in R [@Friedman2019-uw]. These learners included a learner with the standard lasso penalty [@Tibshirani1996-vt], a learner with the ridge penalty [@Tibshirani1996-vt], and three versions of the elastic net model [cite] using different $\alpha$ penalties (Table 3). The elastic net model combines lasso and ridge penalties within the same model [citation].

### Random forests

We implemented random forest algorithms [@Breiman2001-vm] using the `randomForest` package [@Liaw2018-fe], tuning parameters governing the number of regression trees and minimum observations allowed in terminal nodes (Table 3).

### Support vector regression

We implemented several support vector regressions [@Hastie2009-ft] using the `e1071` package [@Meyer2019-gr]---one using a radial kernel and three others using a polynomial kernel of differing degrees (Table 3).

### Neural networks

We implemented several neural networks [@Hastie2009-ft] using the `nnet` package [@Ripley2020-ds]. All learners proposed contained a single hidden layer, altering tuning parameters governing the number of nodes in this hidden layer and the magnitude of decay applied to node weights (Table 3).

### Loess

We submitted several loess-based learners as implemented in base R, varying the smoothing kernel span (Table 3). 

### Polynomial multivariate adaptive regression splines

We submitted several polyMARS models using the `polspline` package [@Kooperberg2019-ma], varying the generalized cross-validation value, larger values of which produce smaller models [@Kooperberg2019-ma].

## Technical details

The super learner algorithm was implemented on a high-performance computing cluster housed at the Center for Computation and Visualization at Brown University. Each run was submitted to a large-memory node and parallelized across 32 central processing units based on the native functionality provided in `sl3` [@Coyle2020-ze] via the `delayed` [@Coyle2020-rq] and `future` [@Bengtsson2020-oz] packages in R.


# Results

## Peak rate

For the peak rate prediction target, the ensemble super learner produced risk estimates similar to the simple mean prediction and generally worse than the discrete super learner (Table 4). Throughout the flu season, the discrete super learner (best-performing component model) consistently exhibited a lower average risk than a prediction based on the mean, indicating better predictive ability. Furthermore, the discrete super learner's cross-validated risk generally decreased as the season progressed. In each week a different discrete super learner was selected, with neural networks and penalized regressions being selected in the first several weeks, support vector regressions being selected toward the middle of the flu season, and polyMARS being selected more frequently toward the end of the season.

```{r risks-datadir}

risktab_cnames <- c(
  Week = "Week",
  SuperLearner = "Super Learner",
  BestComponent = "Discrete SL Risk",
  Mean = "Mean",
  BestComponentModel = "Discrete SL"
)

risktab_cnames_word <- function(dat) {
  set_header_labels(
    dat,
    Week = "Week",
    SuperLearner = "Super Learner",
    BestComponent = "Discrete SL Risk",
    Mean = "Mean",
    BestComponentModel = "Discrete SL"
  )
}

```

```{r tab04-peakrate-risk-table}

pkrate_risks <- fread(file.path(assetdir, "table-04_peakrate-risks.csv"))

cap <- "Cross-validated risks for the ensemble super learner, discrete super learner, and simple mean predictions of peak hospitalization rate, by week of influenza season. Estimates presented as mean risk (standard error)."

fn <- c("The ensemble super learner (SuperLearner) is the prediction generated as a weighted combination of component model predictions. The discrete super learner (Discrete SL) is the best-performing component model. The mean (Mean) is the average peak hospitalization rate per 100,000 population across the simulated hospitalization curves. The absolute error loss function used to optimized the super learner targets the median peak hospitalization rate.")

if (knitr::is_latex_output()) {
  pkrate_risks %>%
    kable(
      col.names = risktab_cnames,
      booktabs = TRUE,
      format = "latex",
      caption = cap
    ) %>%
    kableExtra::kable_styling(
      latex_options = "scale_down"
    ) %>%
    kableExtra::footnote(
      general_title = "",
      fn,
      threeparttable = TRUE
    )
} else {
  pkrate_risks %>%
    flextable(
      cwidth = c(0.6, rep(1.1, 3), 2.6)
    ) %>%
    risktab_cnames_word() %>%
    bold(., part = "header") %>%
    add_header_lines(paste("Table 4:", cap)) %>%
    add_footer_lines(fn)
}

```


## Peak week

For the peak week prediction target, the ensemble superlearner generally produced risk estimates close to those of the peak week predictions based on a simple mean (Table 5). The discrete super learner consistently exhibited lower mean risk than both the ensemble super learner and predictions based on the simple mean. As with peak rate, the discrete super learner tended to perform better as the flu season progressed. Over the first 6 weeks, some form of neural network was chosen as the discrete super learner in 5 of those weeks. Throughout the rest of the season, various versions of the random forest algorithms predominated the list of discrete super learners.


```{r tab05-peakweek-risk-table}

pkweek_risks <- fread(file.path(assetdir, "table-05_peakweek_risks.csv"))

cap <- "Cross-validated risks for the ensemble super learner, discrete super learner, and mean predictions of the week in which the peak hospitalization rate occurs, by week of influenza season. Estimates presented as mean risk (standard error)."

fn <- c("The ensemble super learner (SuperLearner) is the prediction generated as a weighted combination of component model predictions. The discrete super learner (Discrete SL) is the best-performing component model. The mean (Mean) is the average week during which the peak hospitalization rate occurred across the simulated hospitalization curves. The absolute error loss function used to optimized the super learner targets the median peak hospitalization rate.")

if (knitr::is_latex_output()) {
  pkweek_risks %>%
    kable(
      col.names = risktab_cnames,
      booktabs = TRUE,
      format = "latex",
      caption = cap
    ) %>%
    kableExtra::kable_styling(
      latex_options = "scale_down"
    ) %>%
    kableExtra::footnote(
      general_title = "",
      fn,
      threeparttable = TRUE
    )
} else {
  pkweek_risks %>%
    flextable(
      cwidth = c(0.6, rep(1.1, 3), 2.6)
    ) %>%
    risktab_cnames_word() %>%
    bold(., part = "header") %>%
    add_header_lines(paste("Table 5:", cap)) %>%
    add_footer_lines(fn)
}

```


## Cumulative hospitalizations

For the cumulative hospitalizations prediction target, the ensemble super learner produced risk estimates generally near to those based on a prediction based on the simple mean (Table 6). Again, the discrete super learner consistently outperformed both the ensemble super learner and the simple mean prediction, improving substantially as the flu season progressed. Some form of support vector regression was chosen most frequently as the discrete super learner, with penalized regressions, polyMARS, and neural networks among the other models chosen in at least one of the weeks.

```{r tab06-cumhosp-risk-table}

cumhosp_risks <- fread(file.path(assetdir, "table-06_cumhosp_risks.csv"))

cap <- "Cross-validated risks for the ensemble super learner, discrete super learner, and mean predictions of the cumulative hospitalization rate, by week of influenza season. Estimates presented as mean risk (standard error)."

fn <- c("The ensemble super learner (SuperLearner) is the prediction generated as a weighted combination of component model predictions. The discrete super learner (Discrete SL) is the best-performing component model. The mean (Mean) is the average cumulative hospitalization rate per 100,000 across the simulated hospitalization curves. The absolute error loss function used to optimized the super learner targets the median peak hospitalization rate.")


if (knitr::is_latex_output()) {
  cumhosp_risks %>%
    kable(
      col.names = risktab_cnames,
      booktabs = TRUE,
      format = "latex",
      caption = cap
    ) %>%
    kableExtra::kable_styling(
      latex_options = "scale_down"
    ) %>%
    kableExtra::footnote(
      general_title = "",
      fn,
      threeparttable = TRUE
    )
} else {
  cumhosp_risks %>%
    flextable(
      cwidth = c(0.6, rep(1.1, 3), 2.6)
    ) %>%
    risktab_cnames_word() %>%
    bold(., part = "header") %>%
    add_header_lines(paste("Table 6:", cap)) %>%
    add_footer_lines(fn)
}

```


# Discussion

In our study, we found that discrete super learners were consistently better at predicting median peak rate, peak week, and cumulative hospitalizations compared to a simple mean prediction when fit to a distribution of hypothetical influenza hospitalization curves. On the contrary, the ensemble super learner generally tended to perform comparably to a simple mean prediction and consistently worse than the discrete super learners with respect to cross-validated risk. In fact, the discrete super learner routinely provided the best predictions (lowest mean cross-validated risk) across all prediction targets. The discrete super learner also appeared to provide greater improvements over the simple mean prediction as a season wore on, a result that comports with similar findings in the domain of influenza-like illness [@Ray2018-ef]. 

The relative underperformance of the ensemble super learner, a feature observed across all prediction targets, raises several questions regarding the likely usefulness of the super learner algorithm in the context of predicting influenza hospitalizations. It may be that the ensemble super learner should not be relied upon if used to generate in-season forecasts given the small sample size of observed hospitalization curves [@Centers_for_Disease_Control_and_Prevention_undated-vt] and the fact that we did not observe improvements in prediction in a sample size of 15,000. As discussed in the introduction, the ensemble super learner has desirable asymptotic oracle properties, theoretically guaranteed to preform as well as the discrete super learner [@Van_der_Laan2007-ml]. Furthermore, the ensemble super learner is generally assumed not to be affected adversely by the inclusion of poor algorithms or overfitting, as the cross-validation procedure should result in a final weighted ensemble that assigns low or no weight to poor-performing predictions [@Polley2011-oz]. It is possible that adding additional algorithms such as gradient boosted models or exploring a larger range of tuning parameters for the models incorporated in our analysis would have improved the performance of the ensemble learner.

Our findings should be interpreted in light of several limitations. 

First, while our simulating 15,000 hypothetical hospitalization curves provided an adequate sample size with which to train the super learner---an analysis that would not have been possible using only the 15 observed flu seasons---the validity of our findings rely on the assumption that the distribution of simulated curves accurately characterizes the hypothetical distribution of plausible seasonal influenza hospitalization trajectories. We did not attempt to rigidly match the simulated distributions to their empirical counterparts so as not to restrict the possibility of atypical influenza seasons that could be, but perhaps have not yet been, observed. In other words, we did not assume the 15 observed seasons captured the full range of possible values for the selected prediction targets. A related issue concerns the selection of the $\lambda$ value for each trend filter fit to an empirical season, as these fits were used as templates for the simulated hospitalization curves. This choice of penalty was somewhat arbitrary. Selecting the $\lambda$ penalty that minimized the root mean squared error of the trend filter predictions with respect to the empirical data may have resulted in a distribution of simulated curves with different properties. However, while methods for selecting the $\lambda$ penalty for the $\ell_1$ trend filter have been proposed [@Yamada2016-fq], to our knowledge, no consensus currently exists regarding the best procedure to do so.

Second, we did not include any mechanistic models in the component learner library, such as compartmental or agent-based epidemic models. Future work could incorporate such models or, alternatively, the predictions generated by an ensemble super learner could be incorporated into a wider ensemble model that incorporates predictions from such models.

Third, our approach to simulating hypothetical influenza hospitalization curves incorporated only information on hospitalization rates, without relying on other mechanistic factors to generate these curves. Therefore, we were unable to examine whether incorporating other influenza-related surveillance data (e.g., ILI, viral activity [@Centers_for_Disease_Control_and_Prevention_undated-vt]) or climate data  may have improved the super learner's predictions.

Fourth, we did not model hospitalization rate time series explicitly, in order to increase the number of component models available for inclusion [@Coyle2020-ze]. Nonetheless, we did attempt to incorporate information regarding the trajectory of influenza hospitalization rates up to a given week by incorporating lagged hospitalization rates, differences between the hospitalization rates in the current week and prior weeks, and selected interactions. Future work should explore the potential for explicit modeling of time series, perhaps using an "online" ensemble super learner, which continuously updates the super learner's predictions based on its predictions in prior weeks [@Van_der_Laan2018-xq].

Finally, because we focused on predicting hospitalizations due to seasonal influenza, these findings do not have implications for predicting hospitalizations during years in which a pandemic influenza strain is circulating.

The super learner remains a promising algorithm for predicting influenza-related hospitalizations, but future work should determine the proper breadth of component learners such that the ensemble learner can be relied upon to perform as well as the discrete super learner. Adapting this approach to incorporate empirical surveillance data as well as other data relevant to influenza dynamics (e.g., viral activity, weather patterns), and to focus on hospitalizations from specific settings (e.g., skilled nursing facilities), may also be fruitful avenues for future research. 

# Software and code

The code necessary to reproduce these results and the output from the super learner procedures are provided in a repository at Open Science Framework **(doi: XXXXXXXXX)**. These materials are also housed permanently at the Brown Digital Repository **(doi/url: XXXXXXXXXXX)**, and the package can be installed directly via the Github repository ([https://www.github.com/jrgant/FluHospPrediction](https://www.github.com/jrgant/FluHospPrediction)).

# Declarations

## Acknowledgment

We thank Ashley Naimi, Laura Balzer, and Nicholas Reich for their comments on the study aims and the super learner approach. The super learner algorithm was fit using computational resources and services at the Center for Computation and Visualization, Brown University.

## Funding statement

This work was funded by an unrestricted grant from Sanofi (PI: Andrew Zullo). The funders did not assist in the statistical analysis nor did they have a say in the final decision to submit the manuscript for publication.

## Competing interests

[solicit competing interests from co-authors]


\newpage

# Supporting information

**S1 Fig. Linear trend filter fits to observed influenza hospitalization curves.** \bigskip

```{r s1fig, out.width = "90%", fig.align="center"}

results_folder <- "C:/Users/jason/Documents/Github/FluHospPrediction/results"


if (knitr::is_latex_output()) {
  include_graphics(file.path(results_folder, "trendfilter-fit-facet.pdf"))
} else {
  include_graphics(file.path(results_folder, "trendfilter-fit-facet.png"))
}

```


\newpage
\noindent
**S2 Table. Number of simulated curves based on each observed flu season (Emerging Infections Program).**

```{r s2table}
tmpcts <- fread(file.path(assetdir, "table-s02_template-counts.csv"))

if (knitr::is_latex_output()) {
  tmpcts %>%
    knitr::kable(
      format = "latex",
      booktabs = TRUE
    )
} else {
  tmpcts %>%
    flextable %>%
    bold(part = "header") %>%
    autofit
}

```

\newpage
\noindent
**S3 Figure. Simulated hospitalization curves generated based on each empirical shape template.**  Note that because each parameter used in the curve-generating function was drawn independently, simulated hospitalization curves based on an empirical shape template should have a similar shape (i.e., unimodal, bimodal) but may have very different peak and/or cumulative hospitalization rates compared to the empirical template.

```{r s3-fig-tempsimplot}
knitr::include_graphics(file.path(assetdir, "tempsim_plot.png"))
```

\newpage
\noindent
**S4 Table. Weekly cross-validated risks across the component learners used to predict peak hospitalization rate per 100,000 population.**

```{r s4table-peakrate-meanrisk}

rpsum <- fread(file.path(assetdir, "table-s04_risk-distbyweek-pkrate.csv"))

if (knitr::is_latex_output()) {
  rpsum %>%
    kable(
      format = "latex",
      booktabs = TRUE,
      digits = 2,
    )
} else {
  rpsum %>%
    flextable(cwidth = c(0.6, rep(1.15, 5))) %>%
    colformat_num(
      j = 2:6,
      digits = 2
    ) %>%
    bold(., part = "header")
}

```

\newpage
\noindent
**S5 Table. Weekly cross-validated risks across the component learners used to predict peak week.**

```{r s5table-peakweek-meanrisk}

rwsum <- fread(file.path(assetdir, "table-s05_risk-distbyweek-pkweek.csv"))

if (knitr::is_latex_output()) {
  rwsum %>%
    kable(
      format = "latex",
      booktabs = TRUE,
      digits = 2,
    )
} else {
  rwsum %>%
    flextable(cwidth = c(0.6, rep(1.15, 5))) %>%
    colformat_num(
      j = 2:6,
      digits = 2
    ) %>%
    bold(., part = "header")
}

```

\newpage
\noindent
**S6 Table. Weekly cross-validated risks across the component learners used to predict cumulative hospitalization rate per 100,000 population.**

```{r s6table-cumhosp-meanrisk}

rwsum <- fread(file.path(assetdir, "table-s06_risk-distbyweek-cumhosp.csv"))

if (knitr::is_latex_output()) {
  rwsum %>%
    kable(
      format = "latex",
      booktabs = TRUE,
      digits = 2,
    )
} else {
  rwsum %>%
    flextable(cwidth = c(0.6, rep(1.15, 5))) %>%
    colformat_num(
      j = 2:6,
      digits = 2
    ) %>%
    bold(., part = "header")
}

```

\newpage
\noindent
**S7 Figure. Peak rate, ensemble learner weights as a function of log mean cross-validated risk.** Ensemble learner weights are assigned by regressing the prediction target on corresponding predictions made by each component learner. The coefficients estimated for each independent predictor (i.e., component learner) represent the weight given to the learner's predictions in the final ensemble super learner prediction.

```{r s7fig-risks-weights}

knitr::include_graphics(file.path(assetdir, "pkrate_rw_plot.png"))

```

\newpage
\noindent
**S8 Figure. Peak week, ensemble learner weights as a function of log mean cross-validated risk.** Ensemble learner weights are assigned by regressing the prediction target on corresponding predictions made by each component learner. The coefficients estimated for each independent predictor (i.e., component learner) represent the weight given to the learner's predictions in the final ensemble super learner prediction.

```{r s8fig-risks-weights}

knitr::include_graphics(file.path(assetdir, "pkweek_rw_plot.png"))

```


\newpage
\noindent
**S9 Figure. Cumulative hospitalizations, ensemble learner weights as a function of log mean cross-validated risk.** Ensemble learner weights are assigned by regressing the prediction target on corresponding predictions made by each component learner. The coefficients estimated for each independent predictor (i.e., component learner) represent the weight given to the learner's predictions in the final ensemble super learner prediction.

```{r s9fig-risks-weights}

knitr::include_graphics(file.path(assetdir, "cumhosp_rw_plot.png"))

```


\newpage

# References

\bibliography{references}

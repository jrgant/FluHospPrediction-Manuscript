---
title: "Predicting seasonal influenza hospitalization using an ensemble super learner: a simulation study"

bibliography: references.bib
output: rticles::plos_article
csl: plos.csl

header-includes:
    - \usepackage{booktabs}
    - \usepackage{threeparttable}

always_allow_html: true
---

**Short Title:** Predicting influenza hospitalizations with a super learner

_Two authors from Sanofi may be added after they review the manuscript. Current author order tentative._

Jason R. Gantenberg,(1,2) Kevin W. McConeghy,(2,3) Jon Steingrimsson,(4) Chanelle J. Howe,(5) Andrew R. Zullo(1,2)


**Corresponding author:** Jason R. Gantenberg (jrgant@brown.edu)

1. Department of Epidemiology, Brown University School of Public Health, 121 S. Main St., Providence, RI, 02912

2. Department of Health Services, Policy and Practice, 121 S. Main St., Providence, RI, 02912

3. Providence VA Medical Center, 830 Chalkstone Ave., Providence, RI, 02908

4. Department of Biostatistics, Brown University School of Public Health, 121 S. Main St., Providence, RI 02912

5. Department of Epidemiology, Center for Epidemiology and Environmental Health, Brown University School of Public Health, 121 S. Main St., Providence, RI, 02912

# Abstract

Recent interest in influenza forecasting, spurred in part by focused efforts by the Centers for Disease Control and Prevention in the United States to support forecasting collaborations, has led to improvement in prediction of seasonal influenza dynamics. To date, less focus has been paid to predicting seasonal influenza-related hospitalizations. We conducted a simulation study to measure the performance of a machine-learning algorithm (super learner) to predict three features of a given seasonal influenza epidemic: peak hospitalization rate, the week during which the hospitalization rate peaks, and the cumulative hospitalizations. We incorporated a set of [XX] component models and fit the super learner to a distribution of 15,000 simulated influenza hospitalization curves in order to generate weekly predictions for the prediction targets. We compared the performance of the ensemble super learner (a weighted combination of component model predictions), the discrete super learner (best-performing component model), and a prediction based on a simple mean of a given outcome across all simulated hospitalization curves. We found that the super learner algorithm was consistently capable of choosing individual component models that improved predictions of the three seasonal prediction targets above those of a simple mean prediction. However, we also found that the ensemble super learner consistently had higher prediction error than the discrete super learners and often performed comparably to a simple mean prediction. Given that influenza hospitalization forecasting depends on small sample sizes derived from past flu seasons, the ensembles' underperformance in a large sample of hypothetical influenza seasons suggests future work should investigate optimal combinations of component models that would improve ensemble predictions to the point where they consistently match the predictive accuracy of the best-performing component model.

# Author Summary

Influenza forecasting is a still-maturing science, but notable improvements have been made over the past decade in predicting influenza-like activity. In addition, increasing focus on machine learning approaches has provided hope that such approaches may further improve predictive accuracy. Less work, however, has focused on predicting influenza-related hospitalizations, and we sought to investigate the potential for machine learning approaches to predict several features of influenza hospitalization dynamics that could help inform the allocation of public health efforts and resources during a flu season. We used a machine learning approach, called _super learner_, which is capable of taking a large number of different statistical models and either combining them into a single prediction (the ensemble prediction) or choosing the best-performing model. Our results indicate that ensemble predictions may not perform as well as individual models under certain conditions and that future work should focus on investigating when ensemble predictions based on machine learning would be expected to improve predictive accuracy.

# Meta

**Target journal:**

- _PLoS Computational Biology_

\noindent
**Section:**

- Epidemiology and Clinical/Translational Studies

\noindent
**Potential editors:**

- Benjamin Althouse
- Miles Davenport
- Matthew Ferrari
- Roger Kouyos
- James Lloyd-Smith

\noindent
**Potential reviewers:**

- Ryan J. Tibshirani (co-author on paper we use for curve simulation)
- Logan C. Brooks (co-author on paper we use for curve simulation)
- Roni Rosenfeld (co-author on paper we use for curve simulation)
- Sherri Rose (expert in machine/super learning)
- David A. Osthus (flu modeler highly whose work came highly recommended by Nick Reich)
- Samrachana Adhikari (biostatistician and former lab member in the Carnegie Mellon group [with RJ Tibshirani)
- Oleg Sofrygin (sl3 package author)
- Nima Hejazi (sl3 package author)
- Jeremy Coyle (sl3 package author -- I did have a brief exchange about the sl3 package with him, so it may not be appropriate to have him as a reviewer)

```{r include=FALSE}
pacman::p_load(
  FluHospPrediction,
  ggplot2,
  ggthemes,
  gridExtra,
  data.table,
  tidyverse,
  kableExtra,
  knitr,
  flextable
)

opts_chunk$set(
  echo = FALSE,
  cache = TRUE
)

# paper output directory
assetdir <-
  "C:/Users/jason/Documents/Github/FluHospPrediction/results/paper_output/"

# function to select which figure format to use based on pandoc output format
putfigure <- function(slug) {
  if (knitr::is_latex_output()) {
    knitr::include_graphics(paste0(assetdir, slug, ".pdf"))
  } else {
    knitr::include_graphics(paste0(assetdir, slug, ".png"))
  }
}

```

# Introduction

Between 2010 and 2017, approximately 140,000--570,000 individuals have been hospitalized and 12,000--51,000 have died annually due to seasonal influenza in the United States [@Centers_for_Disease_Control_and_Prevention2020-uc]. Being able to predict the future burden of influenza-related hospitalizations during a given influenza season can assist policymakers, public health officials, and physicians in allocating resources appropriately and preparing for expected changes in hospitalization rates [@Lutz2019-co].

While influenza forecasting is a still-maturing science [@Reich2019-uk; @Chretien2014-dy], researchers have made considerable progress over the past decade in improving the quality of and capacity for forecasting influenza-like illness (ILI) [@Reich2019-uk], thanks in part to the FluSight forecasting competitions sponsored by the Centers for Disease Control and Prevention (CDC) since the 2013--14 flu season [@Reich2019-uk]. Many different types of models have been used to generate forecasts, including statistical time series models [@Reich2019-uk; @Biggerstaff2018-ns], Bayesian methods [@Chretien2014-dy; @Brooks2015-fl], and agent-based models [@Chretien2014-dy], among others. However, ensemble methods have emerged as a promising approach to improving further the accuracy and stability of epidemic predictions due to ensembles' ability to combine predictions from multiple estimators in a principled manner [@Reich2019-ca; @Ray2018-ef; @Lutz2019-co].

Ensemble methods combine predictions generated by a set of component models [@Reich2019-ca; @Hastie2009-ft; @Wolpert1992-pw; @Breiman1996-ez]. In some cases, ensembles aggreggate component model predictions by weighting better predictions more highly in the final ensemble prediction [@Ray2018-ef; @Reich2019-ca], though other weighting criteria can be applied [@Ray2018-ef]. One compelling rationale for using ensemble predictions rests in their ability to borrow the strengths and avoid the weaknesses of any single class of models used as inputs into the ensemble [@Van_der_Laan2007-ml]. This feature tends to lead not only to more accurate predictions but to more stable ones that can be applied across a range of scenarios [@Ray2018-ef]. The CDC's primary in-season ILI forecasts are now based on an ensemble forecast generated by aggregating predictions from a growing library of individual forecasts submitted by research teams around the United States [@Reich2019-uk].

To date, most work in influenza forecasting has focused on ILI [@Reich2019-uk; @Biggerstaff2018-ns; @McGowan2019-ph; @Kandula2018-sq; @Brooks2015-fl], with considerably less work having been focused on predicting influenza-related hospitalization rates [@Kandula2019-tg]. Because the dynamics of influenza-related hospitalizations may evolve differently than those of influenza transmission itself over the course of a flu season---at the very least because consequent hospitalizations lag influenza incidence by a week or two [citation needed]---and because hospitalization rates are an independent signal of the severity of circulating flu strains [@Biggerstaff2018-ns], using ensemble methods to predict hospitalization rates could prove to be a value complement to existing efforts to forecast ILI.

One ensemble machine learning method in particular, dubbed "super learner" [@Van_der_Laan2007-ml; @Polley2010-cb; @Polley2011-oz], exhibits a number of desirable properties that suggest it may be a powerful tool for predicting flu hospitalizations. First, its developers have demonstrated that, asymptotically, the super learner is an oracle estimator, performing as well as the best-fitting component model and converging almost as quickly [@Polley2010-cb] [also will want to read and cite the 2003 paper of van der Laan's]. Second, this oracle property has been found to apply approximately finite samples [@Polley2010-cb; @Polley2011-oz; @Van_der_Laan2007-ml]. Finally, several packages have been developed to implement the super learner algorithm [@Polley2019-sl; @Coyle2020-ze], providing researchers easy access to a large library of component models and a means to efficiently calculate cross-validated prediction risks with respect to a desired loss function [@Coyle2020-ze].

In this study, we trained an ensemble learner on a distribution of simulated influenza hospitalization curves to generate predictions for three national-level seasonal target parameters chosen based on the targets specified in CDC forecasting competitions [@Centers_for_Disease_Control_and_Prevention_undated-tx], in order to compare the performance of the ensemble learner against the best-performing component model and a naive historical average prediction for each of these targets across the 30 weeks of a typical flu season.

# Methods

## Empirical data

We downloaded publicly available surveillance data on seasonal influenza-related hospitalizations from the CDC's FluView Interactive dashboard [@Centers_for_Disease_Control_and_Prevention_undated-vt]. Specifically, we included data from the Emerging Infections Program (EIP) beginning with the 2003--2004 season and ending with the 2018--2019 season, omitting the 2009-2010 pandemic influenza year. The EIP contains data on influenza-related hospitalizations in California, Colorado, Connecticut, Georgia, Maryland, Minnesota, New Mexico, New York, Oregon, and Tennessee [@Centers_for_Disease_Control_and_Prevention_undated-vt]. Since the 2009--2010 season the FluSurv Network has included between 3 and 6 states in addition to those represented in the EIP data, depending on the year [@Centers_for_Disease_Control_and_Prevention_undated-vt]. In order to maintain consistency within the empirical data, and to increase the number of flu seasons available to inform curve simulation, we did not include data from states outside the EIP.

Typically, the CDC releases data for epiweeks 40--53 and 1--17 (approximately October through April of the next year) [@Centers_for_Disease_Control_and_Prevention_undated-vt; @Centers_for_Disease_Control_and_Prevention_undated-pu]. We renumbered the epiweeks 1--30, omitting epiweek 53, as only three seasons had influenza hospitalization data recorded in this week. When we refer subsequently to a "flu season", we refer to this 30-week time period.

## Prediction targets

Following from the CDC's Flu Sight challenge [@Centers_for_Disease_Control_and_Prevention_undated-tx], we focused on three season-level prediction targets:

1. _Peak rate_, defined as the highest weekly rate of influenza-related hospitalizations throughout the course of a flu season (per 100,000 population);
2. _Peak week_, defined as the week during which this peak rate occurred; and
3. _Cumulative hospitalizations_, defined as the cumulative influenza-related hospitalization rate over the 30 weeks of the flu season (per 100,000 population). 

Fifteen empirical observations were available for each prediction target, corresponding to the number of flu seasons contained in the CDC's surveillance data (Table 1).

```{r target-table}

targets <- fread(file.path(assetdir, "table-01_prediction-targets.csv"))

cap <- "Empirical distributions of peak hospitalization rate, peak week and cumulative hospitalization rate in the United States, 2003--2019."

fn <- "Source: CDC FluView (Emerging Infections Program). Peak rate and cumulative rate expressed per 100,000 population. Pandemic influenza season 2009--2010 omitted."

if (knitr::is_latex_output()) { 
  targets %>%
    kable(
      format = "latex",
      booktabs = TRUE,
      caption = cap
    ) %>%
    kableExtra::footnote(
      fn,
      general_title = "",
      threeparttable = TRUE
    )
} else {
  targets %>%
    flextable(., cwidth = 1.6) %>%
    bold(., part = "header") %>%
    add_header_lines(., paste("Table 1:", cap)) %>%
    add_footer_lines(., fn)
}

```

## Hospitalization curve simulation

To simulate a distribution of seasonal influenza hospitalization curves, we adapted an approach by Brooks et al. originally used to predict influenza-like illness [@Brooks2015-fl]. First, we fitted a linear trend filter [@Kim2009-bz; @Tibshirani2014-tr] to the 15 observed influenza hospitalization curves from the EIP using the `glmgen` package in R [@Arnold2015-tb, @RCore2020-ct]. The `glmgen` linear trend filter is a penalized method that fits a piecewise linear function to a time series, testing 50 values of the penalty ($\lambda$) by default [@Arnold2015-tb]. We used these fits as templates for the simulated influenza hospitalization curves. In all cases, we selected the fit that used the 25th $\lambda$ value tested (S1 Fig) based on visual inspection of the overall distribution of simulated curves. 

Next, we incorporated the 15 fit objects into a modified version of the curve generation scheme described in Brooks et al. [@Brooks2015-fl]. Our notation borrows and follows closely from theirs.

Briefly, Brooks et al. conceptualize as seasonal influenza curve as some function plus noise [@Brooks2015-fl]. Adapted to the hospitalization case, the hospitalization rate ($y^s_i$) in season $s$ and week $i$ is given by

$$y^s_i = f^s(i) + \epsilon^s_i, \epsilon \sim N(0, \tau^s),$$

where $f^s(i)$ is a hospitalization rate and $\epsilon^s_i$ is normally distributed error term with  mean 0 and variance $\tau^s$.

For each empirical season $s$, we use its linear trend filter fit and average the squared residuals over $i$ to estimate $\tau^s$:

```{r, results="asis"}
if (knitr::is_latex_output()) {
  cat("$$\\left( \\hat{\\tau}^s \\right)^2 = \\genfrac{}{}{0pt}{}{\\text{avg}}{i} \\left[ y^s_i - \\hat{f}^s (i) \\right]^2.$$")
} else {
  cat("$$( \\hat{\\tau}^s )^2 = \\substack{ \\text{avg} \\\\ i } [ y^s_i - \\hat{f}^s (i) ]^2.$$")
}
```
For each simulated curve, each of the following parameters is sampled randomly and independently of one another:

$$\langle f, \sigma, \nu, \theta, \mu \rangle,$$

where $f$ denotes a randomly selected vector of estimated hospitalization rates based on a linear trend filter fit to empirical season $s$, $\sigma$ denotes the squared error of a linear trend filter fit to randomly sampled season $s'$ and averaged across all weeks, $\nu$ denotes a random uniform draw from the range [0.75, 1.25], $\theta$ denotes a random draw from a uniform distribution bounded by the minimum and maximum peak hospitalization rates from the 15 trend filter fits, and $\mu$ denotes a random draw from a uniform distribution bounded by the minimum and maximum peak weeks based on the 15 trend filter fits (Table 2).

```{r sim-param-input-table}

ptab <- tibble::tribble(
  ~Parameter,
  ~Description,

  "Shape",
  "$f \\sim U \\{ \\hat{f} : \\text{historical season } s \\}$",

  "Noise",
  "$\\sigma \\sim U \\{\\hat{\\tau}^{s'} \\text{ : historical season } s' \\}$",

  "Pacing",
  "$\\nu \\sim U[0.75, 1.25]$, stretches the curve around the peak week",

  "Peak height",
  "$\\theta \\sim U\\left[\\theta_{min} , \\theta_{max}\\right]$",

  "Peak week",
  "$\\mu \\sim U[\\mu_{min}, \\mu_{max}]$"
)

cap <- "Input parameters to the influenza hospitalization curve generating function."
fn <- "The noise parameter for each simulation is drawn separately, and may come from a season different than the season used as the shape."

if (knitr::is_latex_output()) {
    ptab %>%
      kable(
        format = "latex",
        escape = FALSE,
        booktabs = TRUE,
        caption = cap
      ) %>%
      kableExtra::footnote(
        fn,
        general_title = "",
        threeparttable = TRUE
      )
} else {
  ptab %>%
    flextable(cwidth = c(1, 4)) %>%
    bold(part = "header") %>%
    add_header_lines(paste("Table 2:", cap)) %>%
    add_footer_lines(fn)
}
```

The generating function for hospitalization rate in week $i$ of simulated season $sim$ therefore is given by:

```{r, results="asis"}
if (knitr::is_latex_output()) {
  cat("$$f^{sim} (i) = \\frac{\\theta}{\\text{max}_j f(j)} \\left[ f \\left( \\frac{i - \\mu}{v} + \\genfrac{}{}{0pt}{}{\\text{arg max }{j}}{f(j)} \\right) \\right] + \\epsilon_i, \\epsilon_i \\sim N(0, \\hat{\\tau}^{s'}),$$")
} else {
  cat("$$f^{sim} (i) = \\frac{\\theta}{\\text{max}_j f(j)} \\left[ f \\left( \\frac{i - \\mu}{v} + \\substack{\\text{arg max }{j} \\\\ f(j) } \\right) \\right] + \\epsilon_i, \\epsilon_i \\sim N(0, \\hat{\\tau}^{s'}),$$")
}
```

where $f(j)$ now denotes the vector of fitted hospitalization rates from the linear trend filter fit (randomly selected shape $f$) and $j$ the integer week for which we want to retrieve the prediction from this fit (equal to $i$). $max_j f(j)$ denotes the peak hospitalization rate from the selected shape, whereas `r ifelse(knitr::is_latex_output(), "$\\genfrac{}{}{0pt}{}{\\text{arg max } {j}}{f(j)}$", "$\\substack{\\text{arg max }{j} \\\\ f(j) }$")` denotes the week in which this peak occurred. The parameters $\theta$, $\mu$, and $\nu$ follow from their prior description. Finally, we introduce noise for each simulated weekly hospitalization rate based on the selected estimate of $\tau^2$.

We impose a lower bound of 0 on the hospitalization rate via the following transformation of $\hat{y}^s_i$, denoted below as $\hat{z}^s_i$:

$$\hat{z}^s_i = 0.5 \bigg( | \hat{y}^s_i | + \hat{y}^s_i \bigg).$$

This transformation effectively preserves positive simulated hospitalization rates and sets negative hospitalization rates to 0. Negative hospitalization rates may be generated at the tails of a given season, when hospitalization rates are generally low, because the error is normally distributed in all weeks.

In all, we simulated 15,000 curves so as to generate an average of 1,000 simulated curves based on each empirical season's shape (S2 Fig). These simulated curves may be thought of as a plausible distribution of hypothetical flu seasons that could be observed in principle but perhaps have not yet been realized [@Brooks2015-fl].

```{r fig01-empsim-compare, echo = FALSE, out.width="70%", fig.height=8, fig.cap = "Empirical (top) and 15 randomly selected simulated (bottom) hospitalization curves. Empirical source: CDC, Emerging Infections Program (omitting 2009--2010 pandemic influenza season).", message = F}

putfigure("fig01")

```

```{r fig02-simcompare, out.width="100%", fig.height=3, fig.cap="Empirical (N = 15) vs. simulated (N = 15,000) target distributions."}

putfigure("fig02")

```

## Super learner

The super learner is a loss-based estimation algorithm [@Van_der_Laan2007-ml; @Polley2010-cb; @Polley2011-oz]. It takes as inputs training data, a library of component models (called _learners_), and a desired loss function specified to optimize model fits against a given prediction target. See Naimi & Balzer for an accessible introduction [@Naimi2018-fv].

For each prediction target (peak rate, peak week, and cumulative hospitalizations), we specified the same loss function and component model library and trained the super learner on the simulated distribution of hospitalization curves using the `sl3` package in R [@Coyle2020-ze].

We used the $L_1$ absolute error loss function to target the median of each prediction target distribution [@Polley2011-oz], meaning the ensemble learner would seek to minimize prediction risk by minimizing the absolute difference between a simulated instance of the prediction target and a given learner's prediction. We conducted the entire super learner procedure sequentially at each week *i* using the following covariates: hospitalization rate per 100,000 population in week *i*, cumulative hospitalizations per 100,000 population through week *i*, hospitalization rates from all prior weeks, cumulative hospitalizations from all prior weeks, the difference between the hospitalization rate in week *i* and the hospitalization rate in each prior week up to 5 weeks in the past, the difference between the cumulative hospitalization rate in week *i* and the cumulative hospitalization rate in each prior week up to 5 weeks in the past, interactions between the hospitalization rate in week *i* and each of the cumulative hospitalization differences, and interactions between the cumulative hospitalization rate in week *i* and each of the hospitalization rate differences.

After specifying the library of component models and the loss function, we repeated the following procedure for each prediction target:

1. Create 30 data sets, one for each week of the flu season, containing the outcomes and covariates as described previously.

3. Assign each observation to a fold for use in cross-validation. 
   a. We assigned all simulated curves based on a common observed season shape to the same fold, in order to account for the dependence between these simulated curves. We did not account for potential dependencies due to repeated sampling of the error term, as we believed the season shape was more consequential with respect to inducing dependence between simulated curves. 

4. Run the super learner algorithm:
   a. Train each learner on the data set using 15-fold cross-validation and calculate its cross-validated risk.
   b. Generate predictions with each component learner on the full data set.
   c. Fit a metalearner by regressing each learner's predictions on the prediction target to estimate the ensemble weights for each model. For this purpose, we used a non-negative least squares regression model, constraining it to produce a convex combination of coefficients between 0 and 1 (inclusive) [@Naimi2018-fv]. Doing so normalizes the model coefficients (i.e., the weights assigned to each learner's predictions in the ensemble super learner) so that they sum to 1.
   d. Calculate the cross-validated risk for the ensemble super learner.

In all, we trained [XXXXXX] component models in for each prediction target and week of the season, including variations on tuning and input parameters for the various learners (Table 2).


### Target 1: Peak rate

The general form of the models proposed to predict peak rate is as follows:

$$E[I(Y_i = 1)A_i | \bar{A}_{i=d}, \bar{X}_{i=d}) = \beta_0 + \vec{\beta}_a \bar{A}_{i=d} + \vec{\beta}_x \bar{X}_{i=d} + \vec{\beta}_t \bar{T}(i=d)$$

where $i$ indexes the week of the season, $c$ indicates the current week at which we are making a prediction of the target, $Y_i = 1$ identifies the week as the week in which the peak hospitalization rate occurred, $\bar{A}_{i=d}$ indicates the history of hospitalization rates through week $i=d$, $\bar{X}_{i=d}$ indicates the history of cumulative hospitalization rates through week $i=d$, and $\bar{T}(i)$ indicates derived variables to capture time trends and/or proposed interactions between variables. Each beta coefficient topped with an arrow and including a lowercase subscript indicates the vector of coefficients implied by the corresponding variable history. The notation above does not incorporate basis functions for splines, kernels, penalties, or other terms that varied across component learners.


### Target 2: Peak week

The general form of the models proposed to predict peak week is as follows:

$$E(I(Y_i = 1)i | \bar{A}_{i=d}, \bar{X}_{i=d}) = \beta_0 + \vec{\beta}_a \bar{A}_{i=d} + \vec{\beta}_x \bar{X}_{i=d} + \vec{\beta}_t  \bar{T}(i)$$

where $i$ indexes the week of the season, $I(Y_i = 1)$ is the indicator function that identifies a week as being the season's peak, $\bar{A}_{i=d}$ indicates the history of hospitalization rates through week $i=d$, $\bar{X}_{i=d}$ indicates the history of cumulative hospitalization rates through week $i=d$, and $\bar{T}(i)$ indicates derived variables to capture time trends and/or proposed interactions between variables. Each beta coefficient topped with an arrow and including a lowercase subscript indicates the vector of coefficients implied by the corresponding variable history. The notation above does not incorporate basis functions for splines, kernels, penalties, or other terms that varied across component learners.


### Target 3: Cumulative hospitalizations

The general form of the models proposed to predict cumulative hospitalizations is as follows:

$$E[X_{i=30} | \bar{A}_{i=d}, \bar{X}_{i=d}) = \beta_0 + \vec{\beta}_a \bar{A}_{i=d} + \vec{\beta}_x \bar{X}_{i=d} + \vec{\beta}_t \bar{T}(i=d)$$

where $i$ indexes a week of the season, $c$ indicates the current week at which we are making a prediction of the target, $\bar{A}_{i=d}$ indicates the history of hospitalization rates through week $d$, $\bar{X}_{i=d}$ indicates the history of cumulative hospitalization rates through week $d$, and $\bar{T}(i)$ indicates derived variables to capture time trends and/or proposed interactions between variables. Each beta coefficient topped with an arrow and including a lowercase subscript indicates the vector of coefficients implied by the corresponding variable history. The notation above does not incorporate basis functions for splines, kernels, penalties, or other terms that varied across component learners.

## Component models

We used the same library of component models to build the super learners for all three prediction targets (Table 3). The component models included a standard linear regression model, generalized additive models [@Wood2019-rc], loss-based regression models (e.g., lasso, ridge) [@Tibshirani1996-vt], random forests [@Breiman2001-vm], support vector regression [@Hastie2009-ft], neural networks [@Hastie2009-ft], loess [@Harrell2015-cd], and polynomial multivariate adaptive regression splines (polyMARS) [@Kooperberg2019-ma].

In all cases the learning algorithms incorporating these various component models were implemented using native functions provided in the `sl3` [@Coyle2020-ze] and `SuperLearner` [@Polley2019-sl] packages in R.


```{r candmodels-tuning-table}
tunetab <- tribble(
  ~ Model,
  ~ `Tuning parameters`,
  ~ `R package`,

  "Linear regression",
  "Variables screened for inclusion first using a cross-validated lasso procedure to omit variables with zero-valued coefficients.",
  "base",

  "Random forest (regression trees)",
  "number of trees = [50, 100, 200, 500], terminal node sizes = [3, 5, 10]",
  "randomForest",

  "Generalized additive model",
  "gamma penalty = [1, 2, 3, 4, 5]",
  "gam",

  "Support vector regression",
  "kernel = [radial, polynomial], degree = [1, 2, 3] applied only to polynomial",
  "svm",

  "Loss-based regression",
  "penalty = [lasso, ridge]",
  "glmnet",

  "Elastic net",
  "alpha penalty = [0.25, 0.5, 0.75]",
  "glmnet",

  "Neural network",
  "number of nodes in hidden layer = [5, 10, 25, 50, 75, 100], decay = [0, 0.005, 0.1, 0.2, 0.4]",
  "nnet",

  "Loess",
  "span = [0.25, 0.5, 0.75, 1]",
  "base",

  "Polynomial multivariate adaptive regression spline",
  "gcv penalty = [2, 4, 6, 8, 10]",
  "polspline",
  )

cap <- "Component models and tuning parameters."
fn <- "For the random forest and neural network learners, all combinations of the tuning parameters shown were proposed."

if (knitr::is_latex_output()) {
  tunetab %>%
    kable(
      booktabs = T,
      format = "latex",
      caption = cap
    ) %>%
    kable_styling(
      latex_options = c("striped", "scale_down"),
      ) %>%
    column_spec(1:2, width = "2.1in") %>%
    kableExtra::footnote(general_title = "", fn)
} else {
  tunetab %>%
    flextable(., cwidth = c(2.1, 3.2, 1.2)) %>%
    fit_to_width(., max_width = 8) %>% 
    bold(., part = "header") %>%
    add_header_lines(paste("Table 3:", cap)) %>%
    add_footer_lines(fn)
}
 
```

### Linear regression

For each week of the flu season, we specified a linear regression model that included covariates chosen by a lasso-based screening algorithm [@Friedman2019-uw; @Polley2019-sl] in which covariates with non-zero coefficients were identified using 10-fold cross validation and then entered into the linear regression. The linear model was specified using the `glm()` function in R with a Gaussian error distribution and identity link.

### Loss-based regression

We implemented several loss-based regression learners using the `glmnet` package in R [@Friedman2019-uw]. These learners included a learner with the standard lasso penalty [@Tibshirani1996-vt], a learner with the ridge penalty [@Tibshirani1996-vt], and three versions of the elastic net model [cite] using different $\alpha$ penalties (Table 3). The elastic net model combines lasso and ridge penalties within the same model [citation].

### Random forests

We implemented random forest alogirthms [@Breiman2001-vm] using the `randomForest` package [@Liaw2018-fe], tuning parameters governing the number of regression trees and minimum observations allowed in terminal nodes (Table 3).

### Support vector regression

We implemented several support vector regressions [@Hastie2009-ft] using the `e1071` package [@Meyer2019-gr]---one using a radial kernel and three others using a polynomial kernel of differing degrees (Table 3).

### Neural networks

We implemented several neural networks [@Hastie2009-ft] using the `nnet` package [@Ripley2020-ds]. All learners proposed contained a single hidden layer, altering tuning parameters governing the number of nodes in this hidden layer and the magnitude of decay applied to node weights (Table 3).

### Loess

We submitted several loess-based learners as implemented in base R, varying the smoothing kernel span (Table 3). 

### Polynomial multivariate adaptive regression splines

We submitted several polyMARS models using the `polspline` package [@Kooperberg2019-ma], varying the generalized cross-validation value, larger values of which produce smaller models [@Kooperberg2019-ma].

## Technical details

The super learner algorithm was implemented on a high-performance computing cluster housed at the Center for Computation and Visualization at Brown University. Each run was submitted to a large-memory node and parallelized across 32 central processing units based on the native functionality provided in `sl3` [@Coyle2020-ze] via the `delayed` [@Coyle2020-rq] and `future` [@Bengtsson2020-oz] packages in R.


# Results

## Peak rate

For the peak rate prediction target, the ensemble superlearner exhibited [XXXXXXXXX] (Table 4). Throughout the flu season, the discrete super learner (best-performing component model) consistently exhibited a lower average risk than a prediction based on the mean, indicating better predictive ability. Furthermore, the discrete super learner's cross-validated risk generally decreased as the season progressed. In each week a different discrete super learner was selected, with neural networks and penalized regressions being selected in the first several weeks, support vector regressions being selected toward the middle of the flu season, and polyMARS being selected more frequently toward the end of the season.

```{r risks-datadir}
results_output <- "C:/Users/jason/Documents/Github/FluHospPrediction/results/paper_output"

risktab_cnames <- c(
  Week = "Week",
  SuperLearner = "Super Learner",
  BestComponent = "Discrete SL Risk",
  Mean = "Mean",
  BestComponentModel = "Discrete SL"
)

risktab_cnames_word <- function(dat) {
  set_header_labels(
    dat,
    Week = "Week",
    SuperLearner = "Super Learner",
    BestComponent = "Discrete SL Risk",
    Mean = "Mean",
    BestComponentModel = "Discrete SL"
  )
}

```

```{r tab04-peakrate-risk-table}

pkrate_risks <- fread(file.path(results_output, "table-04_peakrate-risks.csv"))

cap <- "Cross-validated risks for the ensemble super learner, discrete super learner, and simple mean predictions of peak hospitalization rate, by week of influenza season. Estimates presented as mean risk (standard error)."

fn <- c("The ensemble super learner (SuperLearner) is the prediction generated as a weighted combination of component model predictions. The discrete super learner (Discrete SL) is the best-performing component model. The mean (Mean) is the average peak hospitalization rate per 100,000 population across the simulated hospitalization curves. The absolute error loss function used to optimized the super learner targets the median peak hospitalization rate.")

if (knitr::is_latex_output()) {
  pkrate_risks %>%
    kable(
      col.names = risktab_cnames,
      booktabs = TRUE,
      format = "latex",
      caption = cap
    ) %>%
    kableExtra::kable_styling(
      latex_options = "scale_down"
    ) %>%
    kableExtra::footnote(
      general_title = "",
      fn,
      threeparttable = TRUE
    )
} else {
  pkrate_risks %>%
    flextable(
      cwidth = c(0.6, rep(1.1, 3), 2.6)
    ) %>%
    risktab_cnames_word() %>%
    bold(., part = "header") %>%
    add_header_lines(paste("Table 4:", cap)) %>%
    add_footer_lines(fn)
}

```


## Peak week

For the peak week prediction target, the ensemble superlearner generally produced risk estimates close to those of the peak week predictions based on a simple mean (Table 5). The discrete super learner consistently exhibited lower mean risk than both the ensemble super learner and predictions based on the simple mean. As with peak rate, the discrete super learner tended to perform better as the flu season progressed. Over the first 6 weeks, some form of neural network was chosen as the discrete super learner in 5 of those weeks. Throughout the rest of the season, various versions of the random forest algorithms predominated the list of discrete super learners.


```{r tab05-peakweek-risk-table}

pkweek_risks <- fread(file.path(results_output, "table-05_peakweek_risks.csv"))

cap <- "Cross-validated risks for the ensemble super learner, discrete super learner, and mean predictions of the week in which the peak hospitalization rate occurs, by week of influenza season. Estimates presented as mean risk (standard error)."

fn <- c("The ensemble super learner (SuperLearner) is the prediction generated as a weighted combination of component model predictions. The discrete super learner (Discrete SL) is the best-performing component model. The mean (Mean) is the average week during which the peak hospitalization rate occurred across the simulated hospitalization curves. The absolute error loss function used to optimized the super learner targets the median peak hospitalization rate.")

if (knitr::is_latex_output()) {
  pkweek_risks %>%
    kable(
      col.names = risktab_cnames,
      booktabs = TRUE,
      format = "latex",
      caption = cap
    ) %>%
    kableExtra::kable_styling(
      latex_options = "scale_down"
    ) %>%
    kableExtra::footnote(
      general_title = "",
      fn,
      threeparttable = TRUE
    )
} else {
  pkweek_risks %>%
    flextable(
      cwidth = c(0.6, rep(1.1, 3), 2.6)
    ) %>%
    risktab_cnames_word() %>%
    bold(., part = "header") %>%
    add_header_lines(paste("Table 5:", cap)) %>%
    add_footer_lines(fn)
}

```


## Cumulative hospitalizations

For the cumulative hospitalizations prediction target, the ensemble super learner produced risk estimates generally near to those based on a prediction based on the simple mean (Table 6). Again, the discrete super learner consistently outperformed both the ensemble super learner and the simple mean prediction, improving substantially as the flu season progressed. Some form of support vector regression was chosen most frequently as the discrete super learner, with penalized regressions, polyMARS, and neural networks among the other models chosen in at least one of the weeks.

```{r tab06-cumhosp-risk-table}

cumhosp_risks <- fread(file.path(results_output, "table-06_cumhosp_risks.csv"))

cap <- "Cross-validated risks for the ensemble super learner, discrete super learner, and mean predictions of the cumulative hospitalization rate, by week of influenza season. Estimates presented as mean risk (standard error)."

fn <- c("The ensemble super learner (SuperLearner) is the prediction generated as a weighted combination of component model predictions. The discrete super learner (Discrete SL) is the best-performing component model. The mean (Mean) is the average cumulative hospitalization rate per 100,000 across the simulated hospitalization curves. The absolute error loss function used to optimized the super learner targets the median peak hospitalization rate.")


if (knitr::is_latex_output()) {
  cumhosp_risks %>%
    kable(
      col.names = risktab_cnames,
      booktabs = TRUE,
      format = "latex",
      caption = cap
    ) %>%
    kableExtra::kable_styling(
      latex_options = "scale_down"
    ) %>%
    kableExtra::footnote(
      general_title = "",
      fn,
      threeparttable = TRUE
    )
} else {
  cumhosp_risks %>%
    flextable(
      cwidth = c(0.6, rep(1.1, 3), 2.6)
    ) %>%
    risktab_cnames_word() %>%
    bold(., part = "header") %>%
    add_header_lines(paste("Table 6:", cap)) %>%
    add_footer_lines(fn)
}

```


# Discussion

In our study, we found that discrete super learners were consistently better at predicting median peak rate, peak week, and cumulative hospitalizations compared to a simple mean prediction when fit to a distribution of hypothetical influenza hospitalization curves. On the contrary, the ensemble super learner generally tended to perform comparably to a simple mean prediction and consistently worse than the discrete super learners with respect to cross-validated risk. In fact, the discrete super learner routinely provided the best predictions (lowest mean cross-validated risk) across all prediction targets. The discrete super learner also appeared to provide greater improvements over the simple mean prediction as a season wore on, a result that comports with similar findings in the domain of influenza-like illness [@Ray2018-ef]. 

The relative underperformance of the ensemble super learner, a feature observed across all predictions targets, raises several questions regarding the likely usefulness of the super learner algorithm in the context of predicting influenza hospitalizations. It may be that the ensemble super learner should not be relied upon if used to generate in-season forecasts given the small sample size of observed hospitalization curves [@Centers_for_Disease_Control_and_Prevention_undated-vt] and the fact that we did not observe improvements in prediction in a sample size of 15,000. As discussed in the introduction, the ensemble super learner has desirable asymptotic oracle properties, theoretically guaranteed to preform as well as the discrete super learner [@Van_der_Laan2007-ml]. Furthermore, the ensemble super learner is generally assumed not to be affected adversely by the inclusion of poor algorithms or overfitting, as the cross-validation procedure should result in a final weighted ensemble that assigns low or no weight to poor-performing predictions [@Polley2011-oz]. It is possible that adding additional algorithms such as gradient boosted models or exploring a larger range of tuning parameters for the models incorporated in our analysis would have improved the performance of the ensemble learner.

Our findings should be interpreted in light of several limitations. 

First, while our simulating 15,000 hypothetical hospitalization curves provided an adequate sample size with which to train the super learner---an analysis that would not have been possible using only the 15 observed flu seasons---the validity of our findings rely on the assumption that the distribution of simulated curves accurately characterizes the hypothetical distribution of plausible seasonal influenza hospitalization trajectories. We did not attempt to rigidly match the simulated distributions to their empirical counterparts so as not to restrict the possibility of atypical influenza seasons that could be, but perhaps have not yet been, observed. In other words, we did not assume the 15 observed seasons captured the full range of possible values for the selected prediction targets. A related issue concerns the selection of the $\lambda$ value for each trend filter fit to an empirical season, as these fits were used as templates for the simulated hospitalization curves. This choice of penalty was somewhat arbitrary. Selecting the $\lambda$ penalty that minimized the root mean squared error of the trend filter predictions with respect to the empirical data may have resulted in a distribution of simulated curves with different properties. However, while methods for selecting the $\lambda$ penalty for the $\ell_1$ trend filter have been proposed [@Yamada2016-fq], to our knowledge, no consensus currently exists regarding the best procedure to do so.

Second, we did not include any mechanistic models in the component learner library, such as compartmental or agent-based epidemic models. Future work could incorporate such models or, alternatively, the predictions generated by an ensemble super learner could be incorporated into a wider ensemble model that incorporates predictions from such models.

Third, our approach to simulating hypothetical influenza hospitalization curves incorporated only information on hospitalization rates, without relying on other mechanistic factors to generate these curves. Therefore, we were unable to examine whether incorporating other influenza-related surveillance data (e.g., ILI, viral activity [@Centers_for_Disease_Control_and_Prevention_undated-vt]) or climate data  may have improved the super learner's predictions.

Fourth, we did not model hospitalization rate time series explicitly, in order to increase the number of component models available for inclusion [@Coyle2020-ze]. Nonetheless, we did attempt to incorporate information regarding the trajectory of influenza hospitalization rates up to a given week by incorporating lagged hospitalization rates, differences between the hospitalization rates in the current week and prior weeks, and selected interactions. Future work should explore the potential for explicit modeling of time series, perhaps using an "online" ensemble super learner, which continuously updates the super learner's predictions based on its predictions in prior weeks [@Van_der_Laan2018-xq].

Finally, because we focused on predicting hospitalizations due to seasonal influenza, these findings do not have implications for predicting hospitalizations during years in which a pandemic influenza strain is circulating.

The super learner remains a promising algorithm for predicting influenza-related hospitalizations, but future work should determine the proper breadth of component learners such that the ensemble learner can be relied upon to perform as well as the discrete super learner. Adapting this approach to incorporate empirical surveillance data as well as other data relevant to influenza dynamics (e.g., viral activity, weather patterns) may also be fruitful avenues for future research. 

# Software and code

The code necessary to reproduce these results and the output from the super learner procedures are provided in a repository at Open Science Framework (doi: XXXXXXXXX). These materials are also housed permanently at the Brown Digital Repository (doi/url: XXXXXXXXXXX), and the package can be installed directly via the Github repository ([https://www.github.com/jrgant/FluHospPrediction](https://www.github.com/jrgant/FluHospPrediction)).

# Declarations

## Acknowledgment

We thank Ashley Naimi, Laura Balzer, and Nicholas Reich for their comments on the study aims and the super learner approach. The super learner algorithm was fit using computational resources and services at the Center for Computation and Visualization, Brown University.

## Funding statement

This work was funded by an unrestricted grant from Sanofi (PI: Andrew Zullo). The funders did not assist in the statistical analysis nor did they have a say in the final decision to submit the manuscript for publication.

## Competing interests

[solicit competing interests from co-authors]


\newpage

# Supporting information

**S1 Fig. Linear trend filter fits to observed influenza hospitalization curves.** \bigskip

```{r s1fig, out.width = "90%", fig.align="center"}

results_folder <- "C:/Users/jason/Documents/Github/FluHospPrediction/results"


if (knitr::is_latex_output()) {
  include_graphics(file.path(results_folder, "trendfilter-fit-facet.pdf"))
} else {
  include_graphics(file.path(results_folder, "trendfilter-fit-facet.png"))
}

```


\newpage
\noindent
**S2 Table. Number of simulated curves based on each observed flu season (Emerging Infections Program).**

```{r s2table}
tmpcts <- fread(file.path(results_output, "table-s02_template-counts.csv"))

if (knitr::is_latex_output()) {
  tmpcts %>%
    knitr::kable(
      format = "latex",
      booktabs = TRUE
    )
} else {
  tmpcts %>%
    flextable %>%
    bold(part = "header") %>%
    autofit
}

```

\newpage
\noindent
**S3 Table. Weekly cross-validated risks across the component learners used to predict peak hospitalization rate per 100,000 population.**

```{r s3table-peakrate-meanrisk}

rpsum <- fread(file.path(results_output, "table-s03_risk-distbyweek-pkrate.csv"))

if (knitr::is_latex_output()) {
  rpsum %>%
    kable(
      format = "latex",
      booktabs = TRUE,
      digits = 2,
    )
} else {
  rpsum %>%
    flextable(cwidth = c(0.6, rep(1.15, 5))) %>%
    colformat_num(digits = 2) %>%
    bold(., part = "header")
}

```

\newpage
\noindent
**S4 Table. Weekly cross-validated risks across the component learners used to predict peak week.**

```{r s4table-peakweek-meanrisk}

rwsum <- fread(file.path(results_output, "table-s04_risk-distbyweek-pkweek.csv"))

if (knitr::is_latex_output()) {
  rwsum %>%
    kable(
      format = "latex",
      booktabs = TRUE,
      digits = 2,
    )
} else {
  rwsum %>%
    flextable(cwidth = c(0.6, rep(1.15, 5))) %>%
    colformat_num(digits = 2) %>%
    bold(., part = "header")
}

```

\newpage
\noindent
**S5 Table. Weekly cross-validated risks across the component learners used to predict cumulative hospitalization rate per 100,000 population.**

```{r s5table-cumhosp-meanrisk}

rwsum <- fread(file.path(results_output, "table-s05_risk-distbyweek-cumhosp.csv"))

if (knitr::is_latex_output()) {
  rwsum %>%
    kable(
      format = "latex",
      booktabs = TRUE,
      digits = 2,
    )
} else {
  rwsum %>%
    flextable(cwidth = c(0.6, rep(1.15, 5))) %>%
    colformat_num(digits = 2) %>%
    bold(., part = "header")
}

```

\newpage

# References

\bibliography{references}

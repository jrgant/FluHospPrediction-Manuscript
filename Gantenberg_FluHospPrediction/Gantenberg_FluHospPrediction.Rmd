---
title: "Predicting seasonal influenza hospitalization using an ensemble super learner: a simulation study"

bibliography: references.bib
csl: plos.csl

always_allow_html: true
---

```{r, include = FALSE}
library(officer)
sfp <- fp_text(font.family = "NimbusRomNo9L-Reg", vertical.align = "superscript")
```

Jason R. Gantenberg`r ftext("1,2", sfp)`\*, Kevin W. McConeghy`r ftext("2,3", sfp)`, Chanelle J. Howe`r ftext("4", sfp)`, Jon Steingrimsson`r ftext("5", sfp)`, Robertus van Aalst`r ftext("6,7", sfp)`, Ayman Chit`r ftext("6,8", sfp)`, Andrew R. Zullo`r ftext("1,2,3", sfp)`


`r ftext("1", sfp)` Department of Epidemiology, Brown University School of Public Health, Providence, RI

`r ftext("2", sfp)` Department of Health Services, Policy and Practice, Providence, RI

`r ftext("3", sfp)` Providence VA Medical Center, Providence, RI

`r ftext("4", sfp)` Center for Epidemiology and Environmental Health, Brown University School of Public Health, Providence, RI

`r ftext("5", sfp)` Department of Biostatistics, Brown University School of Public Health, Providence, RI

`r ftext("6", sfp)` Sanofi Pasteur, Swiftwater, PA

`r ftext("7", sfp)` Department of Health Sciences, University Medical Center Groningen, University of Groningen, Groningen, the Netherlands

`r ftext("8", sfp)` Leslie Dan Faculty of Pharmacy, University of Toronto, Ontario, Canada

\* Corresponding author

E-mail: jrgant@brown.edu (JRG)

\newpage


**Target Journal:** PLoS Computational Biology

**Short Title:** Predicting influenza hospitalizations using a super learner

**Section:** Epidemiology and Clinical/Translational Studies

\newpage
    
# Abstract

Accurate forecasts of influenza epidemics can inform public health response to outbreaks. To date, most efforts in influenza forecasting have focused on predicting influenza-like activity. Fewer have sought to predict other features of seasonal influenza epidemics, such as influenza-related hospitalizations. We conducted a simulation study to explore the potential for a machine-learning algorithm (super learner) to predict three seasonal measures of influenza hospitalizations at the national level in the United States: the peak hospitalization rate, the week this peak hospitalization rate occurs, and the cumulative hospitalization rate. We trained a super learner on 15,000 simulated influenza hospitalization curves to generate predictions at each week of the season for these seasonal prediction targets. For each week, we compared the performance of the ensemble super learner (a weighted combination of predictions from a set of individual prediction algorithms), the discrete super learner (the best-performing individual prediction algorithm), and a naive prediction based on the median of a given outcome (e.g., median peak hospitalization rate). For the peak hospitalization rate prediction target, we found that the super learner algorithm consistently identified individual prediction algorithms that improved upon a prediction using the median of the outcome distribution. We found that the ensemble predictions in general performed slightly better than the median prediction late in the flu season for the peak hospitalization rate outcome and comparably earlier in the season. While the discrete super learner during this period of the season typically exhibited substantially lower prediction error than the ensemble super learner, the component model selected as the discrete super learner varied by week. In contrast, the ensemble performed stably throughout the flu season. For the peak week outcome, the ensemble super learner's predictions exhibited slightly lower prediction error than the naive median prediction. For the cumulative hospitalization rate outcome, the ensemble super learner performed comparably to the median prediction across all weeks, while individual discrete super learners exhibited lower prediction error than the naive prediction, particularly late in the flu season. This study suggests that the super learner may be a useful tool for predicting influenza hospitalizations. Future work should examine the super learner's performance using empirical data and additional influenza-related indicators such as influenza-like-illness and viral activity. Whether the super learner is most useful for selecting an individual prediction algorithm versus producing ensemble predictions may vary by context: determining which uses of the super learner, if any, might improve in-season forecasting of influenza hospitalizations should be the focus of future applied investigations.

# Author Summary

While influenza prediction is a still-maturing science, the past decade has seen notable improvements in predicting influenza-like activity. Machine-learning approaches may further improve the predictive accuracy of influenza forecasts. However, predicting influenza-related hospitalizations has received somewhat less attention. Using a machine-learning approach called the _super learner_, we examined the potential ability of this machine-learning approach to predict several features of influenza hospitalization dynamics that could help public health authorities and institutions allocate their efforts and resources more effectively during a flu season. The super learner is capable of taking a large number of prediction algorithms (e.g., statistical models) and either combining them into a single prediction (the ensemble prediction) or choosing the best-performing algorithm. Our results suggest that ensemble super learner predictions may not perform as well as the best-performing prediction algorithm in any given season but do provide predictions with generally stable performance across a set of simulated flu seasons. Future work should explore approaches, such as incorporating empirical data on indicators of influenza transmission or modifying the prediction algorithms presented to the super learner, that might either improve the performance of super learner ensembles or guide appropriate application(s) of the super learner for predicting influenza hospitalizations.

\newpage

# Introduction

```{r include=FALSE}
pacman::p_load(
  FluHospPrediction,
  ggplot2,
  ggthemes,
  gridExtra,
  data.table,
  tidyverse,
  kableExtra,
  knitr,
  flextable,
  officedown
)

opts_chunk$set(
  echo = FALSE,
  cache = FALSE,
  tab.cap.style = "Table Caption",
  tab.cap.pre = "Table ",
  tab.cap.sep = ": ",
  fig.cap.style = "Image Caption",
  fig.cap.pre = "Fig ",
  fig.cap.sep = ": ",
  fig.width = 6.5,
  reference_num = FALSE
)

# paper output directory
assetdir <-
  "C:/Users/jason/Documents/Github/FluHospPrediction/results/00_paper_output/"

# table font
global_table_font <- "NimbusRomNo9L-Reg"

# function to get a set of files based on their creation date
global_date <- "2020-08-20"     # set accordingly

get_asset <- function(type, descr, date = global_date) {

  if (!type %in% c("FIG", "TAB", "VAL")) stop("Invalid asset type.")

  if (type == "FIG") {
    ext <- "png"
  } else if (type == "TAB") {
    ext <- "csv"
  } else {
    ext <- "Rds"
  }

  file.path(assetdir, paste0(type, "_", descr, "_", date, ".", ext))
}

# results objects
prop_weeks_transformed <- readRDS(
  get_asset("VAL", "Proportion-Weeks-Transformed")
)

# Set up supplemental material numbering/labeling

## This function makes a caption for the prediction risk tables in the supplement.
rttab_cap <- function(target, analysis) {
  rateslug <- "hospitalization rate per 100,000 population"
  texts <- c(
    "Peak rate" = paste("peak", rateslug),
    "Peak week" = "peak week",
    "Cumulative hospitalizations" = paste("cumulative", rateslug))
  t <- match(target, names(texts))
  
  paste0(
    "Weekly cross-validated risks across the component learners used to predict ", texts[t], " (", analysis, " analysis). Estimates presented as mean (standard error)."
  )
}

# This function makes the footnote for a prediction risk table.
risktabfn <- function(target, sqe = FALSE) {
  texts <- c(
    "Peak rate" = "peak hospitalization rate.",
    "Peak week" = "week in which the peak hospitalization rate occurred.",
    "Cumulative hospitalizations" = "cumulative hospitalization rate"
  )
  
  paste(
    "The ensemble super learner (EnsembleSL) is the prediction generated as a weighted combination of component model predictions. The discrete super learner (DiscreteSL) is the best-performing component model. The super learner was optimized to minimize the",
    ifelse(sqe, "squared error loss", "absolute error loss"),
    "and, hence, the prediction error for the",
    ifelse(sqe, "mean", "median"),
    texts[names(texts) == target]
  )
}


## This function makes a caption for the risktile figures in the supplement.
rtfig_cap <- function(target, analysis) {
  paste0(
    target, ", ",
    "ensemble learner weights as a function of log mean cross-validated risk ",
    "(", analysis, " analysis). Ensemble learner weights are assigned by regressing the prediction target on corresponding predictions made by each component learner. The coefficients estimated for each independent predictor (i.e., component learner) represent the weight given to the learner's predictions in the final ensemble super learner prediction, as assigned by the metalearner. In addition, LOESS learners frequently returned warnings due possibly to sparsity in the data but still returned predictions to which the metalearner sometimes assigned non-zero weights. As such, we elected not to drop these models from the analysis."
  )
}

## slugs for various analyses
mainslug <- "main"
lseslug <- "alternate trend filter penalty"
erfslug <- "component learner subset"
sqeslug <- "squared error loss"

## Function to make labels for the two trend filter fit plots.
tf_cap <- function(analysis) {
  paste0(
    "Linear trend filter fits to observed influenza hospitalization curves using the ",
    ifelse(analysis == "main", "$\\lambda_{min}$", "$\\lambda_{SE}$"),
    " penalty (", ifelse(analysis == "main", mainslug, lseslug), " analysis)."
    )
}


supp <- data.table(
  type = c(
    ## Main analysis
    "Fig", "Table", "Appendix", "Table", "Table", "Table",
    "Fig", "Fig", "Fig",
    ## Alternate trend filter penalty
    "Fig", "Table", "Table", "Table", "Fig", "Fig", "Fig",
    ## Component learner subset
    "Table", "Table", "Table", "Fig", "Fig", "Fig",
    ## Squared error loss
    "Table", "Table", "Table", "Fig", "Fig", "Fig"
  ),
  file = c(
    ## Main analysis
    "TF-Predictions_lambda.min",
    "Simulation-Template-Counts",
    "",
    "Risk-Week_Peak-Rate",
    "Risk-Week_Peak-Week",
    "Risk-Week_Cum-Hosp",
    "Risktiles_Peak-Rate",
    "Risktiles_Peak-Week",
    "Risktiles_Cum-Hosp",
    ## Alternate trend filter penalty
    "TF-Predictions_lambda.1se",
    "Risks-EDSL-Mean_PeakRate-L1SE",
    "Risks-EDSL-Mean_PeakWeek-L1SE",
    "Risks-EDSL-Mean_CumHosp-L1SE",
    "Risktiles_Peak-Rate-L1SE",
    "Risktiles_Peak-Week-L1SE",
    "Risktiles_Cum-Hosp-L1SE",
    ## Component learner subset
    "Risks-EDSL-Mean_PeakRate-ERF",
    "Risks-EDSL-Mean_PeakWeek-ERF",
    "Risks-EDSL-Mean_CumHosp-ERF",
    "Risktiles_Peak-Rate-ERF",
    "Risktiles_Peak-Week-ERF",
    "Risktiles_Cum-Hosp-ERF",
    ## Squared error loss
    "Risks-EDSL-Mean_PeakRate-SQE",
    "Risks-EDSL-Mean_PeakWeek-SQE",
    "Risks-EDSL-Mean_CumHosp-SQE",
    "Risktiles_Peak-Rate-SQE",
    "Risktiles_Peak-Week-SQE",
    "Risktiles_Cum-Hosp-SQE"
  ),
  cap = c(
    ## Main analysis
    tf_cap("main"),
    "Number of simulated curves based on each observed flu season (Emerging Infections Program).",
    "Additional methods.",
    rttab_cap("Peak rate", mainslug),
    rttab_cap("Peak week", mainslug),
    rttab_cap("Cumulative hospitalizations", mainslug),
    rtfig_cap("Peak rate", mainslug),
    rtfig_cap("Peak week", mainslug),
    rtfig_cap("Cumulative hospitalizations", mainslug),
    ## Alternate trend filter sensitivity
    tf_cap("lse"),
    rttab_cap("Peak rate", lseslug),
    rttab_cap("Peak week", lseslug),
    rttab_cap("Cumulative hospitalizations", lseslug),
    rtfig_cap("Peak rate", lseslug),
    rtfig_cap("Peak week", lseslug),
    rtfig_cap("Cumulative hospitalizations", lseslug),
    ## Component learner subset sensitivity
    rttab_cap("Peak rate", erfslug),
    rttab_cap("Peak week", erfslug),
    rttab_cap("Cumulative hospitalizations", erfslug),
    rtfig_cap("Peak rate", erfslug),
    rtfig_cap("Peak week", erfslug),
    rtfig_cap("Cumulative hospitalizations", erfslug),
    ## Squared error loss sensitivity
    rttab_cap("Peak rate", sqeslug),
    rttab_cap("Peak week", sqeslug),
    rttab_cap("Cumulative hospitalizations", sqeslug),
    rtfig_cap("Peak rate", sqeslug),
    rtfig_cap("Peak week", sqeslug),
    rtfig_cap("Cumulative hospitalizations", sqeslug)
  )
)

supp[, number := .I]
supp

## A function to output a supplemental material caption
## as Markdown markup.
supp_cap <- function(fileslug) {
  info <- supp[file == fileslug]
  cat(info[, paste0("S", number, " ", type, ". ", cap)])
}
```

Between 2010 and 2017, approximately 140,000--570,000 individuals were hospitalized and 12,000--51,000 died annually due to seasonal influenza in the United States [@Centers_for_Disease_Control_and_Prevention2020-uc]. The ability to predict the future burden of influenza-related hospitalizations during a given influenza season can help policymakers, public health officials, physicians, and other stakeholders to allocate resources appropriately and prepare for expected changes in hospitalization rates [@Lutz2019-co]. For example, forecasts could provide hospitals with enough time to make inpatient hospital beds available for patients admitted with influenza-related illnesses [@Nap2008-pandem; @Nap2007-pandem].

While influenza forecasting is a still-maturing science [@Reich2019-uk; @Chretien2014-dy], researchers have made considerable progress over the past decade in improving the quality of and capacity for forecasting influenza-like illness (ILI) [@Reich2019-uk]. The FluSight forecasting competitions sponsored by the Centers for Disease Control and Prevention (CDC) since the 2013--14 influenza season have contributed to the improvement and standardization of influenza forecasts in general [@Reich2019-uk]. A diverse set of models have been used to generate forecasts, including statistical time series models [@Reich2019-uk; @Biggerstaff2018-ns], Bayesian methods [@Chretien2014-dy; @Brooks2015-fl], and mechanistic compartmental or agent-based models [@Chretien2014-dy; @Reich2019-uk]. Ensemble methods have emerged as a particularly promising approach, further improving the accuracy and stability of epidemic predictions by combining predictions from multiple estimators in a principled manner [@Reich2019-ca; @Ray2018-ef; @Lutz2019-co].

Ensemble methods combine predictions generated by a set of component models [@Reich2019-ca; @Hastie2009-ft; @Wolpert1992-pw; @Breiman1996-ez]. In some cases, ensembles aggregate component model predictions by weighting better predictions more highly in the final ensemble prediction [@Ray2018-ef; @Reich2019-ca], though other weighting criteria can be applied [@Ray2018-ef]. One compelling rationale for using ensemble predictions rests in their ability to borrow the strengths and avoid the weaknesses of any single class of models used as inputs into the ensemble [@Van_der_Laan2007-ml]. This feature tends to lead not only to more accurate predictions but also to predictions that might produce more stable performance across a range of scenarios [@Ray2018-ef]. The CDC's primary in-season outpatient ILI forecasts are now based on an ensemble forecast generated by aggregating predictions from a growing library of individual forecasts submitted by research teams around the United States [@Reich2019-uk]. Briefly, the ensembles used in the CDC FluSight competitions combine in-season forecasts based on a standardized protocol for submitting and scoring forecasts of several prespecified weekly and season-level prediction targets [@Reich2019-uk]. The forecasts submitted by various teams are generated using a wide variety of prediction algorithms, some of which are ensemble methods themselves [@Reich2019-uk].

One ensemble machine learning method, dubbed "super learner" [@Van_der_Laan2007-ml; @Polley2010-cb; @Polley2011-oz], exhibits desirable properties that suggest it may be a powerful tool for predicting flu hospitalizations. Its developers have demonstrated that, asymptotically, the ensemble super learner is guaranteed to perform as well as the best-fitting component model [@Polley2010-cb; @Polley2011-oz]. Several software packages have been developed to implement the super learner algorithm [@Polley2019-sl; @Coyle2020-ze], providing researchers and others easy access to a large library of component models and a means to efficiently evaluate individual models' predictive performance [@Coyle2020-ze].

To date, most work in influenza forecasting has focused on ILI [@Reich2019-uk; @Biggerstaff2018-ns; @McGowan2019-ph; @Kandula2018-sq; @Brooks2015-fl], with some [@Kandula2019-tg] but considerably less work on predicting influenza-related hospitalization rates. Because influenza-related hospitalization dynamics may evolve differently than those of influenza transmission itself during a given flu season---for instance, consequent hospitalizations would be expected to lag influenza incidence and to depend on hospital access---and because hospitalization rates are at least one signal used to determine the severity of circulating flu strains [@Biggerstaff2018-ns], using ensemble methods to predict hospitalization rates could be a value complement to existing efforts in ILI forecasting.

While the super learner does not necessarily require a large sample size in order to implement embedded procedures (e.g., cross-validation) [@Van_der_Laan2007-ml], the 15 empirical flu seasons for which  hospitalization surveillance data are available from the CDC are too few in number to accommodate the algorithm. We were motivated, therefore, to conduct a simulation study to examine the super learner's potential as a tool for predicting national-level seasonal influenza hospitalizations.

In this study, we trained a super learner on 15,000 influenza hospitalization curves, simulated using data from 15 empirical flu seasons as templates, to generate predictions for three national-level seasonal target parameters chosen based on the targets specified in CDC forecasting competitions [@Centers_for_Disease_Control_and_Prevention_undated-tx]. We ran the super learner algorithm  procedure independently at each of the 30 weeks in a typical influenza season and compared the predictive performance of the ensemble super learner against that of a discrete super learner and a prediction based on the mean of a given outcome across the simulated hospitalization curves.
    
# Methods
    
## Empirical data

We downloaded publicly available surveillance data on seasonal influenza-related hospitalizations from the CDC's FluView Interactive dashboard [@Centers_for_Disease_Control_and_Prevention_undated-vt]. Specifically, we included data from the Emerging Infections Program (EIP) beginning with the 2003--2004 season and ending with the 2018--2019 season, omitting the 2009-2010 pandemic influenza year. The EIP contains data on influenza-related hospitalizations in California, Colorado, Connecticut, Georgia, Maryland, Minnesota, New Mexico, New York, Oregon, and Tennessee [@Centers_for_Disease_Control_and_Prevention_undated-vt]. Since the 2009--2010 season the FluSurv Network has included between 3 and 6 states in addition to those represented in the EIP data, depending on the year [@Centers_for_Disease_Control_and_Prevention_undated-vt]. To maintain consistency within the empirical data, and to maximize the number of flu seasons available to inform curve simulation, we excluded data from states outside the EIP.

Influenza hospitalization cases reflect individuals hospitalized during the flu season and for whom infection control logs and/or hospital laboratory or admissions data indicated a positive test for influenza [@Centers_for_Disease_Control_and_Prevention_undated-vt]. According to the CDC, a hospitalization's being identified as related to influenza depends upon laboratory tests ordered at the discretion of a physician, suggesting that surveillance statistics probably underestimate the true burden of influenza hospitalization [@Centers_for_Disease_Control_and_Prevention_undated-vt]. Typically, the CDC releases influenza hospitalization data for epiweeks 40--53 and 1--17, corresponding approximately to October through April of the next year [@Centers_for_Disease_Control_and_Prevention_undated-vt; @Centers_for_Disease_Control_and_Prevention_undated-pu]. Epiweeks are integers assigned to each week (beginning on Sunday) of the calendar year, where epiweek 1 is the first week of the calendar year and epiweeks 52/53 the last, depending on leap years [@Centers_for_Disease_Control_and_Prevention_undated-pu]. We renumbered epiweeks falling within the range of influenza hospitalization data as 1--30, omitting epiweek 53, which was present in only 3 of the 15 available empirical seasons. Hereafter we refer to this 30-week period generically as a _flu season_.
    
## Prediction targets

Following from the CDC's Flu Sight challenge [@Centers_for_Disease_Control_and_Prevention_undated-tx], we specified 3 season-level prediction targets:

1. _Peak rate_, defined as the highest weekly rate of influenza-related hospitalizations throughout the course of a flu season (per 100,000 population),
2. _Peak week_, defined as the week during which this peak rate occurred, and
3. _Cumulative hospitalizations_, defined as the cumulative influenza-related hospitalization rate over the 30 weeks of the flu season (per 100,000 population). 

Fifteen empirical observations were available for each prediction target, corresponding to the flu seasons recorded in the CDC's surveillance data (**Table \@ref(tab:targtable)**).

```{r tab.id="targtable", tab.cap="**Empirical distributions of peak hospitalization rate, peak week, and cumulative hospitalization rate in the United States, 2003--2019.**"}

targets <- fread(get_asset("TAB", "Prediction-Targets", global_date))
targets <- targets[Season != "2009-10"]

fn <- "Source: CDC FluView (Emerging Infections Program). Peak rate and cumulative rate expressed per 100,000 population. Pandemic influenza season 2009-2010 omitted."

targets %>%
  flextable(., cwidth = c(0.7, rep(1.2, 3))) %>%
  bold(., part = "header") %>%
  add_footer_lines(fn) %>%
  font(., fontname = global_table_font, part = "all")

```

## Simulation of hospitalization curves

To simulate a distribution of seasonal influenza hospitalization curves, we adapted an approach by Brooks et al. originally used to predict influenza-like illness [@Brooks2015-fl]. First, we fitted a linear trend filter [@Kim2009-bz; @Tibshirani2014-tr] to each of the 15 observed influenza hospitalization curves from the EIP using the `genlasso` package in R [@Arnold-2019-gl, @RCore2020-ct]. The linear trend filter is a penalized method that fits a piecewise linear function to a time series, testing a number of values of the penalty ($\lambda$) by default [@Arnold-2019-gl]. We used these fits as templates for the simulated influenza hospitalization curves. The $\lambda$ penalty was chosen based on 5-fold cross-validation [@Brooks2015-fl]. Specifically, in the main analysis we used the $\lambda$ value that minimized the trend filter's prediction error across the folds [@Brooks2015-fl], hereafter referred to as $\lambda_{min}$.

Next, we used the resulting 15 trend filter fits in a modified version of the curve generation scheme described in Brooks et al. [@Brooks2015-fl]. Specifically, the original scheme was developed to simulate potential ILI curves, and therefore, the curve-generating function included a term for season-onset, representing the threshold ILI at which the flu season would be considered to have started [@Brooks2015-fl]. This onset is an additional prediction target in FluSight ILI forecasting because ILI surveillance is conducted year-round. On the contrary, such a threshold does not exist for influenza-related hospitalizations. Rather, hospitalization rates are recorded starting in epiweek 40 through the end of the flu season. Save for one season, hospitalization rates start at a rate of 0 per 100,000 [@Centers_for_Disease_Control_and_Prevention_undated-vt]. We modified the curve-generating function to omit the seasonal onset term and added a transformation that constrains simulated hospitalization rates to be greater than or equal to 0. Otherwise, the implementation closely followed the Brooks et al. approach.

Briefly, Brooks et al. conceptualize a seasonal influenza curve generically as a function of the week of the season, plus a noise term with mean 0 and variance $\tau^2$ [@Brooks2015-fl], which we adapted to the hospitalization case. 

For each empirical season, we use its linear trend filter fit and average the squared residuals over all weeks to estimate $(\tau^s)^2$.

For each simulated curve, each of the following parameters is sampled randomly and independently of one another:

  * a vector of estimated hospitalization rates based on a linear trend filter fit to empirical season
  * the root mean squared error of one of a linear trend filter fit, averaged across all weeks 
  * a draw from a continuous uniform distribution with range [0.75, 1.25]
  * a draw from a continuous uniform distribution bounded by the minimum and maximum peak hospitalization rates from the 15 trend filter fits
  * a draw from a continuous uniform distribution bounded by the minimum and maximum peak weeks based on the 15 trend filter fits

We then feed these parameters into the curve-generating function to produce a hypothetical influenza hospitalization curve, imposing a lower bound of 0 on the hospitalization rate. This constraint preserves positive simulated hospitalization rates while setting negative hospitalization rates to 0. Negative hospitalization rates may be generated at the tails of a given season, when hospitalization rates are generally low, because the noise is normally distributed in all weeks. This transformation was imposed on `r round(prop_weeks_transformed * 100, 2)`% of the simulated weekly hospitalization rates generated for the main analysis.

In all, we simulated 15,000 curves. Collectively, these simulated curves may be thought of as a plausible distribution of hypothetical flu seasons that could be observed in principle but perhaps have not yet been realized [@Brooks2015-fl] (**Fig \@ref(fig:empsim-compare)**). While the distribution of prediction targets across the simulated seasons differed from the empirical distribution represented in the FluView data (**Fig \@ref(fig:simcompare)**), we note that the empirical distributions capture only 15 flu seasons, and it is unlikely that so few seasons' worth of data capture the full range of potential influenza-related hospitalization dynamics [@Brooks2015-fl].

```{r empsim-compare, fig.asp = 2100/2400, echo = FALSE, fig.cap = "**Simulated distributions of hospitalization rates at each week (boxplots), by empirical season template.** All simulated curves were based on linear trend filter fits using the $\\lambda_{min}$ trend filter penalty. Note that because each parameter used in the curve-generating function was drawn independently, simulated hospitalization curves based on an empirical shape template should have a similar shape (i.e., unimodal, bimodal) but may have different peak and/or cumulative hospitalization rates compared to the empirical template. Empirical data source: CDC, Emerging Infections Program (omitting 2009--2010 pandemic influenza season).", message = F}

knitr::include_graphics(get_asset("FIG", "Simulation-Curves-by-Template-Boxplot"))
```

```{r simcompare, fig.width = 6.5, fig.asp = 1440/3000, fig.cap="**Empirical (*N* = 15) vs. simulated (*N* = 15,000) target distributions. $\\lambda_{min}$, trend filter penalty used in the main analysis; $\\lambda_{SE}$, trend filter penalty used in the alternate trend filter penalty sensitivity analysis.**"}

knitr::include_graphics(get_asset("FIG", "TargetDists-Emp-vs-Sim"))
```

## Super learner

The super learner is an ensemble prediction model that is a linear combination of individual prediction models, where weights in the linear combination are selected by minimizing a user-specified loss function [@Van_der_Laan2007-ml; @Polley2010-cb; @Polley2011-oz]. To provide some intuition, one familiar example of a loss function is the mean squared error. Optimizing the super learner algorithm to minimize the mean squared error amounts to improving predictions of the conditional mean of the outcome of interest [@Polley2011-oz], and the prediction error calculated based on the mean squared error is referred to as the squared error loss. The super learner takes as inputs a data set, a set of prediction algorithms (hereafter referred to as individual component learners), and a desired loss function. Component learners might include parametric models familiar to researchers in the health sciences (e.g., generalized linear models), semi-parametric models, and machine-learning algorithms (e.g., neural networks, random forests), among other possibilities. See Naimi & Balzer for an accessible introduction to the super learner [@Naimi2018-fv].

For each prediction target selected in the current study--peak rate, peak week, and cumulative hospitalizations---we specified the same library of learners and measured the performance of both the component learners and the ensemble super learner using the  absolute error loss function, which denotes the absolute difference between the prediction and the true value of a given observation in the simulated data set [@Polley2011-oz]. This loss function targets the median of the outcome distribution [@Polley2011-oz]---for example, the median peak hospitalization rate---which we selected due to skewness present in the simulated peak week and cumulative hospitalization distributions (**Fig \@ref(fig:simcompare)**). 

We trained the super learner on the simulated hospitalization curves using the `sl3` package in R [@Coyle2020-ze]. Progressing sequentially through the entire flu season, we ran the super learner algorithm at each week *i* to predict the seasonal-level target of interest as a function of the hospitalization rates and cumulative hospitalization rates up to that point in the season. To capture these dynamics, we included the following predictor variables in each component learner (with one exception, discussed later):

  * hospitalization rate per 100,000 population in week *i*, 
  * cumulative hospitalizations per 100,000 population through week *i*,
  * lagged hospitalization rates from all weeks prior to week *i*, 
  * lagged cumulative hospitalizations from all weeks prior to week *i*,
  * the difference between the hospitalization rate in week *i* and the hospitalization rate in each prior week up to 5 weeks in the past, 
  * the difference between the cumulative hospitalization rate in week *i* and the cumulative hospitalization rate in each prior week up to 5 weeks in the past
  * product terms between the hospitalization rate in week *i* and each of the cumulative hospitalization differences, and 
  * product terms between the cumulative hospitalization rate in week *i* and each of the hospitalization rate differences.

Thus, we attempted to mimic the real-world situation in which forecasters are tasked with providing predictions about the future course of a seasonal influenza epidemic based only on data available at a given point in time. The component models, therefore, were specified so as to predict the season-level target, say, peak hospitalization rate, using data only up to and including the current week. In creating the lagged terms, differences, and product terms, our intent was to create predictors that would carry some information regarding hospitalization dynamics, including the rate of change in hospitalization rates and interdependencies between the current hospitalization rate in a given week and cumulative hospitalization rates. Therefore, the super learner was fit 30 times for each prediction target, corresponding to each week in the flu season.

The predictive accuracy for the component models and the ensemble learner using 15-fold cross-validation [@Harrell2015-cd]. Briefly, *V*-fold cross-validation is an iterative sample-splitting procedure in which each observation is assigned to a group, called a fold, and where each fold serves as the hold-out validation set for a model trained on the rest of the sample [@Harrell2015-cd; @Naimi2018-fv]. This procedure maximizes the amount of data available for model-fitting by avoiding the need to retain a separate hold-out validation sample while at the same time reducing the potential for overfitting [@Van_der_Laan2007-ml; @Naimi2018-fv]. The predictive accuracy of each learning algorithm is evaluated as a "risk", in our case defined as the average absolute error between a learner's prediction and the true observation (e.g., simulated peak hospitalization rate). These risks are estimated in each fold when it is assigned to be the validation set during cross-validation. Therefore, in the current context, the "cross-validated risk" refers to the average risk across all 15 folds [@Van_der_Laan2007-ml, @Naimi2018-fv].

We repeated the following procedure for each prediction target:

1. Create 30 data sets, one for each week of the flu season, containing the outcome variables (peak rate, peak week, and cumulative hospitalizations) and the predictors described previously.
1. Assign each observation to a fold for use in cross-validation. Each weekly dataset contained 15,000 rows, one for each simulated influenza hospitalization curve.
   a. To account for the lack of independence between curves, we assigned all simulated curves based on the same empirical season shape to the same fold (e.g., all curves based on the trend filter fit to season 2004-05). We did not account for potential dependencies due to repeated sampling of the noise term, as we believed the season shape was more consequential with respect to inducing dependence between simulated curves. For one, the noise injected into the simulated curves was typically very small. In addition, the season shape is the strongest determinant of how the simulated hospitalization curve changes over time.    
1. Execute the super learner algorithm:
   a. Train each learner on the data set using 15-fold cross-validation and calculate its cross-validated risk as the average prediction risk across the 15 validation sets.
   b. Generate predictions using each component learner in the full data set.
   c. c.	Create a linear combination of the individual learners where the weights are selected by minimizing the absolute error [@Van_der_Laan2007-ml; @Naimi2018-fv]. The model that assigns each component learner its weights is referred to as the "metalearner" [@Coyle2020-ze].
   d. Calculate the cross-validated risk for the ensemble super learner as in step 4a.

In all, we trained 80 component models for each prediction target and within each week of the season. This total includes variations within classes of learners achieved by perturbing various tuning and input parameters \@ref(tab:candmodels-tuning-table). The metalearner was specified using the `sl3` [@Coyle2020-ze] package's implementation of the `solnp` function from the `Rsolnp` package [@Ghalanos2015-sn]. 


## Component models

We used the same library of component models to build the super learners for all three prediction targets (**Table \@ref(tab:candmodels-tuning-table)**). Theory and finite-sample demonstrations support the idea that the super learner should benefit from being provided with a large variety of component models, even if some of those models perform poorly in the given data [@Van_der_Laan2007-ml]. Therefore, we attempted to specify component learners and tuning parameter sets (e.g., variable model parameters such as kernels and complexity penalties) in order to probe a diverse set of models and learning algorithms.

All component learners were implemented using functions provided in the `sl3` [@Coyle2020-ze] and `SuperLearner` [@Polley2019-sl] packages in R. Additional information on the component learners is provided in the **S3 Appendix**.

```{r tab.id="candmodels-tuning-table", tab.cap="**Component learners and tuning parameters.**"}
tunetab <- tribble(
  ~ Model,
  ~ `Tuning parameters`,
  ~ `R package`,

  "Median prediction",
  "Median of the outcome distribution",
  "N/A",

  "Linear regression with variable screening",
  "Variables screened for inclusion in the linear prediction model using correlation-based measures.",
  "base",

  "Random forest (regression trees)",
  "Number of trees: {50, 100, 500}; Terminal node sizes: {3, 5, 10}; Number of variables sampled for use in data-splitting: {ncov / 3, ncov / 2, ncov / 1.5}",
  "randomForest",

  "Support vector regression",
  "Kernel: {radial, polynomial}; Degree: {1, 2, 3}, applied only to polynomial",
  "svm",

  "Penalized regression",
  "Penalty: {lasso, ridge}",
  "glmnet",

  "Elastic net",
  "Alpha penalty: {0.25, 0.5, 0.75}",
  "glmnet",

  "Neural network",
  "Number of nodes in hidden layer: {5, 10, 25, 50, 75, 100}; Decay: {0, 0.005, 0.1, 0.2, 0.4}",
  "nnet",

  "LOESS",
  "Span = {0.25, 0.5, 0.75, 1}",
  "base",

  "Polynomial multivariate adaptive regression spline",
  "Gcv penalty = {2, 4, 6, 8, 10}",
  "polspline",
)

fn <-  as_paragraph(as_i("ncov"),  ", number of predictors. For the linear regression model, the variable-screening procedure is not a tuning parameter per se. We describe it in the \"tuning parameter\" column for ease of presentation. For the random forest and neural network learners, all combinations of the tuning parameters presented were proposed as separate learning algorithms.")

tt <- tunetab %>%
  flextable(., cwidth = c(1.9, 3.5, 1.2)) %>%
  fit_to_width(., max_width = 7.5) %>% 
  bold(., part = "header") %>%
  align(j = 1:2, align = "left", part = "header") %>%
  padding(padding = 7, part = "body") %>%
  align(j = 1:2, align = "left") %>%
  valign(valign = "top") %>%
  add_footer_row(top = FALSE, colwidths = 3, values = "") %>%
  font(., fontname = global_table_font, part = "all")

tt <- compose(tt, part = "footer", value = fn)
tt
```
 
For each target parameter, we compared the ensemble super learner's performance against the absolute error loss of a naive prediction using only the outcome median. The loss for this benchmark was calculated across the entire outcome distribution.
 
## Sensitivity analyses

We conducted three sensitivity analyses to explore the ensemble learner's performance under different assumptions or analytic choices. 

In the first, we simulated curves using linear trend filter fits based on an alternate $\lambda$ penalty. The alternate penalty was selected as the $\lambda$ that produced a cross-validated error estimate within one standard error of the prediction error produced by $\lambda_{min}$ [@Arnold-2019-gl]. We refer subsequently to this alternate penalty as $\lambda_{SE}$.

In the second, we included only the elastic net and random forest learners, based on their generally good performance in the main analysis. This post-hoc analysis was intended to compare the quality of predictions produced by the ensemble super learner given a small subset of consistently well-performing component learners.

In the third, we optimized the ensemble super learner using the squared error loss [@Polley2011-oz] instead of the absolute error loss to investigate whether the choice of optimization goal qualitatively changed our conclusions about the ensemble's performance. All other procedures in this post-hoc analysis were conducted as described in the main analysis, except that 1) the benchmark used as a comparison in this analysis was the squared error loss of a naive prediction based on the outcome mean, and 2) we included only a subset of weeks (1, 5, 10, 15, 20, 25, 30) to avoid unnecessary computation while still characterizing the ensemble super learner's performance across the season.

## Technical details

The super learner algorithm was implemented on a high-performance computing cluster housed at the Center for Computation and Visualization at Brown University. Each run was submitted to a large-memory node and parallelized across 32 central processing units using functionality provided in `sl3` [@Coyle2020-ze] via the `delayed` [@Coyle2020-rq] and `future` [@Bengtsson2020-oz] packages in R. In general, running the super learner algorithm required between (roughly) 30--60 minutes per week across all prediction targets, with approximately 22--24 hours required to conduct a complete set of computing jobs for a single prediction target. 

## Ethics statement

This manuscript involves only the use of publicly available influenza surveillance data aggregated at the national level. Therefore, the study is not considered Human Subjects Research and no Institutional Review Board approval was required.

# Results

## Peak rate

```{r main-peak-rate-setup}
## This function retrieves risks from formatted risk tables and converts
## them to numeric vectors for reporting within the text.
get_risks <- function(data, variable) {
  data[[variable]] %>%
    stringr::str_extract("[0-9]*.\\.[0-9]{1,2}") %>%
    as.numeric
}


pkrate_risks <- fread(get_asset("TAB", "Risks-EDSL-Mean_PeakRate"))
presl_num <- get_risks(pkrate_risks, "SuperLearner")
prdsl_num <- get_risks(pkrate_risks, "DiscreteSL")
```

For the peak rate prediction target, the ensemble super learner produced risk estimates similar to the median prediction (**Table \@ref(tab:peakrate-risk-table)**). Throughout the flu season, the discrete super learner (best-performing component model) consistently exhibited a lower cross-validated risk than both the median prediction and the ensemble super learner, indicating better predictive ability in general. Furthermore, the discrete super learner's cross-validated risk generally decreased as the season progressed. However, the component models selected as the  discrete super learner varied by week, while the ensemble super learner's risk remained quite stable throughout the simulated flu season, and began to improve slightly over the median prediction late in the flu season (**Table \@ref(tab:peakrate-risk-table), Fig \@ref(fig:ensemble-perf-main)**). The ensemble predictions for the peak hospitalization rate exhibited median prediction risks ranging between `r min(presl_num)` and `r max(presl_num)` per 100,000 population across the flu season. (**Fig \@ref(fig:ensemble-perf-regscale)**). For the discrete super learner, these risks ranged between `r min(prdsl_num)` and `r max(prdsl_num)` per 100,000 population.


```{r risks-datadir}

risktab_cnames_word <- function(dat) {
  set_header_labels(
    dat,
    Week = "Week",
    SuperLearner = "EnsembleSL",
    BestComponent = "DiscreteSL"
  )
}

```

```{r tab.id="peakrate-risk-table", tab.cap="**Cross-validated risks for the ensemble and discrete super learner predictions of peak hospitalization rate, by week of influenza season. Estimates are presented as mean risk (standard error).**"}

pkrate_risks %>%
  flextable(
    cwidth = c(0.6, rep(1.1, 2))
  ) %>%
  risktab_cnames_word() %>%
  bold(., part = "header") %>%
  add_footer_lines(risktabfn("Peak rate")) %>%
  font(., fontname = global_table_font, part = "all")
```


```{r fig.id="ensemble-perf-main", fig.cap="**Ensemble, component learner, and naive median prediction risks by week of simulated flu season and prediction target (main analysis).** Learners assigned zero weights by the metalearner are omitted.", fig.width = 4.8, fig.asp = 3000/1800}
knitr::include_graphics(get_asset("FIG", "Ensemble-Summary_All-Targets"))
```

```{r fig.id="ensemble-perf-regscale", fig.cap="**Ensemble prediction risks by week of simulated flu season, prediction target, and sensitivity analysis.** Estimates are plotted on the original scale for each prediction target. The pointranges display the means and 95% confidence intervals for the ensemble prediction risks in each week.", fig.width = 6.5, fig.asp = 2400/3000}
knitr::include_graphics(get_asset("FIG", "Ensemble-Summary_All-Targets_Regular-Scale"))
```

## Peak week 

```{r main-peak-week-setup}
pkweek_risks <- fread(get_asset("TAB", "Risks-EDSL-Mean_PeakWeek"))
pwesl_num <- get_risks(pkweek_risks, "SuperLearner")
pwdsl_num <- get_risks(pkweek_risks, "DiscreteSL")
```

For the peak week prediction target, the ensemble super learner generally produced risk estimates close to those of the peak week predictions based on a simple mean (**Table \@ref(tab:peakweek-risk-table)**). The discrete super learner consistently exhibited lower mean risk than both the ensemble super learner and predictions based on the simple mean. As with peak rate, the discrete super learner tended to perform better as the flu season progressed. The ensemble super learner, on the other hand, again provided generally stable cross-validated risk throughout the season, though with more variability around the mean prediction and several instances in which the ensemble prediction appeared to be affected adversely by the presence of lower-performing component learners. In addition, the ensemble super learner did not appear to improve later in the flu season, as we observed for the peak rate prediction target (**Table \@ref(tab:peakweek-risk-table), Fig \@ref(fig:ensemble-perf-main)**). The ensemble predictions for the peak week exhibited mean prediction risks ranging between `r min(pwesl_num)` and `r max(pwesl_num)` weeks across the flu season. (**Fig \@ref(fig:ensemble-perf-regscale)**). For the discrete super learner, these risks ranged between `r min(pwdsl_num)` and `r max(pwdsl_num)` weeks.


```{r tab.id="peakweek-risk-table", tab.cap="**Cross-validated risks for the ensemble and discrete super learner predictions of the week in which the peak hospitalization rate occurs, by week of influenza season. Estimates presented as mean risk (standard error).**"}

pkweek_risks <- fread(get_asset("TAB", "Risks-EDSL-Mean_PeakWeek"))

pkweek_risks %>%
  flextable(
    cwidth = c(0.6, rep(1.1, 2))
  ) %>%
  risktab_cnames_word() %>%
  bold(., part = "header") %>%
  add_footer_lines(risktabfn("Peak week")) %>%
  font(., fontname = global_table_font, part = "all")
```

## Cumulative hospitalizations

```{r main-cum-hosp-setup}
cumhosp_risks <- fread(get_asset("TAB", "Risks-EDSL-Mean_CumHosp"))
chesl_num <- get_risks(pkweek_risks, "SuperLearner")
chdsl_num <- get_risks(pkweek_risks, "DiscreteSL")
```

For the cumulative hospitalizations prediction target, the ensemble super learner produced risk estimates generally near to those based on a prediction based on the outcome median (**Table \@ref(tab:cumhosp-risk-table)**). Again, the discrete super learner consistently outperformed both the ensemble super learner and the simple mean prediction, improving substantially as the flu season progressed. Different discrete super learners were selected throughout the season, and the ensemble super learner again provided predictions with stable risk throughout the simulated flu season but did not generally improve as the season progressed (**Table \@ref(tab:cumhosp-risk-table), Fig \@ref(fig:ensemble-perf-main)**). The ensemble predictions for the cumulative hospitalization rate exhibited mean prediction risks between `r min(chesl_num)` and `r max(chesl_num)` per 100,000 population, and these risks appeared to be higher later in the season. (**Fig \@ref(fig:ensemble-perf-regscale)**). For the discrete super learner, these risks ranged between `r min(chdsl_num)` and `r max(chdsl_num)` per 100,000 population.

```{r tab.id="cumhosp-risk-table", tab.cap="**Cross-validated risks for the ensemble and discrete super learner predictions of the cumulative hospitalization rate, by week of influenza season. Estimates presented as mean risk (standard error).**"}

cumhosp_risks <- fread(get_asset("TAB", "Risks-EDSL-Mean_CumHosp"))

cumhosp_risks %>%
  flextable(
    cwidth = c(0.6, rep(1.1, 2))
  ) %>%
  risktab_cnames_word() %>%
  bold(., part = "header") %>%
  add_footer_lines(risktabfn("Cumulative hospitalizations")) %>%
  font(., fontname = global_table_font, part = "all")

```

## Sensitivity analyses 

```{r sen-l1se-setup}
pkrate_risks_1se <- fread(get_asset("TAB", "Risks-EDSL-Mean_PeakRate-L1SE"))
presl_num_1se <- get_risks(pkrate_risks_1se, "SuperLearner")
prdsl_num_1se <- get_risks(pkrate_risks_1se, "DiscreteSL")

pkweek_risks_1se <- fread(get_asset("TAB", "Risks-EDSL-Mean_PeakWeek-L1SE"))
pwesl_num_1se <- get_risks(pkweek_risks_1se, "SuperLearner")
pwdsl_num_1se <- get_risks(pkweek_risks_1se, "DiscreteSL")

cumhosp_risks_1se <- fread(get_asset("TAB", "Risks-EDSL-Mean_CumHosp-L1SE"))
chesl_num_1se <- get_risks(cumhosp_risks_1se, "SuperLearner")
chdsl_num_1se <- get_risks(cumhosp_risks_1se, "DiscreteSL")
```
**Alternate trend filter penalty.** The ensemble super learner trained on a distribution of simulated influenza hospitalization curves based on an alternate trend filter penalty generally produced mean prediction risks lower than those of the naive median prediction (**Fig \@ref(fig:ensemble-perf-regscale)**). The ensemble's prediction risks were relatively stable across the season for each prediction target except the peak rate target. For the peak rate target, the ensemble's predictions tended to have somewhat higher risk toward the beginning of the season, in some cases underperforming relative to the naive median prediction. Estimates of the mean prediction risks varied more than in the main analysis, but the confidence intervals around these means were narrower.

For the peak rate prediction target, the ensemble super learner's predictions exhibited mean prediction risks ranging between `r min(presl_num_1se)` and `r max(presl_num_1se)` per 100,000 population and the discrete super learner's between `r min(prdsl_num_1se)` and `r max(presl_num_1se)` per 100,000 population. For the peak week prediction target, the ensemble super learner's predictions exhibited mean prediction risks ranging between `r min(pwesl_num_1se)` and `r max(pwesl_num_1se)` weeks and the discrete super learner's between `r min(pwdsl_num_1se)` and `r max(pwdsl_num_1se)` weeks. For the cumulative hospitalization prediction target, the ensemble super learner's predictions exhibited mean prediction risks ranging between `r min(chesl_num_1se)` and `r max(chesl_num_1se)` per 100,000 population and the discrete super learner's ranged between `r min(chdsl_num_1se)` and `r max(chdsl_num_1se)` per 100,000 population.

```{r sen-erf-setup}
pkrate_risks_erf <- fread(get_asset("TAB", "Risks-EDSL-Mean_PeakRate-ERF"))
presl_num_erf <- get_risks(pkrate_risks_erf, "SuperLearner")
prdsl_num_erf <- get_risks(pkrate_risks_erf, "DiscreteSL")

pkweek_risks_erf <- fread(get_asset("TAB", "Risks-EDSL-Mean_PeakWeek-ERF"))
pwesl_num_erf <- get_risks(pkweek_risks_erf, "SuperLearner")
pwdsl_num_erf <- get_risks(pkweek_risks_erf, "DiscreteSL")

cumhosp_risks_erf <- fread(get_asset("TAB", "Risks-EDSL-Mean_CumHosp-ERF"))
chesl_num_erf <- get_risks(cumhosp_risks_erf, "SuperLearner")
chdsl_num_erf <- get_risks(cumhosp_risks_erf, "DiscreteSL")
```

**Component learner subset.** The ensemble learner trained using a subset of component learners that included only the elastic net and random forest learners tended to produce mean prediction risks consistently lower than those of the naive median prediction across the flu season (**Fig \@ref(fig:ensemble-perf-regscale)**). We observed this general pattern across all prediction targets. For the peak rate and cumulative hospitalization prediction targets, the ensemble's prediction risks also appeared to improve later in the season, generally at or before week 10. 

For the peak rate prediction target, the ensemble super learner's predictions exhibited mean prediction risks ranging between `r min(presl_num_erf)` and `r max(presl_num_erf)` per 100,000 population and the discrete super learner's between `r min(prdsl_num_erf)` and `r max(presl_num_erf)` per 100,000 population. For the peak week prediction target, the ensemble super learner's predictions exhibited mean prediction risks ranging between `r min(pwesl_num_erf)` and `r max(pwesl_num_erf)` weeks and the discrete super learner's between `r min(pwdsl_num_erf)` and `r max(pwdsl_num_erf)` weeks. For the cumulative hospitalization prediction target, the ensemble super learner's predictions exhibited mean prediction risks ranging between `r min(chesl_num_erf)` and `r max(chesl_num_erf)` per 100,000 populationand the discrete super learner's between `r min(chdsl_num_erf)` and `r max(chdsl_num_erf)` per 100,000 population.

```{r sen-sqe-setup}
pkrate_risks_sqe <- fread(get_asset("TAB", "Risks-EDSL-Mean_PeakRate-SQE"))
presl_num_sqe <- get_risks(pkrate_risks_sqe, "SuperLearner")
prdsl_num_sqe <- get_risks(pkrate_risks_sqe, "DiscreteSL")

pkweek_risks_sqe <- fread(get_asset("TAB", "Risks-EDSL-Mean_PeakWeek-SQE"))
pwesl_num_sqe <- get_risks(pkweek_risks_sqe, "SuperLearner")
pwdsl_num_sqe <- get_risks(pkweek_risks_sqe, "DiscreteSL")

cumhosp_risks_sqe <- fread(get_asset("TAB", "Risks-EDSL-Mean_CumHosp-SQE"))
chesl_num_sqe <- get_risks(cumhosp_risks_sqe, "SuperLearner")
chdsl_num_sqe <- get_risks(cumhosp_risks_sqe, "DiscreteSL")
```

**Squared error loss.** The ensemble learner trained to optimize predictions with respect to the squared error loss (as opposed to the absolute error loss) consistently exhibited mean prediction risks lower than those of the naive mean prediction across the flu season and for all prediction targets (**Fig 5**). Predictions did not tend to improve as the season progressed, as observed for some prediction targets in the main and other sensitivity analyses, but the ensemble predictions tended to be more stable throughout the entire season and provided a more substantial improvement over the naive predictor.

For the peak rate prediction target, the ensemble super learner's predictions exhibited mean prediction risks ranging between `r min(presl_num_sqe)` and `r max(presl_num_sqe)` per 100,000 population and the discrete super learner's between `r min(prdsl_num_sqe)` and `r max(presl_num_sqe)` per 100,000 population. For the peak week prediction target, the ensemble super learner's predictions exhibited mean prediction risks ranging between `r min(pwesl_num_sqe)` and `r max(pwesl_num_sqe)` weeks and the discrete super learner's between `r min(pwdsl_num_sqe)` and `r max(pwdsl_num_sqe)` weeks. For the cumulative hospitalization prediction target, the ensemble super learner's predictions exhibited mean prediction risks ranging between `r min(chesl_num_sqe)` and `r max(chesl_num_sqe)` per 100,000 population and the discrete super learner's between `r min(chdsl_num_sqe)` and `r max(chdsl_num_sqe)` per 100,000 population.

```{r fig.id="ensemble-perf-sqe", fig.cap="**Ensemble, component learner, and naive mean prediction risks by week of simulated flu season and prediction target (squared error sensitivity analysis).** Learners assigned zero weights by the metalearner are omitted.", fig.width = 4.8, fig.asp = 3000/1800}

knitr::include_graphics(get_asset("FIG", "Ensemble-Summary_All-Targets-SQE"))
```

# Discussion

In our study, we found that discrete super learners were consistently better at predicting median peak rate, peak week, and cumulative hospitalizations compared to a naive prediction based on the outcome median when fit to a distribution of hypothetical influenza hospitalization curves. The discrete super learner's predictions also improved over the course of the season compared to the median prediction, which comports with prior research in influenza-like illness prediction [@Ray2018-ef]. While rarely matching the performance of the discrete super learner, the ensemble super learner in the main analysis nevertheless exhibited stable cross-validated risk across the course of the flu season for all prediction targets. In general, the ensemble performed comparably to the median prediction and, for the peak rate prediction target, somewhat better than the median prediction late in the season. The magnitude of this improvement was small, however, amounting to an improvement in the prediction of the peak rate around 2 hospitalizations per 100,000 population. 

We also found the ensemble estimates tended to balance the performance of the various component learners, avoiding extreme prediction errors. Theory suggests that the ensemble super learner is generally assumed not to be affected adversely by the inclusion of poor algorithms or overfitting, as the cross-validation procedure should result in a final weighted ensemble that assigns low or no weight to poor-performing predictions [@Polley2010-cb; @Polley2011-oz]. We did find the super learner assigned weights to prediction algorithms that appeared to affect the ensemble's predictive performance adversely. Nonetheless, the ensemble predictions in the main analysis produced generally consistent risk estimates across the flu season, which might be a desirable property in its own right, particularly in contexts when little information exists to guide a priori  selection of component learners. 

Sensitivity analyses provided qualitatively similar results as the main analysis. Namely, in all cases the ensemble super learner consistently performed at least as well as a naive prediction based on the outcome median (or mean, in the case of the squared error loss sensitivity analysis). We did observe some notable differences, however. In the alternate trend filer penalty analysis, mean predictions risks for the peak rate target tended to be more variable across the season compared to the main analysis. In the component learner subset analysis, the ensemble predictions appeared to benefit from our limiting the component learner library to elastic net and random forest learners. Specifically, ensemble predictions exhibited lower prediction risk later in the season, apparently picking up more of the signal from high-performing learners than in the main analysis. Perhaps the most notable difference between the component learner subset and the main analysis involves the cumulative hospitalization prediction target. In the main analysis, the ensemble predictions tended not to outperform the naive prediction later in the flu season, after providing some additional predictive power early in the season. However, the component learner subset produced prediction risks for the cumulative hospitalization rate that did improve over the naive prediction later in the season, suggesting the ensemble in the main analysis suffered due to the inclusion of the larger library. Despite these findings, an important caveat remains regarding the component learner subset: the elastic net and random forest learners were selected for inclusion post-hoc, based on generally low prediction risks demonstrated by these learners in the main analysis. Our results suggest that restricting component learners based on an _a priori_ expectation of good predictive ability may be beneficial for certain applications of the ensemble super learner approach, but future investigation would need to test such a hypothesis formally by measuring the external predictive validity of ensemble super learners trained using different prespecified component learner libraries.

Our findings should be interpreted in light of several limitations. 

First, while our simulating 15,000 hypothetical hospitalization curves provided a large sample on  which to train the super learner, the validity of our findings rely on the assumption that the distribution of simulated curves accurately characterizes the hypothetical distribution of plausible seasonal influenza hospitalization trajectories. We did not attempt to rigidly match the simulated distributions to their empirical counterparts so as not to restrict the possibility of atypical influenza seasons that could be, but perhaps have not yet been, observed. In other words, we did not assume the 15 recorded seasons' worth of national-level influenza hospitalization data captured the full range of possible values for the selected prediction targets, simply that the empirical data provided a reasonable diversity of curve shapes from which to simulate the hypothetical distribution. Because surveillance data likely underestimate the rate of influenza-attributable hospitalizations, as discussed in the Methods, our simulated flu seasons might not capture potential extreme seasons marked by especially severe peaks or high cumulative hospitalization rates. On the other hand, our simulations produced right-skewed distributions of peak hospitalization rate and cumulative hospitalizations that may effectively compensate for the potential bias in the empirical data. 

Second, to our knowledge, no consensus currently exists regarding the best procedure to select the $\lambda$ penalty for the trend filter, although at least one method has been proposed [@Yamada2016-fq]. Therefore, we emulated Brooks et al. [@Brooks2015-fl] and chose the trend filter $\lambda$ penalties based on cross-validation. It is possible that other choices of the $\lambda$ penalty might result in simulated distributions with different properties that affect the behavior of the super learner.

Third, we did not include several types of models that might have improved the ensemble super learner's performance, due either to computational limitations (e.g., gradient boosted and mechanistic models) or the additional complexity required to implement time series methods in the super learner framework (e.g., exponential smoothing or trend differencing models [@Hyndman-2018-fp]). Where mechanistic models might provide additional predictive capabilities, a super learner might incorporate such models or, alternatively, the predictions generated by an ensemble super learner could be incorporated into a wider ensemble that also includes predictions from mechanistic models [@Reich2019-uk]. One might also incorporate time series methods using online ensembles, which are designed in part to reduce the computational load required to generate time series-based ensemble predictions [@Benkeser2018-if]. 

Fourth, our approach to simulating hypothetical influenza hospitalization curves used only information on hospitalization rates. As a consequence, we were unable to examine whether incorporating other influenza-related surveillance data (e.g., ILI, viral activity [@Centers_for_Disease_Control_and_Prevention_undated-vt]) or climate data  may have improved the super learner's predictions.

Finally, we focused our analysis on a simulated distribution of season influenza hospitalization curves. Pandemic influenza hospitalizations follow different dynamics, and our results likely do not apply to influenza pandemics. However, ensemble forecasting may become an increasingly important tool during infectious disease pandemics in general (e.g., the ongoing SARS-CoV-2 pandemic [@CovidHub-2020-fh]).

Our study found that an ensemble super learner, provided only with information about influenza-related hospitalization rates, was capable of producing predictions of stable quality throughout the flu season in a distribution of simulated hospitalization curves. Future work should focus on investigating the super learner's performance in empirical data sets, incorporating additional information (e.g., data on influenza-like illness, viral activity, and climate), and translating the approach to generate in-season forecasts that comply with CDC FluSight Network formats and guidelines. In addition, applying the super learner to predict influenza hospitalizations at the regional and state levels, rather than simply at the national level, may be a promising avenue for improving the quality of information available to public health and medical institutions tasked with responding proactively to changes in health care needs during a seasonal flu epidemic.

# Software and code

The code necessary to reproduce these results and the output from the super learner procedures are provided in a repository at Open Science Framework **(doi: XXXXXXXXX)**. These materials are also housed permanently at the Brown Digital Repository **(doi/url: XXXXXXXXXXX)**, and the package can be installed directly via the Github repository ([https://www.github.com/jrgant/FluHospPrediction](https://www.github.com/jrgant/FluHospPrediction)).

# Declarations
  
## Acknowledgment

We would like to thank Ashley Naimi, Laura Balzer, and Nicholas Reich for their helpful comments concerning the study aims and the super learner approach, as well as Edward Thommes for his comments on a draft of the manuscript. The super learner algorithm was fit using computational resources and services provided by the Center for Computation and Visualization at Brown University.

## Funding statement

This Brown University collaborative research was funded by Sanofi Pasteur (PI: Andrew Zullo). Dr. Zullo was also supported, in part, by grants R21AG061632 and R01AG065722 from the National Institute on Aging and U54GM115677 from the National Institute of General Medical Sciences. In addition, Dr. Zullo was supported by a US Department of Veterans Affairs Office of Academic Affiliations Advanced Fellowship in Health Services Research and Development.

## Disclaimer

The views expressed in this article are those of the authors and do not necessarily reflect the position or policy of the Department of Veterans Affairs or the United States Government.

## Competing interests

KWM receives funding support from Seqirus pharmaceuticals.

\newpage
# References

<div id="refs"></div>
\bibliography{references}
\newpage

\newpage 
# Supplemental Information

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-tf-pred-lmin, results = "asis", fig.asp = 1800/2250}
slug <- "TF-Predictions_lambda.min"
supp_cap(slug)
include_graphics(get_asset("FIG", slug))
```
\newpage

<!-- SUPPLEMENTAL TABLE -->

```{r -supp-simtemplate-counts, results = "asis"}
slug <- "Simulation-Template-Counts"
tmpcts <- fread(get_asset("TAB", slug))

supp_cap(slug)

tmpcts %>%
  flextable %>%
  bold(part = "header") %>%
  font(., fontname = global_table_font, part = "all") %>%
  autofit
```
\newpage

<!-- SUPPLEMENTAL APPENDIX -->

```{r supp-appendix, results = "asis"}
supp_cap("")
cat("\n\nSee supplemental appendix file.")
```
\newpage


<!-- SUPPLEMENTAL TABLE -->

```{r supp-peakrate-meanrisk, results = "asis"}
slug <- "Risk-Week_Peak-Rate"
rpsum <- fread(get_asset("TAB", slug))

supp_cap(slug)

rpsum %>%
  flextable(cwidth = c(0.6, rep(1.15, 5))) %>%
  colformat_num(
    j = 2:6,
    digits = 2
  ) %>%
  bold(., part = "header") %>%
  padding(padding = 0, part = "all") %>%
  font(., fontname = global_table_font, part = "all")

```
\newpage

<!-- SUPPLEMENTAL TABLE -->

```{r supp-peakweek-meanrisk, results = "asis"}
slug <- "Risk-Week_Peak-Week"
rwsum <- fread(get_asset("TAB", slug))

supp_cap(slug)

rwsum %>%
  flextable(cwidth = c(0.6, rep(1.15, 5))) %>%
  colformat_num(
    j = 2:6,
    digits = 2
  ) %>%
  bold(., part = "header") %>%
  padding(padding = 0, part = "all") %>%
  font(., fontname = global_table_font, part = "all")
```
\newpage

<!-- SUPPLEMENTAL TABLE -->

```{r supp-cumhosp-meanrisk, results = "asis"}
slug <- "Risk-Week_Cum-Hosp"
rwsum <- fread(get_asset("TAB", slug))

supp_cap(slug)

rwsum %>%
  flextable(cwidth = c(0.6, rep(1.15, 5))) %>%
  colformat_num(
    j = 2:6,
    digits = 2
  ) %>%
  bold(., part = "header") %>%
  padding(padding = 0, part = "all") %>%
  font(., fontname = global_table_font, part = "all")
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-peakrate-risktiles, results = "asis", fig.asp = 1950/3600}
slug <- "Risktiles_Peak-Rate"
supp_cap(slug)
knitr::include_graphics(get_asset("FIG", slug))
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-peakweek-risktiles, results = "asis", fig.asp = 1950/3600}
slug <- "Risktiles_Peak-Week"
supp_cap(slug)
knitr::include_graphics(get_asset("FIG", slug))
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-cumhosp-risktiles, results = "asis", fig.asp = 1950/3600}
slug <- "Risktiles_Cum-Hosp"
supp_cap(slug)
knitr::include_graphics(get_asset("FIG", slug))
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-tfpred-lse, results = "asis", fig.asp = 1800/2250}
slug <- "TF-Predictions_lambda.1se"
supp_cap(slug)
include_graphics(get_asset("FIG", slug))
```
\newpage

<!-- SUPPLEMENTAL TABLE -->

```{r supp-peakrate-risks-lse, results = "asis"}
slug <- "Risks-EDSL-Mean_PeakRate-L1SE"
pkrate_risks <- fread(get_asset("TAB", slug))

supp_cap(slug)

pkrate_risks %>%
  flextable(
    cwidth = c(0.6, rep(1.1, 2))
  ) %>%
  risktab_cnames_word() %>%
  bold(., part = "header") %>%
  add_footer_lines(risktabfn("Peak rate")) %>%
  font(., fontname = global_table_font, part = "all")
```
\newpage

<!-- SUPPLEMENTAL TABLE -->

```{r supp-peakweek-risks-lse, results = "asis"}
slug <- "Risks-EDSL-Mean_PeakWeek-L1SE"
pkweek_risks <- fread(get_asset("TAB", slug))

supp_cap(slug)

pkweek_risks %>%
  flextable(
    cwidth = c(0.6, rep(1.1, 2))
  ) %>%
  risktab_cnames_word() %>%
  bold(., part = "header") %>%
  add_footer_lines(risktabfn("Peak week")) %>%
  font(., fontname = global_table_font, part = "all")
```
\newpage

<!-- SUPPLEMENTAL TABLE -->

```{r supp-cumhosp-risks-lse, results = "asis"}
slug <- "Risks-EDSL-Mean_CumHosp-L1SE"
cumhosp_risks <- fread(get_asset("TAB", slug))

supp_cap(slug)

cumhosp_risks %>%
  flextable(
    cwidth = c(0.6, rep(1.1, 2))
  ) %>%
  risktab_cnames_word() %>%
  bold(., part = "header") %>%
  add_footer_lines(risktabfn("Cumulative hospitalizations")) %>%
  font(., fontname = global_table_font, part = "all")
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-peakrate-risktiles-lse, results = "asis", fig.asp = 1800/3600}
slug <- "Risktiles_Peak-Rate-L1SE"
supp_cap(slug)
knitr::include_graphics(get_asset("FIG", slug))
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-peakweek-risktiles-lse, results = "asis", fig.asp = 1800/3600}
slug <- "Risktiles_Peak-Week-L1SE"
supp_cap(slug)
knitr::include_graphics(get_asset("FIG", slug))
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-cumhosp-risktiles-lse, results = "asis", fig.asp = 1800/3600}
slug <- "Risktiles_Cum-Hosp-L1SE"
supp_cap(slug)
knitr::include_graphics(get_asset("FIG", slug))
```
\newpage


<!-- SUPPLEMENTAL TABLE -->

```{r supp-peakrate-risks-erf, results = "asis"}
slug <- "Risks-EDSL-Mean_PeakRate-ERF"
pkrate_risks <- fread(get_asset("TAB", slug))

supp_cap(slug)

pkrate_risks %>%
  flextable(
    cwidth = c(0.6, rep(1.1, 2))
  ) %>%
  risktab_cnames_word() %>%
  bold(., part = "header") %>%
  add_footer_lines(risktabfn("Peak rate")) %>%
  font(., fontname = global_table_font, part = "all")
```
\newpage

<!-- SUPPLEMENTAL TABLE -->

```{r supp-peakweek-risks-erf, results = "asis"}
slug <- "Risks-EDSL-Mean_PeakWeek-ERF"
pkweek_risks <- fread(get_asset("TAB", slug))

supp_cap(slug)

pkweek_risks %>%
  flextable(
    cwidth = c(0.6, rep(1.1, 2))
  ) %>%
  risktab_cnames_word() %>%
  bold(., part = "header") %>%
  add_footer_lines(risktabfn("Peak week")) %>%
  font(., fontname = global_table_font, part = "all")
```
\newpage

<!-- SUPPLEMENTAL TABLE -->

```{r supp-cumhosp-risks-erf, results = "asis"}
slug <- "Risks-EDSL-Mean_CumHosp-ERF"
cumhosp_risks <- fread(get_asset("TAB", slug))

supp_cap(slug)

cumhosp_risks %>%
  flextable(
    cwidth = c(0.6, rep(1.1, 2))
  ) %>%
  risktab_cnames_word() %>%
  bold(., part = "header") %>%
  add_footer_lines(risktabfn("Cumulative hospitalizations")) %>%
  font(., fontname = global_table_font, part = "all")
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-peakrate-risktiles-erf, results = "asis", fig.asp = 1950/3600}
slug <- "Risktiles_Peak-Rate-ERF"
supp_cap(slug)
knitr::include_graphics(get_asset("FIG", slug))
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-peakweek-risktiles-erf, results = "asis", fig.asp = 1950/3600}
slug <- "Risktiles_Peak-Week-ERF"
supp_cap(slug)
knitr::include_graphics(get_asset("FIG", slug))
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-cumhosp-risktiles-erf, results = "asis", fig.asp = 1950/3600}
slug <- "Risktiles_Cum-Hosp-ERF"
supp_cap(slug)
knitr::include_graphics(get_asset("FIG", slug))
```
\newpage


<!-- SUPPLEMENTAL TABLE -->

```{r supp-peakrate-risks-sqe, results = "asis"}
slug <- "Risks-EDSL-Mean_PeakRate-SQE"
pkrate_risks <- fread(get_asset("TAB", slug))

supp_cap(slug)

pkrate_risks %>%
  flextable(
    cwidth = c(0.6, rep(1.1, 2))
  ) %>%
  risktab_cnames_word() %>%
  bold(., part = "header") %>%
  add_footer_lines(risktabfn("Peak rate", sqe = TRUE)) %>%
  font(., fontname = global_table_font, part = "all")
```
\newpage

<!-- SUPPLEMENTAL TABLE -->

```{r supp-peakweek-risks-sqe, results = "asis"}
slug <- "Risks-EDSL-Mean_PeakWeek-SQE"
pkweek_risks <- fread(get_asset("TAB", slug))

supp_cap(slug)

pkweek_risks %>%
  flextable(
    cwidth = c(0.6, rep(1.1, 2))
  ) %>%
  risktab_cnames_word() %>%
  bold(., part = "header") %>%
  add_footer_lines(risktabfn("Peak week", sqe = TRUE)) %>%
  font(., fontname = global_table_font, part = "all")
```
\newpage

<!-- SUPPLEMENTAL TABLE -->

```{r supp-cumhosp-risks-sqe, results = "asis"}
slug <- "Risks-EDSL-Mean_CumHosp-SQE"
cumhosp_risks <- fread(get_asset("TAB", slug))

supp_cap(slug)

cumhosp_risks %>%
  flextable(
    cwidth = c(0.6, rep(1.1, 2))
  ) %>%
  risktab_cnames_word() %>%
  bold(., part = "header") %>%
  add_footer_lines(risktabfn("Cumulative hospitalizations", sqe = TRUE)) %>%
  font(., fontname = global_table_font, part = "all")
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-peakrate-risktiles-sqe, results = "asis", fig.asp = 1350/3600}
slug <- "Risktiles_Peak-Rate-SQE"
supp_cap(slug)
knitr::include_graphics(get_asset("FIG", slug))
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-peakweek-risktiles-sqe, results = "asis", fig.asp = 1350/3600}
slug <- "Risktiles_Peak-Week-SQE"
supp_cap(slug)
knitr::include_graphics(get_asset("FIG", slug))
```
\newpage

<!-- SUPPLEMENTAL FIGURE -->

```{r supp-cumhosp-risktiles-sqe, results = "asis", fig.asp = 1350/3600}
slug <- "Risktiles_Cum-Hosp-SQE"
supp_cap(slug)
knitr::include_graphics(get_asset("FIG", slug))
```
